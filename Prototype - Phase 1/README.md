Project Overview

This repository contains all components required to build a warehouse allocation simulation environment and apply reinforcement learning to optimize item placement. The project is divided into three main layers:

Synthetic data generation

Warehouse simulation environment

Reinforcement learning agents

The synthetic data feeds the environment. The environment acts as a controlled world in which RL agents can take actions, receive rewards, and learn policies for improved allocation.

Code Structure
root/
|-- data/
|   |-- raw/
|   |-- synthetic/
|   |-- processed/
|
|-- generator/
|   |-- generate_data.ipynb
|   |-- utils_data_gen.py
|
|-- env/
|   |-- warehouse_env.py
|   |-- models/
|   |   |-- parts.py
|   |   |-- locations.py
|   |   |-- allocations.py
|   |-- utils_env.py
|
|-- rl/
|   |-- agent_base.py
|   |-- dqn_agent.py
|   |-- ppo_agent.py
|   |-- replay_buffer.py
|   |-- training_loop.py
|
|-- scripts/
|   |-- run_simulation.py
|   |-- run_training.py
|
|-- README.md

Inputs and Outputs
Inputs

The environment consumes the following CSV files (already generated by the data synthesis module):

parts.csv

locations.csv

location_types.csv

allocations.csv

These are loaded once at environment initialization.

The RL agents take environment states as input.
The environment may also take an action definition from the agent at every step.

Outputs

The system produces:

Environment state transitions

Reward values

Valid/invalid action feedback

Episode logs for RL training

Optional export of modified allocation tables (environment snapshot)

Trained RL model weights

No component writes directly to the synthetic CSVs unless explicitly called by a script.

Module Responsibilities and Interaction

Below is a detailed explanation of how each Python file is expected to behave and how they integrate.

Data Generation Layer
generate_data.ipynb

Main entry point for generating synthetic CSVs.

Loads raw source data.

Applies rules, correlations, and statistical models.

Outputs the four final CSV files.

Should not be used by the simulation or RL components directly.

utils_data_gen.py

Helper functions for sampling distributions, applying constraints, or generating correlated variables.

Used only by the notebook.

Environment Layer
warehouse_env.py

This is the core of the simulation. It should provide:

Key functions

__init__(paths)
Loads CSVs into memory and initializes models: Parts, Locations, Allocations.

reset()
Restores initial data and returns the initial state representation.

step(action)
Applies the chosen action, updates internal state, computes reward, and returns:

next_state

reward

done

info (diagnostics)

get_state()
Encodes current warehouse state as a vector, dictionary, or structured object depending on RL needs.

is_valid_action(action)
Ensures no illegal moves (capacity, geometry, or logic violations).

apply_action(action)
Performs the allocation update (move, swap, reassign units).

Expected integration
warehouse_env.py imports the classes from parts.py, locations.py, and allocations.py to manage the warehouse objects.

models/parts.py

Contains a Part class that represents an item in the warehouse.

Attributes

item_id

description

weight

quantity per box

boxes on hand

demand

dimensions

Methods

fits_in(location)
Optional helper to determine whether it can physically fit in a location.

models/locations.py

Defines two classes:

LocationType

Type code

Dimensions

Number of instances

LocationInstance

Instantiated from LocationType

Tracks which items and how many units are stored

Tracks used vs available capacity

Tracks geometric suitability based on item dimensions

Methods

can_store(part, units)

store(part, units)

remove(part, units)

free_capacity()

models/allocations.py

Represents the mapping between items and their storage locations.

Responsibilities

Hold initial allocation data

Provide indexing for quick lookups

Update allocations when items are moved or swapped

Methods

get_item_locations(item_id)

allocate(item_id, location_id, units)

deallocate(item_id, location_id, units)

utils_env.py

Helper functions for distance calculations, capacity checking, reward shaping utilities, or state encoding.

Imported by warehouse_env.py.

Reinforcement Learning Layer
agent_base.py

Defines the abstract API for all agents.

Methods

act(state)

learn(batch)

update_target() (optional for target networks)

save(path)

load(path)

dqn_agent.py

Implements a Deep Q-Network agent.

Key components

Neural network model

Epsilon-greedy action selection

Replay buffer integration

Batch gradient updates

Target network for stability

Interaction
Calls env.step(action) at every training iteration.

ppo_agent.py

Implements a Proximal Policy Optimization agent.

Key components

Policy network

Value network

Trajectory collection

Clipped policy loss

Entropy regularization

Interaction
Collects full rollouts from the environment, then optimizes policy and value estimator.

replay_buffer.py

For off-policy algorithms.

Responsibilities

Store transitions (state, action, reward, next_state, done)

Sample random batches for learning

training_loop.py

Coordinates training.

Responsibilities

Initialize environment and agent

Run episodes

Log performance metrics

Save models and optionally environment snapshots

Script Layer
run_simulation.py

Manual environment runner for debugging or demonstration.

Responsibilities

Load environment

Execute scripted or random actions

Print or visualize state transitions

Does not involve RL

run_training.py

Main entry point for RL training.

Responsibilities

Instantiate selected agent

Instantiate environment

Start training loop

Save results and model weights

Integration Summary

Synthetic data is generated once and stored in CSVs.

The environment loads all CSVs into in-memory structures using the classes in env/models.

RL agents interact with the environment exclusively through the step and reset interfaces.

The environment handles all warehouse logic, constraints, and updates.

Training scripts tie the agent and the environment together.
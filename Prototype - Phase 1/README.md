# Warehouse Optimization Project

## Overview

This repository contains the codebase for a warehouse item relocation optimization project. The workflow involves synthetic data generation, warehouse simulation, and reinforcement learning (RL)-based optimization of item allocations.

The code is organized into multiple Python scripts and notebooks, with clear responsibilities for each component. Inputs and outputs are CSV files representing warehouse locations, item parts, and allocations. The system is designed to scale from small datasets for testing to larger synthetic warehouses.

---

## Code Structure

### Notebooks
- `data_summary.ipynb`  
  Computes summary statistics, correlation analysis, and conditional rules to guide synthetic data generation.

- `synthetic_generation.ipynb`  
  Generates synthetic warehouse datasets using the summary stats and conditional rules. Produces CSV files for locations, location types, allocations, and parts.

### Python Scripts
- `data_generator.py`  
  Functions to read summary stats and generate synthetic CSV datasets.

- `simulation.py`  
  Contains the warehouse simulation environment, including:
  - Item relocation functions
  - Allocation updates
  - Inventory operations
  - Logging of operations

- `rl_agent.py`  
  Reinforcement learning agent for optimizing item allocations. Should interface with the simulation environment to propose moves and learn from rewards.

- `utils.py`  
  Helper functions:
  - CSV I/O
  - Metric conversions
  - Ratio and constraint checks
  - Any common utilities shared across modules

---

## Inputs and Outputs

### Inputs
1. **Synthetic CSV datasets**
   - `locations.csv`  
     Columns: `LOC_INST_CODE`, `LOC_TYPE`
   - `location_types.csv`  
     Columns: `LOC_CODE`, `WID_MM`, `HT_MM`, `DEP_MM`, `NUM_LOCS`
   - `allocations.csv`  
     Columns: `ITEM_ID`, `LOC_CODE`, `UNITS_ALLOCATED`
   - `parts.csv`  
     Columns: `ITEM_ID`, `ITEM_DESC`, `WT_KG`, `QTY_PER_BOX`, `BOXES_ON_HAND`, `DEMAND`, `LEN_MM`, `WID_MM`, `DEP_MM`

2. **Summary statistics / conditional rules**  
   Generated by `data_summary.ipynb`:
   - `summary_stats_obf.csv`
   - `conditional_rules.csv`
   - `ratio_stats.json`
   - `correlations.csv`

### Outputs
1. **Synthetic CSVs**:  
   Generated datasets that are anonymized and structurally consistent for simulation and RL testing.

2. **Simulation results**:  
   Optional CSV or logs documenting item moves, allocation efficiency, and warehouse states.

3. **RL results**:  
   Policies, reward histories, and optimized allocation outputs.

---

## Recommended Class and Function Structure

### `data_generator.py`
- **Functions**
  - `load_summary_stats(path)`
  - `generate_synthetic_locations()`
  - `generate_synthetic_location_types()`
  - `generate_synthetic_parts()`
  - `generate_synthetic_allocations()`
  - `save_csv(df, path)`

### `simulation.py`
- **Classes**
  - `Warehouse`
    - Properties: `locations`, `location_types`, `parts`, `allocations`
    - Methods: `move_item(item_id, from_loc, to_loc)`, `check_constraints()`, `current_state()`
  - `Simulator`
    - Methods: `run_simulation_step()`, `apply_move(move)`, `calculate_reward()`
  
### `rl_agent.py`
- **Classes**
  - `RLAgent`
    - Methods: `select_action(state)`, `update_policy(reward)`, `train(episodes)`
- **Functions**
  - `compute_reward()`
  - `encode_state(state)`

### `utils.py`
- Functions for:
  - Unit conversions
  - Constraint enforcement
  - CSV reading/writing
  - Data validation

---

## Workflow Integration

1. **Data Generation**
   - Run `synthetic_generation.ipynb` or `data_generator.py`  
   - Produces CSV files used by simulation and RL

2. **Simulation**
   - `simulation.py` loads CSVs
   - Provides environment API for RL agents
   - Allows manual tests or baseline policies

3. **Reinforcement Learning**
   - `rl_agent.py` interacts with `simulation.py`
   - Trains on warehouse environment
   - Outputs improved allocation strategies

4. **Iterative Development**
   - Synthetic dataset can be regenerated
   - Simulation and RL modules can be independently updated
   - CSVs act as the interface between components

---

## Notes

- All modules read data into memory for small to medium datasets. For large warehouse datasets, consider a database backend or chunked processing to avoid memory issues.
- CSVs are the standard interface for input/output between modules.
- Maintain a consistent naming convention to avoid confusion when loading synthetic data into the simulation or RL modules.

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f48fa64c",
      "metadata": {
        "id": "f48fa64c"
      },
      "source": [
        "# Reinforcement Learning Relocation Engine ( MDP & Tabular Q-learning based)\n",
        "\n",
        "This notebook is organized into **Markdown explanations** and **Python code cells** so that the methodology, assumptions, and implementation are easy to review.\n",
        "\n",
        "## What this notebook does\n",
        "- Loads the parts, locations, and baseline allocation CSVs, and cleans/standardizes their schemas (types, required columns, numeric conversions).\n",
        "\n",
        "- Uses the official geometry library to compute feasible orientations + max capacity per bin for each SKU (no custom packing logic).\n",
        "\n",
        "- Performs Dynamic ABC classification (A/B/C) based on demand ranking, and marks heavy items (weight threshold) for safety constraints.\n",
        "\n",
        "- Builds warehouse zones (Fast zone + Ergonomic zone) from coordinates to support operational priorities (pick speed + safety).\n",
        "\n",
        "- Trains an RL agent using tabular Q-learning (one-step / bandit-like updates) with ε-greedy exploration to learn preferred macro placement strategies.\n",
        "\n",
        "- Generates an optimized allocation using a consolidation-focused policy:\n",
        "\n",
        "    - prioritizes A→B→C and higher demand first,\n",
        "\n",
        "    - selects bins to maximize global consolidation (fill large bins for high-quantity items, avoid “percentage trap”),\n",
        "\n",
        "    - enforces single SKU per bin, but allows same SKU across multiple bins.\n",
        "\n",
        "- Exports a validator-compliant output CSV including geometry grid fields, orientation dimensions, and utilization.\n",
        "\n",
        "- Produces baseline vs RL dashboards/plots using the shared visualization/metrics library for fair comparison.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ab101e",
      "metadata": {
        "id": "58ab101e"
      },
      "source": [
        "## Reinforcement Learning Hyper Parameters\n",
        "\n",
        "The core learning routine uses **ε-greedy tabular Q-learning**. The following hyperparameters control exploration and learning dynamics (defaults taken from the code):\n",
        "\n",
        "| Parameter | Default | Role in learning |\n",
        "|---|---:|---|\n",
        "| **episodes** | **6000** | Number of training episodes (iterations over relocation decisions). |\n",
        "| **alpha** | **0.25** | Learning rate in the Q-update: how strongly new rewards overwrite old estimates. |\n",
        "| **epsilon** | **1.0** | Initial exploration probability in ε-greedy action selection. |\n",
        "| **epsilon_decay** | **0.996** | Multiplicative decay factor applied to ε each episode. |\n",
        "| **epsilon_min** | **0.06** | Lower bound on ε to preserve some exploration. |\n",
        "\n",
        "\n",
        "> Note: The implementation updates toward the immediate reward `r` (no explicit discount factor `γ` is used in the update). This corresponds to a **myopic / bandit-like** Q-update rather than the classical discounted-return form.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04114b2",
      "metadata": {
        "id": "f04114b2"
      },
      "source": [
        "## Reproducibility and execution notes\n",
        "\n",
        "- The original script imports helper modules (`geometry.py`, `distance.py`, `metrics_viz_lib.py`) via absolute paths (`/content/...`) because it was exported from Google Colab.\n",
        "- For local execution, either:\n",
        "  1. Place these helper modules next to this notebook and update the import paths, **or**\n",
        "  2. Keep the same directory structure and adjust paths accordingly.\n",
        "\n",
        "The next cell contains the original *robust local import* helper and global configuration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f70c40",
      "metadata": {
        "id": "d1f70c40"
      },
      "source": [
        "## Preamble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "99560536",
      "metadata": {
        "id": "99560536"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "\n",
        "# Required standard and third-party library imports\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import importlib.util\n",
        "\n",
        "# -------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc322e69",
      "metadata": {
        "id": "bc322e69"
      },
      "source": [
        "## 0) Robust local imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45284d8f",
      "metadata": {
        "id": "45284d8f"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def import_from_path(module_name: str, path: str):\n",
        "    spec = importlib.util.spec_from_file_location(module_name, path)\n",
        "    mod = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(mod)  # type: ignore\n",
        "    return mod\n",
        "\n",
        "# Change the path of all the input files when using different files\n",
        "GEOM = import_from_path(\"geometry\", \"/content/geometry.py\")\n",
        "DIST = import_from_path(\"distance\", \"/content/distance.py\")\n",
        "VIZ  = import_from_path(\"metrics_viz_lib\", \"/content/metrics_viz_lib.py\")\n",
        "\n",
        "# -----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34cc761",
      "metadata": {
        "id": "a34cc761"
      },
      "source": [
        "## 1) File configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9d97d0",
      "metadata": {
        "id": "cb9d97d0"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "\n",
        "ALLOC_BASELINE_FILE = \"/content/allocations_prototype.csv\"\n",
        "LOCATIONS_FILE      = \"/content/locations_dummy_prototype.csv\"\n",
        "PARTS_FILE          = \"/content/synthetic_parts_generated_prototype.csv\"\n",
        "OUTPUT_ALLOC_FILE   = \"/content/allocations_rl_optimized.csv\"\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e510945",
      "metadata": {
        "id": "5e510945"
      },
      "source": [
        "## 2) Loading + schema harmonization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19b8cbf",
      "metadata": {
        "id": "b19b8cbf"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------------------------------\n",
        "\n",
        "def read_csv_semicolon_if_needed(path: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(path, sep=None, engine=\"python\")\n",
        "    except Exception:\n",
        "        return pd.read_csv(path, sep=\";\")\n",
        "\n",
        "def normalize_locations(df_loc: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_loc.copy()\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    required = [\"loc_inst_code\", \"width\", \"depth\", \"height\", \"x\", \"y\", \"z\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"locations_dummy.csv missing columns: {missing}\")\n",
        "\n",
        "    df[\"loc_inst_code\"] = df[\"loc_inst_code\"].astype(str)\n",
        "\n",
        "    if \"LOCATION_TYPE\" not in df.columns:\n",
        "        df[\"LOCATION_TYPE\"] = \"BIN\"\n",
        "\n",
        "    for c in [\"width\", \"depth\", \"height\", \"x\", \"y\", \"z\"]:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    df[\"LOCATION_VOL_MM3\"] = (df[\"width\"] * df[\"depth\"] * df[\"height\"]).astype(float)\n",
        "\n",
        "    # Optional: row/bay/level (if present, we keep for affinity neighbor logic)\n",
        "    for c in [\"row_num\", \"bay_num\", \"level_num\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def normalize_parts(df_parts: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_parts.copy()\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    required = [\"ITEM_ID\", \"LEN_MM\", \"WID_MM\", \"DEP_MM\", \"WT_KG\", \"BOXES_ON_HAND\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"synthetic_parts_generated.csv missing columns: {missing}\")\n",
        "\n",
        "    df[\"ITEM_ID\"] = df[\"ITEM_ID\"].astype(str)\n",
        "\n",
        "    for c in [\"LEN_MM\", \"WID_MM\", \"DEP_MM\", \"WT_KG\", \"BOXES_ON_HAND\"]:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    if \"DEMAND\" not in df.columns:\n",
        "        df[\"DEMAND\"] = df[\"BOXES_ON_HAND\"].astype(float)\n",
        "    else:\n",
        "        df[\"DEMAND\"] = pd.to_numeric(df[\"DEMAND\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    if \"QTY_PER_BOX\" not in df.columns:\n",
        "        df[\"QTY_PER_BOX\"] = 1\n",
        "\n",
        "    df[\"UNIT_VOL_MM3\"] = (df[\"LEN_MM\"] * df[\"WID_MM\"] * df[\"DEP_MM\"]).astype(float)\n",
        "    df[\"IS_HEAVY\"] = df[\"WT_KG\"] > 15.0  # Guide: heavy items if WT_KG > 15kg\n",
        "\n",
        "    return df\n",
        "\n",
        "def normalize_allocations_baseline(df_alloc: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Baseline is used only for:\n",
        "      - baseline dashboard display\n",
        "      - old_bin reference (movement distance between old/new is not part of guide score,\n",
        "        but we keep old_bin for optional tie-breakers if needed)\n",
        "    We also sanitize single row per bin (if baseline violated).\n",
        "    \"\"\"\n",
        "    df = df_alloc.copy()\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    rename_map = {}\n",
        "    if \"LOCATION_ID\" in df.columns and \"loc_inst_code\" not in df.columns:\n",
        "        rename_map[\"LOCATION_ID\"] = \"loc_inst_code\"\n",
        "    if \"LOC_CODE\" in df.columns and \"loc_inst_code\" not in df.columns:\n",
        "        rename_map[\"LOC_CODE\"] = \"loc_inst_code\"\n",
        "    if \"SKU\" in df.columns and \"ITEM_ID\" not in df.columns:\n",
        "        rename_map[\"SKU\"] = \"ITEM_ID\"\n",
        "    if \"QTY\" in df.columns and \"QTY_ALLOCATED\" not in df.columns:\n",
        "        rename_map[\"QTY\"] = \"QTY_ALLOCATED\"\n",
        "    if \"UNITS_ALLOCATED\" in df.columns and \"QTY_ALLOCATED\" not in df.columns:\n",
        "        rename_map[\"UNITS_ALLOCATED\"] = \"QTY_ALLOCATED\"\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "    required = [\"loc_inst_code\", \"ITEM_ID\", \"QTY_ALLOCATED\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            f\"allocations.csv must contain {required}. Missing: {missing}. \"\n",
        "            f\"Columns found: {list(df.columns)}\"\n",
        "        )\n",
        "\n",
        "    df[\"loc_inst_code\"] = df[\"loc_inst_code\"].astype(str)\n",
        "    df[\"ITEM_ID\"] = df[\"ITEM_ID\"].astype(str)\n",
        "    df[\"QTY_ALLOCATED\"] = pd.to_numeric(df[\"QTY_ALLOCATED\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    # Enforce single SKU per bin in baseline state (if baseline violates, keep first)\n",
        "    df = df.sort_values([\"loc_inst_code\"]).drop_duplicates(subset=[\"loc_inst_code\"], keep=\"first\")\n",
        "    df = df[df[\"QTY_ALLOCATED\"] > 0].copy()\n",
        "\n",
        "    return df[[\"loc_inst_code\", \"ITEM_ID\", \"QTY_ALLOCATED\"]]\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ed058d6",
      "metadata": {
        "id": "0ed058d6"
      },
      "source": [
        "## 3) Geometry wrapper (ONLY via input library), RL Training & Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae43803c",
      "metadata": {
        "id": "ae43803c"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def geom_solve_capacity_and_layout(loc_row: pd.Series, part_row: pd.Series, qty: int):\n",
        "    # This wrapper handles both Series and Dict inputs because optimizations convert rows to Dicts\n",
        "    loc_dims = [float(loc_row[\"width\"]), float(loc_row[\"depth\"]), float(loc_row[\"height\"])]\n",
        "    sku_dims = [float(part_row[\"LEN_MM\"]), float(part_row[\"WID_MM\"]), float(part_row[\"DEP_MM\"])]\n",
        "\n",
        "    max_units, best_orient, grid = GEOM.compute_layered_capacity(loc_dims, sku_dims)\n",
        "    if max_units <= 0 or best_orient is None:\n",
        "        return None\n",
        "\n",
        "    if qty > max_units:\n",
        "        return None\n",
        "\n",
        "    gx, gy, gz = grid\n",
        "    ox, oy, oz = best_orient\n",
        "\n",
        "    full_layers, units_per_layer, partial_units = GEOM.compute_actual_layout(qty, grid)\n",
        "\n",
        "    return {\n",
        "        \"MAX_UNITS\": int(max_units),\n",
        "        \"GRID_X\": int(gx),\n",
        "        \"GRID_Y\": int(gy),\n",
        "        \"GRID_Z\": int(gz),\n",
        "        \"ORIENT_X_MM\": float(ox),\n",
        "        \"ORIENT_Y_MM\": float(oy),\n",
        "        \"ORIENT_Z_MM\": float(oz),\n",
        "        \"FULL_LAYERS\": int(full_layers),\n",
        "        \"PARTIAL_UNITS\": int(partial_units),\n",
        "    }\n",
        "\n",
        "# ------------------------------------\n",
        "# GUIDE PRE-PROCESSING\n",
        "# ------------------------------------\n",
        "\n",
        "def apply_dynamic_abc(df_parts: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    STRICT parity:\n",
        "      - A = top 20% by DEMAND (floor/int)\n",
        "      - B = next 30% (until 50%)\n",
        "      - C = rest\n",
        "      - Heavy = WT_KG > 15kg\n",
        "    \"\"\"\n",
        "    df = df_parts.copy()\n",
        "    df[\"ITEM_ID\"] = df[\"ITEM_ID\"].astype(str)\n",
        "\n",
        "    df = df.sort_values(\"DEMAND\", ascending=False).reset_index(drop=True)\n",
        "    n = len(df)\n",
        "\n",
        "    if n == 0:\n",
        "        df[\"ABC_CLASS\"] = \"C\"\n",
        "        df[\"IS_HEAVY\"] = False\n",
        "        return df\n",
        "\n",
        "    # floor style\n",
        "    a_cut = int(n * 0.20)\n",
        "    b_cut = int(n * 0.50)\n",
        "\n",
        "    # safety for tiny datasets\n",
        "    if a_cut == 0 and n > 0:\n",
        "        a_cut = 1\n",
        "    if b_cut < a_cut:\n",
        "        b_cut = a_cut\n",
        "\n",
        "    abc = []\n",
        "    for i in range(n):\n",
        "        if i < a_cut:\n",
        "            abc.append(\"A\")\n",
        "        elif i < b_cut:\n",
        "            abc.append(\"B\")\n",
        "        else:\n",
        "            abc.append(\"C\")\n",
        "\n",
        "    df[\"ABC_CLASS\"] = abc\n",
        "    df[\"IS_HEAVY\"] = (pd.to_numeric(df[\"WT_KG\"], errors=\"coerce\").fillna(0.0) > 15.0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def build_guide_zones(df_loc: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Guide zones:\n",
        "      - Entrance: X=0, Y=maxY/2, Z=0\n",
        "      - Fast zone: X <= 0.25 * MAX(X)\n",
        "      - Ergo zone: 700 <= Z <= 1500\n",
        "    Also returns:\n",
        "      - max_dist_possible used for distance penalty normalization (-100..0)\n",
        "      - max_x_dim (kept because main passes it)\n",
        "    \"\"\"\n",
        "    df = df_loc.copy()\n",
        "    df[\"loc_inst_code\"] = df[\"loc_inst_code\"].astype(str)\n",
        "\n",
        "    max_x = float(df[\"x\"].max() if df[\"x\"].max() > 0 else 1.0)\n",
        "    max_y = float(df[\"y\"].max() if df[\"y\"].max() > 0 else 0.0)\n",
        "\n",
        "    entrance = {\"x\": 0.0, \"y\": max_y / 2.0, \"z\": 0.0}\n",
        "\n",
        "    fast_x_limit = 0.25 * max_x\n",
        "    df[\"IS_FAST_ZONE\"] = (df[\"x\"] <= fast_x_limit)\n",
        "    df[\"IS_ERGO_ZONE\"] = df[\"z\"].between(700.0, 1500.0)\n",
        "\n",
        "    # A reasonable \"max possible\" for manhattan distance from entrance\n",
        "    # furthest x + furthest y deviation from mid\n",
        "    max_dist_possible = float(max_x + (max_y / 2.0))\n",
        "    if max_dist_possible <= 0:\n",
        "        max_dist_possible = 1.0\n",
        "\n",
        "    max_x_dim = max_x\n",
        "    return df, entrance, max_dist_possible, max_x_dim\n",
        "\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# RLRelocator (BOX-LEVEL)\n",
        "# ---------------------------------------------------\n",
        "class RLRelocator:\n",
        "    \"\"\"\n",
        "    BOX-LEVEL placement:\n",
        "      - Places exactly 1 box per decision.\n",
        "      - Single SKU per bin (no mixed storage).\n",
        "      - Same SKU can occupy multiple bins (overflow allowed).\n",
        "      - Uses 4 actions = zone combos:\n",
        "          0: Fast+Ergo\n",
        "          1: Fast+NonErgo\n",
        "          2: NonFast+Ergo\n",
        "          3: NonFast+NonErgo\n",
        "\n",
        "    Reward/Score:\n",
        "      Illegal => -10000 if:\n",
        "        - geometry fit is False  (geom_pack None)\n",
        "        - heavy AND z > 1500mm\n",
        "      Else:\n",
        "        ZoneReward + UtilReward + DistancePenalty + AffinityReward\n",
        "        ZoneReward:\n",
        "          A in Fast&Ergo: +1000\n",
        "          B in Fast&Ergo: +400\n",
        "        UtilReward: ((qty * unit_vol)/bin_vol) * 800 (clamped 0..800)\n",
        "        DistancePenalty: (dist/max_dist_possible) * -100 (range -100..0)\n",
        "        Affinity: +50 if neighbor has SKU volume within ±15%\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df_parts, df_loc, df_alloc_baseline, entrance, max_dist_possible, max_x_dim):\n",
        "        # base tables\n",
        "        self.parts = df_parts.copy()\n",
        "        self.loc = df_loc.copy()\n",
        "        self.alloc = df_alloc_baseline.copy()\n",
        "\n",
        "        # types\n",
        "        self.parts[\"ITEM_ID\"] = self.parts[\"ITEM_ID\"].astype(str)\n",
        "        self.loc[\"loc_inst_code\"] = self.loc[\"loc_inst_code\"].astype(str)\n",
        "        self.alloc[\"ITEM_ID\"] = self.alloc[\"ITEM_ID\"].astype(str)\n",
        "        self.alloc[\"loc_inst_code\"] = self.alloc[\"loc_inst_code\"].astype(str)\n",
        "\n",
        "\n",
        "        # OPTIMIZATION: CONVERT TO DICT FOR FAST LOOKUP\n",
        "\n",
        "        self.part_dict = self.parts.set_index(\"ITEM_ID\").to_dict(orient=\"index\")\n",
        "        self.loc_dict = self.loc.set_index(\"loc_inst_code\").to_dict(orient=\"index\")\n",
        "\n",
        "        # entrance + normalization\n",
        "        self.entrance = {\"x\": float(entrance[\"x\"]), \"y\": float(entrance[\"y\"]), \"z\": float(entrance[\"z\"])}\n",
        "        self.max_dist_possible = float(max_dist_possible) if float(max_dist_possible) > 0 else 1.0\n",
        "        self.max_x_dim = float(max_x_dim) if float(max_x_dim) > 0 else 1.0\n",
        "\n",
        "        # distance.py index + pseudo ENTRANCE\n",
        "        self.locations_index = {\n",
        "            str(r[\"loc_inst_code\"]): {\n",
        "                \"POS_X_MM\": int(float(r[\"x\"])),\n",
        "                \"POS_Y_MM\": int(float(r[\"y\"])),\n",
        "                \"POS_Z_MM\": int(float(r[\"z\"])),\n",
        "            }\n",
        "            for _, r in self.loc.iterrows()\n",
        "        }\n",
        "        self.locations_index[\"ENTRANCE\"] = {\n",
        "            \"POS_X_MM\": int(self.entrance[\"x\"]),\n",
        "            \"POS_Y_MM\": int(self.entrance[\"y\"]),\n",
        "            \"POS_Z_MM\": int(self.entrance[\"z\"]),\n",
        "        }\n",
        "\n",
        "        # actions: (is_fast, is_ergo)\n",
        "        self.actions = [\n",
        "            (True,  True),   # 0\n",
        "            (True,  False),  # 1\n",
        "            (False, True),   # 2\n",
        "            (False, False),  # 3\n",
        "        ]\n",
        "\n",
        "        # Q-table\n",
        "        self.Q = {}\n",
        "\n",
        "        # volume buckets (for state generalization)\n",
        "        qv = self.parts[\"UNIT_VOL_MM3\"].quantile([0.33, 0.66]).to_dict()\n",
        "        self.v1 = float(qv.get(0.33, 0.0))\n",
        "        self.v2 = float(qv.get(0.66, 0.0))\n",
        "\n",
        "        # neighbors for affinity\n",
        "        self.neighbors = self._build_neighbor_map()\n",
        "\n",
        "    # -----------------------\n",
        "    # State representation\n",
        "    # -----------------------\n",
        "    def _vol_bucket(self, v: float) -> int:\n",
        "        if v <= self.v1:\n",
        "            return 0\n",
        "        if v <= self.v2:\n",
        "            return 1\n",
        "        return 2\n",
        "\n",
        "    def _get_state(self, item_id: str):\n",
        "        # OPTIMIZED LOOKUP\n",
        "        r = self.part_dict.get(str(item_id))\n",
        "        if not r:\n",
        "            # Fallback (shouldn't happen if inputs align)\n",
        "            return (\"C\", 0, 0)\n",
        "\n",
        "        abc = str(r.get(\"ABC_CLASS\", \"C\"))\n",
        "        heavy = 1 if bool(r.get(\"IS_HEAVY\", False)) else 0\n",
        "        vb = self._vol_bucket(float(r[\"UNIT_VOL_MM3\"]))\n",
        "        return (abc, vb, heavy)\n",
        "\n",
        "    def _Q_row(self, state):\n",
        "        if state not in self.Q:\n",
        "            self.Q[state] = np.zeros(len(self.actions), dtype=float)\n",
        "        return self.Q[state]\n",
        "\n",
        "    # -------------------------\n",
        "    # Neighbor map for affinity\n",
        "    # -------------------------\n",
        "    def _build_neighbor_map(self):\n",
        "        df = self.loc.copy()\n",
        "        df[\"loc_inst_code\"] = df[\"loc_inst_code\"].astype(str)\n",
        "        nbr = {k: [] for k in df[\"loc_inst_code\"].tolist()}\n",
        "\n",
        "        # structured neighbor if available\n",
        "        if {\"row_num\", \"bay_num\", \"level_num\"}.issubset(df.columns):\n",
        "            g = df.dropna(subset=[\"row_num\", \"bay_num\", \"level_num\"]).copy()\n",
        "            if not g.empty:\n",
        "                for (_, _), grp in g.groupby([\"row_num\", \"level_num\"]):\n",
        "                    grp2 = grp.sort_values(\"bay_num\")\n",
        "                    by_bay = {int(b): str(l) for b, l in zip(grp2[\"bay_num\"].astype(int), grp2[\"loc_inst_code\"].astype(str))}\n",
        "                    for bay, loc_id in by_bay.items():\n",
        "                        for cand in (bay - 1, bay + 1):\n",
        "                            if cand in by_bay:\n",
        "                                nbr[loc_id].append(by_bay[cand])\n",
        "                return nbr\n",
        "\n",
        "        # fallback: same (y,z), nearest x\n",
        "        for (_, _), grp in df.groupby([\"y\", \"z\"]):\n",
        "            grp2 = grp.sort_values(\"x\")\n",
        "            locs = grp2[\"loc_inst_code\"].astype(str).tolist()\n",
        "            for i, loc_id in enumerate(locs):\n",
        "                if i - 1 >= 0:\n",
        "                    nbr[loc_id].append(locs[i - 1])\n",
        "                if i + 1 < len(locs):\n",
        "                    nbr[loc_id].append(locs[i + 1])\n",
        "        return nbr\n",
        "\n",
        "    # -----------------------\n",
        "    # Guide scoring helpers\n",
        "    # -----------------------\n",
        "    def _dist_from_entrance(self, loc_id: str) -> float:\n",
        "        \"\"\"\n",
        "        STRICT Guide distance:\n",
        "          Manhattan distance in X/Y only from Entrance (X=0, Y=maxY/2).\n",
        "          (Do NOT include Z; Z is for ergo/height constraints, not travel.)\n",
        "        \"\"\"\n",
        "        # OPTIMIZED LOOKUP\n",
        "        r = self.loc_dict[str(loc_id)]\n",
        "\n",
        "        x = float(r[\"x\"])\n",
        "        y = float(r[\"y\"])\n",
        "        ex = float(self.entrance[\"x\"])\n",
        "        ey = float(self.entrance[\"y\"])\n",
        "        return float(abs(x - ex) + abs(y - ey))\n",
        "\n",
        "    def _zone_reward(self, abc_class: str, loc_row: dict) -> float:\n",
        "        # loc_row is now a dict from self.loc_dict\n",
        "        in_target = bool(loc_row[\"IS_FAST_ZONE\"]) and bool(loc_row[\"IS_ERGO_ZONE\"])\n",
        "        if abc_class == \"A\" and in_target:\n",
        "            return 1000.0\n",
        "        if abc_class == \"B\" and in_target:\n",
        "            return 400.0\n",
        "        return 0.0\n",
        "\n",
        "    def _distance_penalty(self, dist_from_entrance: float) -> float:\n",
        "        return (dist_from_entrance / self.max_dist_possible) * -100.0\n",
        "\n",
        "    def _util_ratio(self, unit_vol_mm3: float, qty: int, bin_vol_mm3: float) -> float:\n",
        "        if bin_vol_mm3 <= 0:\n",
        "            return 0.0\n",
        "        u = (qty * unit_vol_mm3) / bin_vol_mm3\n",
        "        return max(0.0, min(1.0, u))\n",
        "\n",
        "    def _affinity_reward(self, loc_id: str, unit_vol_mm3: float, bin_sku_map: dict) -> float:\n",
        "        lo = 0.85 * unit_vol_mm3\n",
        "        hi = 1.15 * unit_vol_mm3\n",
        "        for n in self.neighbors.get(str(loc_id), []):\n",
        "            sku_n = bin_sku_map.get(str(n))\n",
        "            if sku_n is None:\n",
        "                continue\n",
        "            try:\n",
        "                # OPTIMIZED LOOKUP\n",
        "                v_n = float(self.part_dict[str(sku_n)][\"UNIT_VOL_MM3\"])\n",
        "            except Exception:\n",
        "                continue\n",
        "            if lo <= v_n <= hi:\n",
        "                return 50.0\n",
        "        return 0.0\n",
        "\n",
        "    def _score_placement(self, item_id: str, loc_id: str, qty_after: int, geom_pack, bin_sku_map: dict) -> float:\n",
        "        # OPTIMIZED LOOKUP\n",
        "        part = self.part_dict[str(item_id)]\n",
        "        loc_row = self.loc_dict[str(loc_id)]\n",
        "\n",
        "        # Illegal\n",
        "        if geom_pack is None:\n",
        "            return -10000.0\n",
        "\n",
        "        if bool(part[\"IS_HEAVY\"]) and float(loc_row[\"z\"]) > 1500.0:\n",
        "            return -10000.0\n",
        "\n",
        "        abc = str(part[\"ABC_CLASS\"])\n",
        "        unit_vol = float(part[\"UNIT_VOL_MM3\"])\n",
        "        bin_vol = float(loc_row[\"LOCATION_VOL_MM3\"])\n",
        "\n",
        "        rz = self._zone_reward(abc, loc_row)\n",
        "        util = self._util_ratio(unit_vol, qty_after, bin_vol)\n",
        "        ru = util * 800.0\n",
        "\n",
        "        dist = self._dist_from_entrance(loc_id)\n",
        "        pdist = self._distance_penalty(dist)\n",
        "\n",
        "        raff = self._affinity_reward(loc_id, unit_vol, bin_sku_map)\n",
        "\n",
        "        return float(rz + ru + pdist + raff)\n",
        "\n",
        "    # -----------------------\n",
        "    # Zone bin listing\n",
        "    # -----------------------\n",
        "    def _bins_in_action_zone(self, action):\n",
        "        is_fast, is_ergo = action\n",
        "        df = self.loc\n",
        "        return df[(df[\"IS_FAST_ZONE\"] == bool(is_fast)) & (df[\"IS_ERGO_ZONE\"] == bool(is_ergo))][\"loc_inst_code\"].astype(str).tolist()\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    # Calculate max capacity for a specific bin\n",
        "    # -------------------------------------------------------\n",
        "    def _get_bin_capacity(self, loc_id, item_id):\n",
        "        \"\"\"\n",
        "        Returns (max_units, geom_data) for a specific bin/item combo.\n",
        "        Returns (0, None) if it doesn't fit or violates constraints.\n",
        "        \"\"\"\n",
        "        loc_row = self.loc_dict[str(loc_id)]\n",
        "        part_row = self.part_dict[str(item_id)]\n",
        "\n",
        "        # Hard constraint: Heavy items (>15kg) forbidden above 1500mm\n",
        "        if bool(part_row[\"IS_HEAVY\"]) and float(loc_row[\"z\"]) > 1500.0:\n",
        "            return 0, None\n",
        "\n",
        "        # Pass qty=1 just to get the MAX_UNITS and GRID from the geometry engine\n",
        "        res = geom_solve_capacity_and_layout(loc_row, part_row, qty=1)\n",
        "        if res is None:\n",
        "            return 0, None\n",
        "\n",
        "        # If the geometry engine says 0 fits, return 0\n",
        "        if int(res[\"MAX_UNITS\"]) <= 0:\n",
        "            return 0, None\n",
        "\n",
        "        return int(res[\"MAX_UNITS\"]), res\n",
        "\n",
        "\n",
        "    def _pick_best_bin_for_action(\n",
        "    self,\n",
        "    item_id: str,\n",
        "    action,\n",
        "    bin_sku_map: dict,\n",
        "    bin_qty_map: dict,\n",
        "    search_cap: int = 250\n",
        "):\n",
        "        \"\"\"\n",
        "        Hard constraints (already enforced by skipping):\n",
        "          1) Geometric feasibility: must fit (geom_pack not None)\n",
        "          2) Weight safety: heavy items forbidden if z > 1500mm\n",
        "\n",
        "        Then choose by priorities in order:\n",
        "          3) Picking efficiency is handled by ACTION choice (Fast/Ergo zones).\n",
        "             Within this action/zone, selection is:\n",
        "          4) Storage utilization (PRIMARY): maximize utilization after placement\n",
        "          5) Pick velocity (tie-break): minimize distance to entrance\n",
        "          6) Organization/polish (tie-break): maximize affinity (+50 if neighbor similar volume)\n",
        "          7) Final tie-break: maximize total Guide score (your _score_placement)\n",
        "\n",
        "        Allowed bins:\n",
        "          - empty bin\n",
        "          - bin already holding SAME SKU (to fill it more)\n",
        "        Disallowed:\n",
        "          - bin holding a DIFFERENT SKU (mixed storage not allowed)\n",
        "        \"\"\"\n",
        "        item_id = str(item_id)\n",
        "\n",
        "        # OPTIMIZED LOOKUP\n",
        "        part = self.part_dict[item_id]\n",
        "\n",
        "        unit_vol = float(part[\"UNIT_VOL_MM3\"])\n",
        "        is_heavy = bool(part[\"IS_HEAVY\"])\n",
        "\n",
        "        zone_bins = self._bins_in_action_zone(action)\n",
        "        if not zone_bins:\n",
        "            return None\n",
        "\n",
        "        # Prefer filling already-started bins of same SKU first (helps utilization)\n",
        "        same_sku_bins, empty_bins = [], []\n",
        "        for loc_id in zone_bins:\n",
        "            cur = bin_sku_map.get(str(loc_id))\n",
        "            if cur is None:\n",
        "                empty_bins.append(str(loc_id))\n",
        "            elif str(cur) == item_id:\n",
        "                same_sku_bins.append(str(loc_id))\n",
        "            # else: different SKU => forbidden\n",
        "\n",
        "        ordered = (same_sku_bins + empty_bins)[: int(search_cap)]\n",
        "\n",
        "        best = None\n",
        "        # MINIMIZE this tuple:\n",
        "        #   (-util_after, dist_to_entrance, -affinity, -score, loc_id)\n",
        "        best_key = None\n",
        "\n",
        "        for loc_id in ordered:\n",
        "            # OPTIMIZED LOOKUP\n",
        "            loc_row = self.loc_dict[str(loc_id)]\n",
        "\n",
        "            # Hard constraint: heavy above 1500mm forbidden\n",
        "            if is_heavy and float(loc_row[\"z\"]) > 1500.0:\n",
        "                continue\n",
        "\n",
        "            cur_qty = int(bin_qty_map.get(str(loc_id), 0))\n",
        "            qty_after = cur_qty + 1  # BOX-LEVEL\n",
        "\n",
        "            geom_pack = geom_solve_capacity_and_layout(loc_row, part, qty_after)\n",
        "            if geom_pack is None:\n",
        "                continue  # Hard constraint: must fit\n",
        "\n",
        "            bin_vol = float(loc_row[\"LOCATION_VOL_MM3\"])\n",
        "            util_after = self._util_ratio(unit_vol, qty_after, bin_vol)  # 0..1\n",
        "            dist = self._dist_from_entrance(loc_id)  # strict 2D manhattan\n",
        "            affinity = self._affinity_reward(loc_id, unit_vol, bin_sku_map)  # 0 or 50\n",
        "            score = self._score_placement(item_id, loc_id, qty_after, geom_pack, bin_sku_map)\n",
        "\n",
        "            # If score is illegal for any reason, skip (keeps behavior consistent)\n",
        "            if score <= -9999:\n",
        "                continue\n",
        "\n",
        "            key = (-util_after, dist, -affinity, -score, str(loc_id))\n",
        "\n",
        "            if best_key is None or key < best_key:\n",
        "                best_key = key\n",
        "                best = (str(loc_id), geom_pack, float(score))\n",
        "\n",
        "        return best\n",
        "\n",
        "\n",
        "    # ---------------------------------\n",
        "    # Training (one-step, bandit-like)\n",
        "    # ---------------------------------\n",
        "    def train(self, episodes=6000, alpha=0.25, epsilon=1.0, epsilon_decay=0.996, epsilon_min=0.06):\n",
        "        items = self.parts[self.parts[\"BOXES_ON_HAND\"] > 0][\"ITEM_ID\"].astype(str).tolist()\n",
        "        if not items:\n",
        "            raise ValueError(\"No items with BOXES_ON_HAND > 0. Cannot train.\")\n",
        "\n",
        "        loc_ids = self.loc[\"loc_inst_code\"].astype(str).tolist()\n",
        "\n",
        "        for ep in range(int(episodes)):\n",
        "            item_id = str(random.choice(items))\n",
        "            state = self._get_state(item_id)\n",
        "\n",
        "            if random.random() < epsilon:\n",
        "                a_idx = random.randrange(len(self.actions))\n",
        "            else:\n",
        "                a_idx = int(np.argmax(self._Q_row(state)))\n",
        "            action = self.actions[a_idx]\n",
        "\n",
        "            # random occupancy snapshot (single SKU per bin)\n",
        "            bin_sku_map = {l: None for l in loc_ids}\n",
        "            bin_qty_map = {l: 0 for l in loc_ids}\n",
        "\n",
        "            occ_n = max(0, int(0.15 * len(loc_ids)))\n",
        "            occ_bins = random.sample(loc_ids, k=occ_n)\n",
        "\n",
        "            for b in occ_bins:\n",
        "                sku = str(random.choice(items))\n",
        "\n",
        "                # Enforce single SKU per bin (training snapshot)\n",
        "                if bin_sku_map[b] is not None:\n",
        "                  continue\n",
        "\n",
        "                # OPTIMIZED LOOKUP\n",
        "                loc_row = self.loc_dict[str(b)]\n",
        "                part_row = self.part_dict[str(sku)]\n",
        "\n",
        "                # Hard constraint: heavy item forbidden above z>1500\n",
        "                if bool(part_row[\"IS_HEAVY\"]) and float(loc_row[\"z\"]) > 1500.0:\n",
        "                  continue\n",
        "\n",
        "                # Hard constraint: fit/capacity must exist (geometry lib)\n",
        "                # Find max capacity first, by testing qty=1,\n",
        "                # then reading MAX_UNITS from the returned geom pack.\n",
        "                geom1 = geom_solve_capacity_and_layout(loc_row, part_row, qty=1)\n",
        "                if geom1 is None:\n",
        "                  continue\n",
        "\n",
        "                max_units = int(geom1[\"MAX_UNITS\"])\n",
        "                if max_units <= 0:\n",
        "                  continue\n",
        "\n",
        "                # Assign a random *feasible* qty within capacity\n",
        "                qty = random.randint(1, min(3, max_units))\n",
        "\n",
        "                bin_sku_map[b] = sku\n",
        "                bin_qty_map[b] = qty\n",
        "\n",
        "\n",
        "            best = self._pick_best_bin_for_action(item_id, action, bin_sku_map, bin_qty_map, search_cap=180)\n",
        "\n",
        "            q = self._Q_row(state)[a_idx]\n",
        "            if best is None:\n",
        "                self._Q_row(state)[a_idx] = q + alpha * (-5.0 - q)\n",
        "            else:\n",
        "                _, _, r = best\n",
        "                self._Q_row(state)[a_idx] = q + alpha * (r - q)\n",
        "\n",
        "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        return self.Q\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # Optimization: BEST FIT BATCH FILLING (Aggr. Utilization)\n",
        "    # --------------------------------------------------------\n",
        "    def optimize_from_baseline(self):\n",
        "        \"\"\"\n",
        "        Logic Stack Optimization:\n",
        "        1. Hard Constraints: Geometry & Weight (Filtered in _get_bin_capacity)\n",
        "        2. Macro-Placement: RL Zone Selection (Fast/Ergo)\n",
        "        3. Storage Util (Primary): Sort bins by projected % utilization.\n",
        "        4. Pick Velocity (Micro): Tie-break with distance.\n",
        "        5. Organization: Tie-break with affinity.\n",
        "        \"\"\"\n",
        "        items_df = self.parts[self.parts[\"BOXES_ON_HAND\"] > 0].copy()\n",
        "        if items_df.empty:\n",
        "            return pd.DataFrame(columns=[\"loc_inst_code\", \"ITEM_ID\", \"QTY_ALLOCATED\", \"_GEOM\"])\n",
        "\n",
        "        # Rank Items: A -> B -> C, then Demand High -> Low\n",
        "        abc_rank = {\"A\": 0, \"B\": 1, \"C\": 2}\n",
        "        items_df[\"_abc_rank\"] = items_df[\"ABC_CLASS\"].map(lambda x: abc_rank.get(str(x), 3))\n",
        "        items_df = items_df.sort_values([\"_abc_rank\", \"DEMAND\"], ascending=[True, False])\n",
        "\n",
        "        loc_ids = self.loc[\"loc_inst_code\"].astype(str).tolist()\n",
        "        occupied_bins = set()\n",
        "        placed_list = []\n",
        "\n",
        "        # Helper to calculate sorting score for a bin\n",
        "        def evaluate_bin(bin_id, item_id, qty_needed, unit_vol, is_heavy):\n",
        "            loc_row = self.loc_dict[bin_id]\n",
        "\n",
        "            # 1. Hard Constraint: Heavy > 1500mm\n",
        "            if is_heavy and float(loc_row[\"z\"]) > 1500.0:\n",
        "                return None\n",
        "\n",
        "            # 2. Hard Constraint: Geometry check\n",
        "            # Need max capacity to know how much can be actually put in\n",
        "            res = geom_solve_capacity_and_layout(loc_row, self.part_dict[item_id], qty=1)\n",
        "            if res is None or res[\"MAX_UNITS\"] <= 0:\n",
        "                return None\n",
        "\n",
        "            max_cap = int(res[\"MAX_UNITS\"])\n",
        "\n",
        "            # Calculate actual fill for this batch\n",
        "            fill_qty = min(qty_needed, max_cap)\n",
        "\n",
        "            # 3. Utilization Score (0.0 to 1.0)\n",
        "            bin_vol = float(loc_row[\"LOCATION_VOL_MM3\"])\n",
        "            if bin_vol <= 0: return None\n",
        "\n",
        "            stored_vol = fill_qty * unit_vol\n",
        "            util_score = stored_vol / bin_vol\n",
        "\n",
        "            # 4. Distance Score (Lower is better, so we invert or negate)\n",
        "            dist = self._dist_from_entrance(bin_id)\n",
        "\n",
        "            # 5. Affinity (Bonus if neighbor has similar volume)\n",
        "            affinity_bonus = 0.0\n",
        "            nbrs = self.neighbors.get(bin_id, [])\n",
        "            # For batch optimization, checking neighbors is tricky because they might be empty.\n",
        "            # A simplified check: is it next to an occupied bin of similar type?\n",
        "\n",
        "            # Return tuple for sorting:\n",
        "            # (Utilization DESC, Distance ASC) -> Python sorts tuples element-wise\n",
        "            # Used negative for utilization to sort Descending with default sort\n",
        "            return (-util_score, dist, fill_qty, max_cap)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # MAIN LOOP\n",
        "        # ---------------------------------------------------------\n",
        "        for _, item in items_df.iterrows():\n",
        "            item_id = str(item[\"ITEM_ID\"])\n",
        "            qty_remaining = int(item[\"BOXES_ON_HAND\"])\n",
        "            unit_vol = float(item[\"UNIT_VOL_MM3\"])\n",
        "            is_heavy = bool(item[\"IS_HEAVY\"])\n",
        "\n",
        "            if qty_remaining <= 0: continue\n",
        "\n",
        "            # Get RL Zone Preferences\n",
        "            state = self._get_state(item_id)\n",
        "            qrow = self._Q_row(state)\n",
        "            if len(set(qrow.tolist())) == 1: action_indices = [0, 1, 2, 3]\n",
        "            else: action_indices = list(np.argsort(qrow)[::-1])\n",
        "\n",
        "            # Try Zones in order of RL preference\n",
        "            for a_idx in action_indices:\n",
        "                if qty_remaining <= 0: break\n",
        "\n",
        "                action = self.actions[int(a_idx)]\n",
        "\n",
        "                # Get empty candidates in this zone\n",
        "                zone_bins = [b for b in self._bins_in_action_zone(action) if b not in occupied_bins]\n",
        "                if not zone_bins: continue\n",
        "\n",
        "                # SCORING CANDIDATES\n",
        "                candidates = []\n",
        "                for b in zone_bins:\n",
        "                    score = evaluate_bin(b, item_id, qty_remaining, unit_vol, is_heavy)\n",
        "                    if score:\n",
        "                        # score structure: (-util, dist, fill_qty, max_cap)\n",
        "                        candidates.append((score, b))\n",
        "\n",
        "                # Sort: Best Utilization first, then closest Distance\n",
        "                candidates.sort(key=lambda x: x[0])\n",
        "\n",
        "                # Fill\n",
        "                for score_tuple, bin_id in candidates:\n",
        "                    if qty_remaining <= 0: break\n",
        "\n",
        "                    _, _, fill_qty, _ = score_tuple\n",
        "\n",
        "\n",
        "                    # Re-calc capacity just to be safe and get geom data\n",
        "                    max_cap, geom_data = self._get_bin_capacity(bin_id, item_id)\n",
        "                    actual_fill = min(qty_remaining, max_cap)\n",
        "\n",
        "                    if actual_fill > 0:\n",
        "                        placed_list.append({\n",
        "                            \"loc_inst_code\": bin_id,\n",
        "                            \"ITEM_ID\": item_id,\n",
        "                            \"QTY_ALLOCATED\": actual_fill,\n",
        "                            \"_GEOM\": geom_solve_capacity_and_layout(self.loc_dict[bin_id], self.part_dict[item_id], actual_fill)\n",
        "                        })\n",
        "                        occupied_bins.add(bin_id)\n",
        "                        qty_remaining -= actual_fill\n",
        "\n",
        "            # Global Fallback (if zones full)\n",
        "            if qty_remaining > 0:\n",
        "                global_bins = [b for b in loc_ids if b not in occupied_bins]\n",
        "                candidates = []\n",
        "                for b in global_bins:\n",
        "                    score = evaluate_bin(b, item_id, qty_remaining, unit_vol, is_heavy)\n",
        "                    if score: candidates.append((score, b))\n",
        "\n",
        "                candidates.sort(key=lambda x: x[0])\n",
        "\n",
        "                for score_tuple, bin_id in candidates:\n",
        "                    if qty_remaining <= 0: break\n",
        "                    max_cap, geom_data = self._get_bin_capacity(bin_id, item_id)\n",
        "                    actual_fill = min(qty_remaining, max_cap)\n",
        "                    if actual_fill > 0:\n",
        "                        placed_list.append({\n",
        "                            \"loc_inst_code\": bin_id,\n",
        "                            \"ITEM_ID\": item_id,\n",
        "                            \"QTY_ALLOCATED\": actual_fill,\n",
        "                            \"_GEOM\": geom_solve_capacity_and_layout(self.loc_dict[bin_id], self.part_dict[item_id], actual_fill)\n",
        "                        })\n",
        "                        occupied_bins.add(bin_id)\n",
        "                        qty_remaining -= actual_fill\n",
        "\n",
        "        df_solution = pd.DataFrame(placed_list)\n",
        "        if df_solution.empty: return df_solution\n",
        "        return df_solution.sort_values(\"loc_inst_code\").reset_index(drop=True)\n",
        "\n",
        "# -------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0f69657",
      "metadata": {
        "id": "e0f69657"
      },
      "source": [
        "## 4) Exporting the RL Optimised Relocations CSV (validator compliant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2951abd",
      "metadata": {
        "id": "a2951abd"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------------------------------------------\n",
        "def build_validated_output(df_solution: pd.DataFrame, df_loc: pd.DataFrame, df_parts: pd.DataFrame) -> pd.DataFrame:\n",
        "    loc_idx = df_loc.set_index(\"loc_inst_code\")\n",
        "    part_idx = df_parts.set_index(\"ITEM_ID\")\n",
        "\n",
        "    out_rows = []\n",
        "    for _, r in df_solution.iterrows():\n",
        "        loc_id = str(r[\"loc_inst_code\"])\n",
        "        item_id = str(r[\"ITEM_ID\"])\n",
        "        qty = int(r[\"QTY_ALLOCATED\"])\n",
        "\n",
        "        loc_row = loc_idx.loc[loc_id]\n",
        "        part_row = part_idx.loc[item_id]\n",
        "\n",
        "        geom_pack = geom_solve_capacity_and_layout(loc_row, part_row, qty)\n",
        "        if geom_pack is None:\n",
        "            continue\n",
        "\n",
        "        location_vol_mm3 = float(loc_row[\"width\"] * loc_row[\"depth\"] * loc_row[\"height\"])\n",
        "        location_vol_m3 = location_vol_mm3 / 1e9\n",
        "\n",
        "        orient_vol_mm3 = geom_pack[\"ORIENT_X_MM\"] * geom_pack[\"ORIENT_Y_MM\"] * geom_pack[\"ORIENT_Z_MM\"]\n",
        "        stored_vol_mm3 = qty * orient_vol_mm3\n",
        "        stored_vol_m3 = stored_vol_mm3 / 1e9\n",
        "\n",
        "        util_pct = (stored_vol_mm3 / location_vol_mm3) * 100.0 if location_vol_mm3 > 0 else 0.0\n",
        "\n",
        "        out_rows.append({\n",
        "            \"loc_inst_code\": loc_id,\n",
        "            \"LOCATION_TYPE\": str(loc_row.get(\"LOCATION_TYPE\", \"BIN\")),\n",
        "            \"ITEM_ID\": item_id,\n",
        "            \"QTY_ALLOCATED\": int(qty),\n",
        "\n",
        "            \"MAX_UNITS\": int(geom_pack[\"MAX_UNITS\"]),\n",
        "            \"GRID_X\": int(geom_pack[\"GRID_X\"]),\n",
        "            \"GRID_Y\": int(geom_pack[\"GRID_Y\"]),\n",
        "            \"GRID_Z\": int(geom_pack[\"GRID_Z\"]),\n",
        "            \"FULL_LAYERS\": int(geom_pack[\"FULL_LAYERS\"]),\n",
        "            \"PARTIAL_UNITS\": int(geom_pack[\"PARTIAL_UNITS\"]),\n",
        "\n",
        "            \"ORIENT_X_MM\": float(geom_pack[\"ORIENT_X_MM\"]),\n",
        "            \"ORIENT_Y_MM\": float(geom_pack[\"ORIENT_Y_MM\"]),\n",
        "            \"ORIENT_Z_MM\": float(geom_pack[\"ORIENT_Z_MM\"]),\n",
        "\n",
        "            \"LOCATION_VOL_MM3\": float(location_vol_mm3),\n",
        "            \"LOCATION_VOL_M3\": float(location_vol_m3),\n",
        "            \"STORED_VOL_M3\": float(stored_vol_m3),\n",
        "            \"UTILIZATION_PCT\": float(util_pct),\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(out_rows)\n",
        "\n",
        "    # Hard enforce single row per bin\n",
        "    out = out.sort_values(\"loc_inst_code\").drop_duplicates(subset=[\"loc_inst_code\"], keep=\"first\").copy()\n",
        "\n",
        "    # Enforce grid math\n",
        "    out[\"MAX_UNITS\"] = (out[\"GRID_X\"] * out[\"GRID_Y\"] * out[\"GRID_Z\"]).astype(int)\n",
        "\n",
        "    return out\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4874eb8e",
      "metadata": {
        "id": "4874eb8e"
      },
      "source": [
        "## 5) Main runner (Generating the metrics and visualization plots for baseline and RL optimised states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a094ea7d",
      "metadata": {
        "id": "a094ea7d"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "def main():\n",
        "    print(\"Loading input files from Colab working directory...\")\n",
        "\n",
        "    df_loc_raw   = read_csv_semicolon_if_needed(LOCATIONS_FILE)\n",
        "    df_parts_raw = read_csv_semicolon_if_needed(PARTS_FILE)\n",
        "    df_alloc_raw = read_csv_semicolon_if_needed(ALLOC_BASELINE_FILE)\n",
        "\n",
        "    if \"ITEM_ID\" in df_parts_raw.columns:\n",
        "        df_parts_raw[\"ITEM_ID\"] = df_parts_raw[\"ITEM_ID\"].astype(str)\n",
        "    if \"ITEM_ID\" in df_alloc_raw.columns:\n",
        "        df_alloc_raw[\"ITEM_ID\"] = df_alloc_raw[\"ITEM_ID\"].astype(str)\n",
        "    if \"loc_inst_code\" in df_alloc_raw.columns:\n",
        "        df_alloc_raw[\"loc_inst_code\"] = df_alloc_raw[\"loc_inst_code\"].astype(str)\n",
        "    if \"loc_inst_code\" in df_loc_raw.columns:\n",
        "        df_loc_raw[\"loc_inst_code\"] = df_loc_raw[\"loc_inst_code\"].astype(str)\n",
        "\n",
        "    # Normalize\n",
        "    df_loc   = normalize_locations(df_loc_raw)\n",
        "    df_parts = normalize_parts(df_parts_raw)\n",
        "    df_alloc_baseline = normalize_allocations_baseline(df_alloc_raw)\n",
        "\n",
        "    # Guide preprocessing\n",
        "    df_parts = apply_dynamic_abc(df_parts)  # A/B/C per top 20%, next 30%, rest\n",
        "    df_loc, entrance, max_dist_possible, max_x_dim = build_guide_zones(df_loc)\n",
        "\n",
        "    print(f\"LOCATIONS: {len(df_loc)} rows\")\n",
        "    print(f\"PARTS    : {len(df_parts)} rows\")\n",
        "    print(f\"BASELINE : {len(df_alloc_baseline)} occupied bins\")\n",
        "\n",
        "    # Baseline dashboard\n",
        "    print(\"\\n[Dashboard] Generating Baseline (Input) dashboard...\")\n",
        "    try:\n",
        "        _ = VIZ.generate_dashboard(df_alloc_raw, df_loc_raw, df_parts_raw, title=\"Baseline (Input)\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Info] Baseline dashboard skipped (schema mismatch is OK): {e}\")\n",
        "\n",
        "    # Train RL (Guide reward)\n",
        "    print(\"\\nTraining RL (Guide-parity reward, zone actions, box-level)...\")\n",
        "    rl = RLRelocator(\n",
        "        df_parts=df_parts,\n",
        "        df_loc=df_loc,\n",
        "        df_alloc_baseline=df_alloc_baseline,\n",
        "        entrance=entrance,\n",
        "        max_dist_possible=max_dist_possible,\n",
        "        max_x_dim=max_x_dim\n",
        "    )\n",
        "    rl.train(\n",
        "        episodes=6000,\n",
        "        alpha=0.25,\n",
        "        epsilon=1.0,\n",
        "        epsilon_decay=0.996,\n",
        "        epsilon_min=0.06\n",
        "    )\n",
        "\n",
        "    # Optimize (BOX-LEVEL)\n",
        "    print(\"\\nGenerating optimized allocation (BOX-LEVEL, single SKU per bin, SKU can span bins)...\")\n",
        "    df_solution = rl.optimize_from_baseline()\n",
        "\n",
        "    # Coverage info (now compares total boxes)\n",
        "    parts_pos = df_parts[df_parts[\"BOXES_ON_HAND\"] > 0][[\"ITEM_ID\", \"BOXES_ON_HAND\"]].copy()\n",
        "    placed_boxes = df_solution.groupby(\"ITEM_ID\")[\"QTY_ALLOCATED\"].sum() if not df_solution.empty else pd.Series(dtype=int)\n",
        "\n",
        "    missing_skus = []\n",
        "    for _, rr in parts_pos.iterrows():\n",
        "        sku = str(rr[\"ITEM_ID\"])\n",
        "        need = int(rr[\"BOXES_ON_HAND\"])\n",
        "        got = int(placed_boxes.get(sku, 0))\n",
        "        if got < need:\n",
        "            missing_skus.append((sku, need, got))\n",
        "\n",
        "    print(f\"Placed bins: {len(df_solution)}\")\n",
        "    if missing_skus:\n",
        "        print(\"WARNING: Some SKUs could not be fully allocated (likely physical infeasibility with remaining bins):\")\n",
        "        for sku, need, got in missing_skus:\n",
        "            print(f\"  SKU {sku}: need {need}, placed {got}\")\n",
        "    else:\n",
        "        print(\"Coverage OK: all SKUs with BOXES_ON_HAND>0 were fully allocated.\")\n",
        "\n",
        "    # Export validator-compliant CSV\n",
        "    print(\"\\nBuilding validated output CSV (authoritative geometry + volumes)...\")\n",
        "    df_out = build_validated_output(df_solution, df_loc, df_parts)\n",
        "    df_out.to_csv(OUTPUT_ALLOC_FILE, index=False)\n",
        "    print(f\"[OK] Saved optimized allocations to: {OUTPUT_ALLOC_FILE}\")\n",
        "    print(df_out.head(10))\n",
        "\n",
        "    # Optimized dashboard (AFTER RL)\n",
        "    print(\"\\n[Dashboard] Generating RL Optimized (Output) dashboard...\")\n",
        "    try:\n",
        "        df_out_dash = df_out.copy()\n",
        "        df_out_dash[\"ITEM_ID\"] = df_out_dash[\"ITEM_ID\"].astype(str)\n",
        "\n",
        "        df_parts_raw_dash = df_parts_raw.copy()\n",
        "        df_parts_raw_dash[\"ITEM_ID\"] = df_parts_raw_dash[\"ITEM_ID\"].astype(str)\n",
        "\n",
        "        _ = VIZ.generate_dashboard(df_out_dash, df_loc_raw, df_parts_raw_dash, title=\"RL Optimized (Output)\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Warn] Optimized dashboard failed: {e}\")\n",
        "        print(\"This does NOT affect validation output CSV correctness.\")\n",
        "\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "import importlib.util\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f814383b",
      "metadata": {
        "id": "f814383b"
      },
      "source": [
        "## 6) Load the final engine (for generating RL training and reward plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c37c2d",
      "metadata": {
        "id": "b7c37c2d"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"rl_engine_final\", \"/content/rl_engine_final.py\")\n",
        "rlmod = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(rlmod)\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b26be63a",
      "metadata": {
        "id": "b26be63a"
      },
      "source": [
        "## 7) Monkey-patch train() to record reward per episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5097db94",
      "metadata": {
        "id": "5097db94"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "orig_train = rlmod.RLRelocator.train  # keep original\n",
        "\n",
        "def train_with_history(self, episodes=6000, alpha=0.25, epsilon=1.0, epsilon_decay=0.996, epsilon_min=0.06):\n",
        "    items = self.parts[self.parts[\"BOXES_ON_HAND\"] > 0][\"ITEM_ID\"].astype(str).tolist()\n",
        "    if not items:\n",
        "        raise ValueError(\"No items with BOXES_ON_HAND > 0. Cannot train.\")\n",
        "\n",
        "    loc_ids = self.loc[\"loc_inst_code\"].astype(str).tolist()\n",
        "    history = []\n",
        "\n",
        "    for ep in range(int(episodes)):\n",
        "        item_id = str(random.choice(items))\n",
        "        state = self._get_state(item_id)\n",
        "\n",
        "        # epsilon-greedy zone action\n",
        "        if random.random() < epsilon:\n",
        "            a_idx = random.randrange(len(self.actions))\n",
        "        else:\n",
        "            a_idx = int(np.argmax(self._Q_row(state)))\n",
        "        action = self.actions[a_idx]\n",
        "\n",
        "        # random occupancy snapshot (single SKU per bin)\n",
        "        bin_sku_map = {l: None for l in loc_ids}\n",
        "        bin_qty_map = {l: 0 for l in loc_ids}\n",
        "\n",
        "        occ_n = max(0, int(0.15 * len(loc_ids)))\n",
        "        occ_bins = random.sample(loc_ids, k=occ_n)\n",
        "\n",
        "        for b in occ_bins:\n",
        "            sku = str(random.choice(items))\n",
        "            if bin_sku_map[b] is not None:\n",
        "                continue\n",
        "\n",
        "            loc_row = self.loc_dict[str(b)] if hasattr(self, \"loc_dict\") else self.loc_idx.loc[str(b)]\n",
        "            part_row = self.part_dict[str(sku)] if hasattr(self, \"part_dict\") else self.part_idx.loc[str(sku)]\n",
        "\n",
        "            if bool(part_row[\"IS_HEAVY\"]) and float(loc_row[\"z\"]) > 1500.0:\n",
        "                continue\n",
        "\n",
        "            geom1 = geom_solve_capacity_and_layout(loc_row, part_row, qty=1)\n",
        "            if geom1 is None:\n",
        "                continue\n",
        "\n",
        "            max_units = int(geom1[\"MAX_UNITS\"])\n",
        "            if max_units <= 0:\n",
        "                continue\n",
        "\n",
        "            qty = random.randint(1, min(3, max_units))\n",
        "            bin_sku_map[b] = sku\n",
        "            bin_qty_map[b] = qty\n",
        "\n",
        "        best = self._pick_best_bin_for_action(item_id, action, bin_sku_map, bin_qty_map, search_cap=180)\n",
        "\n",
        "        q = self._Q_row(state)[a_idx]\n",
        "        if best is None:\n",
        "            reward = -5.0\n",
        "            self._Q_row(state)[a_idx] = q + alpha * (reward - q)\n",
        "        else:\n",
        "            _, _, reward = best\n",
        "            self._Q_row(state)[a_idx] = q + alpha * (reward - q)\n",
        "\n",
        "        history.append(float(reward))\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    self.training_history = history\n",
        "    return self.Q\n",
        "\n",
        "# Replace train at runtime (file stays unchanged)\n",
        "rlmod.RLRelocator.train = train_with_history\n",
        "\n",
        "print(\"Patched: RLRelocator.train now records training_history (file unchanged).\")\n",
        "\n",
        "df_loc_raw   = read_csv_semicolon_if_needed(LOCATIONS_FILE)\n",
        "df_parts_raw = read_csv_semicolon_if_needed(PARTS_FILE)\n",
        "df_alloc_raw = read_csv_semicolon_if_needed(ALLOC_BASELINE_FILE)\n",
        "\n",
        "df_loc   = normalize_locations(df_loc_raw)\n",
        "df_parts = normalize_parts(df_parts_raw)\n",
        "df_alloc_baseline = normalize_allocations_baseline(df_alloc_raw)\n",
        "\n",
        "df_parts = apply_dynamic_abc(df_parts)\n",
        "df_loc, entrance, max_dist_possible, max_x_dim = build_guide_zones(df_loc)\n",
        "\n",
        "rl = rlmod.RLRelocator(df_parts, df_loc, df_alloc_baseline, entrance, max_dist_possible, max_x_dim)\n",
        "rl.train(episodes=6000, alpha=0.25, epsilon=1.0, epsilon_decay=0.996, epsilon_min=0.06)\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8424662f",
      "metadata": {
        "id": "8424662f"
      },
      "source": [
        "## 8) Plot training curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d43c519",
      "metadata": {
        "id": "8d43c519"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------\n",
        "\n",
        "hist = rl.training_history\n",
        "window = 50\n",
        "smooth = [\n",
        "    sum(hist[max(0, i-window+1):i+1]) / len(hist[max(0, i-window+1):i+1])\n",
        "    for i in range(len(hist))\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(hist, alpha=0.2, label=\"Reward (raw)\")\n",
        "plt.plot(smooth, label=f\"Reward (moving avg, window={window})\")\n",
        "plt.title(\"RL Training Progress (per-episode reward)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
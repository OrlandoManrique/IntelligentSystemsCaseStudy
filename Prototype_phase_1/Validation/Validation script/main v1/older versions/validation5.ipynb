{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a84296f-df2c-4d62-8268-29959ddd9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 15:48:43] Output folder 'validation_results' ready.\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Loading LOCATIONS (locations_dummy.csv) ---\u001b[0m\n",
      "[2025-12-28 15:48:43] SUCCESS: LOCATIONS loaded (357 rows).\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Loading ALLOCATIONS (allocations.csv) ---\u001b[0m\n",
      "[2025-12-28 15:48:43] SUCCESS: ALLOCATIONS loaded (10 rows).\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Loading PARTS (synthetic_parts_generated.csv) ---\u001b[0m\n",
      "[2025-12-28 15:48:43] SUCCESS: PARTS loaded (75 rows).\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Check: Volume Data Integrity ---\u001b[0m\n",
      "[2025-12-28 15:48:43] Starting Volume Data...\n",
      "[2025-12-28 15:48:43]    Est. time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 6815.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 15:48:43] \u001b[92m\u001b[1mPASS\u001b[0m: Stated volumes match location dimensions.\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Check: Inventory Balance (Partial Unallocated) ---\u001b[0m\n",
      "[2025-12-28 15:48:43] \u001b[91m\u001b[1mFAIL\u001b[0m: Found 75 SKUs with inventory mismatches.\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Check: Grid Math ---\u001b[0m\n",
      "[2025-12-28 15:48:43] Starting Grid Math...\n",
      "[2025-12-28 15:48:43]    Est. time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3936.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 15:48:43] \u001b[92m\u001b[1mPASS\u001b[0m: Grid Math consistent.\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Check: Rigid Body ---\u001b[0m\n",
      "[2025-12-28 15:48:43] Starting Rigid Body...\n",
      "[2025-12-28 15:48:43]    Est. time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2880.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 15:48:43] \u001b[92m\u001b[1mPASS\u001b[0m: Rigid Body dimensions valid.\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Check: Stack vs Bin Dimensions ---\u001b[0m\n",
      "[2025-12-28 15:48:43] Starting Stack Fit...\n",
      "[2025-12-28 15:48:43]    Est. time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 4837.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 15:48:43] \u001b[92m\u001b[1mPASS\u001b[0m: All stacks fit within bins.\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Check: Bin Overlaps ---\u001b[0m\n",
      "[2025-12-28 15:48:43] Starting Bin Overlap...\n",
      "[2025-12-28 15:48:43]    Est. time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 357/357 [00:00<00:00, 101214.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 15:48:43] \u001b[92m\u001b[1mPASS\u001b[0m: No bin overlaps.\n",
      "[2025-12-28 15:48:43] \u001b[96m--- Generating Utilization Plots ---\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 15:48:44] Plot saved to validation_results/visual_utilization_max.png\n",
      "[2025-12-28 15:48:44] Plot saved to validation_results/visual_utilization_min.png\n",
      "[2025-12-28 15:48:44] Validation Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & SCHEMA\n",
    "# ==========================================\n",
    "\n",
    "LOCATIONS_FILE = 'locations_dummy.csv'\n",
    "ALLOCATIONS_FILE = 'allocations.csv'\n",
    "PARTS_FILE = 'synthetic_parts_generated.csv'\n",
    "OUTPUT_DIR = 'validation_results'\n",
    "MAX_EXECUTION_TIME_SEC = 300\n",
    "\n",
    "REQUIRED_SCHEMA = {\n",
    "    'LOCATIONS': {\n",
    "        'file': LOCATIONS_FILE,\n",
    "        'columns': ['loc_inst_code', 'width', 'depth', 'height', 'x', 'y', 'z']\n",
    "    },\n",
    "    'ALLOCATIONS': {\n",
    "        'file': ALLOCATIONS_FILE,\n",
    "        'columns': ['LOCATION_ID', 'SKU', \n",
    "                    'GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                    'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                    'MAX_UNITS', 'CURRENT_STOCK', 'INIT_UNITS', \n",
    "                    'LOCATION_VOL_MM3'] # Added for Volume Integrity\n",
    "    },\n",
    "    'PARTS': {\n",
    "        'file': PARTS_FILE,\n",
    "        'columns': ['ITEM_ID', 'LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG', \n",
    "                    'BOXES_ON_HAND', 'QTY_PER_BOX'] # Added for Inventory Balance\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOGGING & UTILS\n",
    "# ==========================================\n",
    "\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    CYAN = '\\033[96m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "report_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    report_buffer.append(f\"[{timestamp}] {message}\")\n",
    "    \n",
    "    c_msg = message\n",
    "    if \"PASS\" in message: c_msg = message.replace(\"PASS\", f\"{Colors.GREEN}{Colors.BOLD}PASS{Colors.RESET}\")\n",
    "    elif \"FAIL\" in message: c_msg = message.replace(\"FAIL\", f\"{Colors.RED}{Colors.BOLD}FAIL{Colors.RESET}\")\n",
    "    elif \"CRITICAL\" in message: c_msg = f\"{Colors.RED}{Colors.BOLD}{message}{Colors.RESET}\"\n",
    "    elif \"WARN\" in message: c_msg = message.replace(\"WARN\", f\"{Colors.YELLOW}{Colors.BOLD}WARN{Colors.RESET}\")\n",
    "    elif \"---\" in message: c_msg = f\"{Colors.CYAN}{message}{Colors.RESET}\"\n",
    "    \n",
    "    print(f\"[{timestamp}] {c_msg}\")\n",
    "\n",
    "def setup_environment():\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    log(f\"Output folder '{OUTPUT_DIR}' ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "\n",
    "def load_and_validate_dataset(key, config):\n",
    "    filepath = config['file']\n",
    "    log(f\"--- Loading {key} ({filepath}) ---\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        log(f\"CRITICAL: File {filepath} not found.\")\n",
    "        return None, False\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=None, engine='python', dtype=str)\n",
    "        df.columns = df.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "        \n",
    "        # Add Original Row ID\n",
    "        df['ROW_ID'] = df.index + 2 \n",
    "\n",
    "        missing = [c for c in config['columns'] if c not in df.columns]\n",
    "        if missing:\n",
    "            log(f\"CRITICAL SCHEMA ERROR in {key}. Missing: {missing}\")\n",
    "            return df, False\n",
    "        \n",
    "        log(f\"SUCCESS: {key} loaded ({len(df)} rows).\")\n",
    "        return df, True\n",
    "    except Exception as e:\n",
    "        log(f\"CRITICAL ERROR reading {filepath}: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def convert_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    setup_environment()\n",
    "    datasets = {}\n",
    "    valid_flags = {}\n",
    "    \n",
    "    # Locations\n",
    "    df, valid = load_and_validate_dataset('LOCATIONS', REQUIRED_SCHEMA['LOCATIONS'])\n",
    "    if valid: df = convert_numeric(df, ['width', 'depth', 'height', 'x', 'y', 'z'])\n",
    "    datasets['LOCATIONS'] = df\n",
    "    valid_flags['LOCATIONS'] = valid\n",
    "\n",
    "    # Allocations\n",
    "    df, valid = load_and_validate_dataset('ALLOCATIONS', REQUIRED_SCHEMA['ALLOCATIONS'])\n",
    "    if valid: \n",
    "        df = convert_numeric(df, ['GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                                  'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                                  'MAX_UNITS', 'CURRENT_STOCK', 'INIT_UNITS',\n",
    "                                  'LOCATION_VOL_MM3'])\n",
    "    datasets['ALLOCATIONS'] = df\n",
    "    valid_flags['ALLOCATIONS'] = valid\n",
    "\n",
    "    # Parts\n",
    "    df, valid = load_and_validate_dataset('PARTS', REQUIRED_SCHEMA['PARTS'])\n",
    "    if valid: df = convert_numeric(df, ['LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG', \n",
    "                                        'BOXES_ON_HAND', 'QTY_PER_BOX'])\n",
    "    datasets['PARTS'] = df\n",
    "    valid_flags['PARTS'] = valid\n",
    "    \n",
    "    return datasets, valid_flags\n",
    "\n",
    "# ==========================================\n",
    "# 4. VALIDATION LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def estimate_and_sample(df, check_name, validation_func, *args):\n",
    "    log(f\"Starting {check_name}...\")\n",
    "    total = len(df)\n",
    "    test_size = min(1000, total)\n",
    "    if test_size == 0: return pd.DataFrame()\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        _ = validation_func(df.head(test_size), *args, quiet=True)\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in {check_name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    dur = time.time() - t0\n",
    "    if dur == 0: dur = 0.001\n",
    "    est_time = (dur / test_size) * total\n",
    "    log(f\"   Est. time: {est_time:.2f}s\")\n",
    "\n",
    "    if est_time > MAX_EXECUTION_TIME_SEC:\n",
    "        safe_rows = int((MAX_EXECUTION_TIME_SEC / dur) * test_size)\n",
    "        log(f\"   WARN: Time limit exceeded. Sampling {safe_rows} rows.\")\n",
    "        df_to_process = df.sample(n=safe_rows, random_state=42)\n",
    "    else:\n",
    "        df_to_process = df\n",
    "\n",
    "    return validation_func(df_to_process, *args, quiet=False)\n",
    "\n",
    "# --- CHECK: Stack vs Bin Dimensions (Geometric Fit) ---\n",
    "def check_stack_fit(df_alloc, df_loc, quiet=False):\n",
    "    merged = df_alloc.merge(df_loc, left_on='LOCATION_ID', right_on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    issues = []\n",
    "    \n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue \n",
    "\n",
    "        stack_x = row['GRID_X'] * row['ORIENT_X_MM']\n",
    "        stack_y = row['GRID_Y'] * row['ORIENT_Y_MM']\n",
    "        stack_z = row['GRID_Z'] * row['ORIENT_Z_MM']\n",
    "        \n",
    "        tol = 1.0\n",
    "        if (stack_x > row['width'] + tol) or (stack_y > row['depth'] + tol) or (stack_z > row['height'] + tol):\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'SKU': row['SKU'],\n",
    "                'Issue': 'Stack Exceeds Bin',\n",
    "                'Details': f\"Bin: {row['width']}x{row['depth']}x{row['height']} | Stack: {stack_x:.1f}x{stack_y:.1f}x{stack_z:.1f}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Inventory Balance (Partial Unallocated) ---\n",
    "def check_inventory_balance(df_alloc, df_parts):\n",
    "    log(\"--- Check: Inventory Balance (Partial Unallocated) ---\")\n",
    "    \n",
    "    # 1. Calculate Total Demand (Inventory) per SKU from Parts\n",
    "    # Assuming 'BOXES_ON_HAND' is count of boxes and we need total units\n",
    "    df_parts['TOTAL_DEMAND'] = df_parts['BOXES_ON_HAND'] * df_parts['QTY_PER_BOX']\n",
    "    demand_map = df_parts.set_index('ITEM_ID')['TOTAL_DEMAND'].to_dict()\n",
    "    \n",
    "    # 2. Calculate Total Allocated per SKU\n",
    "    # Using 'INIT_UNITS' as the allocated amount\n",
    "    alloc_sums = df_alloc.groupby('SKU')['INIT_UNITS'].sum().to_dict()\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    for sku, demand in demand_map.items():\n",
    "        allocated = alloc_sums.get(sku, 0)\n",
    "        \n",
    "        # Check for under-allocation\n",
    "        if allocated < demand:\n",
    "            issues.append({\n",
    "                'SKU': sku,\n",
    "                'Issue': 'Partial/No Allocation',\n",
    "                'Demand': demand,\n",
    "                'Allocated': allocated,\n",
    "                'Missing': demand - allocated\n",
    "            })\n",
    "        # Check for over-allocation (Ghost Inventory)\n",
    "        elif allocated > demand:\n",
    "            issues.append({\n",
    "                'SKU': sku,\n",
    "                'Issue': 'Over-Allocation',\n",
    "                'Demand': demand,\n",
    "                'Allocated': allocated,\n",
    "                'Excess': allocated - demand\n",
    "            })\n",
    "            \n",
    "    df_issues = pd.DataFrame(issues)\n",
    "    \n",
    "    if not df_issues.empty:\n",
    "        log(f\"FAIL: Found {len(df_issues)} SKUs with inventory mismatches.\")\n",
    "        df_issues.to_csv(f\"{OUTPUT_DIR}/fail_inventory_balance.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Total allocated units match inventory on hand.\")\n",
    "\n",
    "# --- CHECK: Volume Data Integrity ---\n",
    "def check_volume_data_integrity(df_alloc, df_loc, quiet=False):\n",
    "    \"\"\"\n",
    "    Checks if LOCATION_VOL_MM3 in Allocations matches actual Location Dims.\n",
    "    \"\"\"\n",
    "    merged = df_alloc.merge(df_loc, left_on='LOCATION_ID', right_on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    issues = []\n",
    "    \n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue\n",
    "        \n",
    "        real_vol = row['width'] * row['depth'] * row['height']\n",
    "        stated_vol = row['LOCATION_VOL_MM3']\n",
    "        \n",
    "        # Allow small float tolerance\n",
    "        if abs(real_vol - stated_vol) > 1.0:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'Issue': 'Volume Data Error',\n",
    "                'Details': f\"Stated: {stated_vol} != Real: {real_vol}\"\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Grid Math ---\n",
    "def check_grid_consistency(df_alloc, quiet=False):\n",
    "    issues = []\n",
    "    iterator = tqdm(df_alloc.iterrows(), total=df_alloc.shape[0]) if not quiet else df_alloc.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        grid_cap = row['GRID_X'] * row['GRID_Y'] * row['GRID_Z']\n",
    "        # Compare Grid Capacity to INIT_UNITS or MAX_UNITS. \n",
    "        # Usually MAX_UNITS is capacity, INIT_UNITS is current.\n",
    "        # Strict check: Grid Size should match Max Capacity.\n",
    "        if abs(grid_cap - row['MAX_UNITS']) > 0.1:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'Issue': 'Grid Math Mismatch',\n",
    "                'Details': f\"Grid ({grid_cap}) != MaxUnits ({row['MAX_UNITS']})\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Rigid Body ---\n",
    "def check_rigid_body(df_alloc, df_parts, quiet=False):\n",
    "    merged = df_alloc.merge(df_parts, left_on='SKU', right_on='ITEM_ID', how='left', suffixes=('', '_PART'))\n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['LEN_MM']): continue\n",
    "        od = sorted([row['ORIENT_X_MM'], row['ORIENT_Y_MM'], row['ORIENT_Z_MM']])\n",
    "        pd_ = sorted([row['LEN_MM'], row['WID_MM'], row['DEP_MM']])\n",
    "        if any(abs(o - p) > 0.5 for o, p in zip(od, pd_)):\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'Issue': 'Dimensions Morphing',\n",
    "                'Details': f\"Alloc {od} vs Part {pd_}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Bin Overlap ---\n",
    "def check_bin_overlaps(df_loc, quiet=False):\n",
    "    df = df_loc.copy()\n",
    "    df['x2'] = df['x'] + df['width']\n",
    "    df['y2'] = df['y'] + df['depth']\n",
    "    df['z2'] = df['z'] + df['height']\n",
    "    df.sort_values('x', inplace=True)\n",
    "    \n",
    "    issues = []\n",
    "    data = df.to_dict('records')\n",
    "    n = len(data)\n",
    "    if n == 0: return pd.DataFrame()\n",
    "\n",
    "    iter_range = tqdm(range(n), total=n) if not quiet else range(n)\n",
    "    for i in iter_range:\n",
    "        a = data[i]\n",
    "        for j in range(i + 1, n):\n",
    "            b = data[j]\n",
    "            if b['x'] >= a['x2'] - 0.1: break\n",
    "            if (a['y'] < b['y2']) and (a['y2'] > b['y']) and (a['z'] < b['z2']) and (a['z2'] > b['z']):\n",
    "                issues.append({\n",
    "                    'LOC_A': a['loc_inst_code'], 'LOC_B': b['loc_inst_code'],\n",
    "                    'Issue': 'Physical Overlap', 'Details': f\"A({a['x']}) vs B({b['x']})\"\n",
    "                })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# ==========================================\n",
    "# 5. VISUALIZATION\n",
    "# ==========================================\n",
    "\n",
    "def plot_single_bin(row, title_prefix, filename_tag):\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=[2, 1])\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax_text = fig.add_subplot(gs[:, 1])\n",
    "    ax_text.axis('off')\n",
    "\n",
    "    bin_w, bin_h, bin_d = row['width'], row['height'], row['depth']\n",
    "    item_w, item_h, item_d = row['ORIENT_X_MM'], row['ORIENT_Z_MM'], row['ORIENT_Y_MM']\n",
    "    gx, gy, gz = int(row['GRID_X']), int(row['GRID_Y']), int(row['GRID_Z'])\n",
    "    \n",
    "    # 1. FRONT\n",
    "    ax1.add_patch(patches.Rectangle((0, 0), bin_w, bin_h, fill=False, edgecolor='red', lw=3, label='Bin'))\n",
    "    for z in range(gz):\n",
    "        for x in range(gx):\n",
    "            ax1.add_patch(patches.Rectangle((x*item_w, z*item_h), item_w, item_h, \n",
    "                                            lw=1, ec='black', fc='skyblue', alpha=0.6))\n",
    "    ax1.set_title(\"FRONT VIEW (X-Z)\", fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlim(-50, bin_w + 50)\n",
    "    ax1.set_ylim(-50, bin_h + 50)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    # 2. TOP\n",
    "    ax2.add_patch(patches.Rectangle((0, 0), bin_w, bin_d, fill=False, edgecolor='red', lw=3))\n",
    "    for y in range(gy):\n",
    "        for x in range(gx):\n",
    "            ax2.add_patch(patches.Rectangle((x*item_w, y*item_d), item_w, item_d, \n",
    "                                            lw=1, ec='black', fc='orange', alpha=0.6))\n",
    "    ax2.set_title(\"TOP VIEW (X-Y)\", fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlim(-50, bin_w + 50)\n",
    "    ax2.set_ylim(-50, bin_d + 50)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    # 3. TEXT\n",
    "    info_text = (\n",
    "        f\"REPORT: {title_prefix}\\n\"\n",
    "        f\"----------------------------------------\\n\"\n",
    "        f\"ALLOCATION ROW:   {row['ROW_ID']}\\n\"\n",
    "        f\"LOCATION ID:      {row['LOCATION_ID']}\\n\"\n",
    "        f\"ITEM SKU:         {row['SKU']}\\n\"\n",
    "        f\"----------------------------------------\\n\"\n",
    "        f\"METRICS:\\n\"\n",
    "        f\"  Utilization:    {row['UTILIZATION_PCT']:.2f} %\\n\"\n",
    "        f\"  Total Units:    {row['MAX_UNITS']}\\n\\n\"\n",
    "        f\"DIMENSIONS (mm):\\n\"\n",
    "        f\"  Bin (WxDxH):    {bin_w} x {bin_d} x {bin_h}\\n\"\n",
    "        f\"  Item Orient:    {row['ORIENT_X_MM']} x {row['ORIENT_Y_MM']} x {row['ORIENT_Z_MM']}\\n\"\n",
    "        f\"  Stack Total:    {gx*item_w:.1f} x {gy*item_d:.1f} x {gz*item_h:.1f}\\n\\n\"\n",
    "        f\"GRID CONFIG:\\n\"\n",
    "        f\"  Cols (X):       {gx}\\n\"\n",
    "        f\"  Rows (Y):       {gy}\\n\"\n",
    "        f\"  Layers (Z):     {gz}\\n\\n\"\n",
    "        f\"VOLUMES (mm3):\\n\"\n",
    "        f\"  Bin Volume:     {row['LOC_VOL']:,.0f}\\n\"\n",
    "        f\"  Bulk Item Vol:  {row['STACK_VOL']:,.0f}\\n\"\n",
    "        f\"  Single Item:    {row['ORIENT_X_MM']*row['ORIENT_Y_MM']*row['ORIENT_Z_MM']:,.0f}\"\n",
    "    )\n",
    "    \n",
    "    ax_text.text(0.05, 0.95, info_text, transform=ax_text.transAxes, fontsize=12, \n",
    "                 verticalalignment='top', family='monospace', \n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"{OUTPUT_DIR}/{filename_tag}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    log(f\"Plot saved to {save_path}\")\n",
    "\n",
    "def visualize_utilization_extremes(datasets):\n",
    "    log(\"--- Generating Utilization Plots ---\")\n",
    "    df_alloc = datasets['ALLOCATIONS']\n",
    "    df_loc = datasets['LOCATIONS']\n",
    "    \n",
    "    merged = df_alloc.merge(df_loc, left_on='LOCATION_ID', right_on='loc_inst_code', how='inner', suffixes=('', '_LOC'))\n",
    "    if merged.empty: return\n",
    "\n",
    "    merged['STACK_VOL'] = (merged['GRID_X'] * merged['ORIENT_X_MM']) * \\\n",
    "                          (merged['GRID_Y'] * merged['ORIENT_Y_MM']) * \\\n",
    "                          (merged['GRID_Z'] * merged['ORIENT_Z_MM'])\n",
    "    merged['LOC_VOL'] = merged['width'] * merged['depth'] * merged['height']\n",
    "    merged = merged[merged['LOC_VOL'] > 0]\n",
    "    merged['UTILIZATION_PCT'] = (merged['STACK_VOL'] / merged['LOC_VOL']) * 100\n",
    "\n",
    "    # 1. Top\n",
    "    top_row = merged.sort_values(by='UTILIZATION_PCT', ascending=False).iloc[0]\n",
    "    plot_single_bin(top_row, \"HIGHEST UTILIZATION\", \"visual_utilization_max\")\n",
    "\n",
    "    # 2. Lowest (active)\n",
    "    active = merged[merged['UTILIZATION_PCT'] > 0]\n",
    "    if not active.empty:\n",
    "        low_row = active.sort_values(by='UTILIZATION_PCT', ascending=True).iloc[0]\n",
    "        plot_single_bin(low_row, \"LOWEST UTILIZATION\", \"visual_utilization_min\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def run_full_diagnostic():\n",
    "    datasets, valid_flags = load_all_data()\n",
    "    \n",
    "    if not all(valid_flags.values()):\n",
    "        log(\"STOPPING: Schema Errors.\")\n",
    "        return\n",
    "\n",
    "    # 1. Volume Data Integrity (Stated vs Real)\n",
    "    log(\"--- Check: Volume Data Integrity ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Volume Data\", check_volume_data_integrity, datasets['LOCATIONS'])\n",
    "    if not res.empty:\n",
    "        log(f\"FAIL: {len(res)} Volume Data mismatches (Stated vs Real).\")\n",
    "        res.to_csv(f\"{OUTPUT_DIR}/fail_volume_data.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Stated volumes match location dimensions.\")\n",
    "\n",
    "    # 2. Inventory Balance\n",
    "    check_inventory_balance(datasets['ALLOCATIONS'], datasets['PARTS'])\n",
    "\n",
    "    # 3. Grid Math\n",
    "    log(\"--- Check: Grid Math ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Grid Math\", check_grid_consistency)\n",
    "    if not res.empty:\n",
    "        log(f\"FAIL: {len(res)} Grid Math errors.\")\n",
    "        res.to_csv(f\"{OUTPUT_DIR}/fail_grid_math.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Grid Math consistent.\")\n",
    "\n",
    "    # 4. Rigid Body\n",
    "    log(\"--- Check: Rigid Body ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Rigid Body\", check_rigid_body, datasets['PARTS'])\n",
    "    if not res.empty:\n",
    "        log(f\"FAIL: {len(res)} Rigid Body errors.\")\n",
    "        res.to_csv(f\"{OUTPUT_DIR}/fail_rigid_body.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Rigid Body dimensions valid.\")\n",
    "\n",
    "    # 5. Stack vs Bin (Geometric Fit)\n",
    "    log(\"--- Check: Stack vs Bin Dimensions ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Stack Fit\", check_stack_fit, datasets['LOCATIONS'])\n",
    "    if not res.empty:\n",
    "        log(f\"FAIL: {len(res)} allocations exceed bin dimensions.\")\n",
    "        res.to_csv(f\"{OUTPUT_DIR}/fail_stack_fit.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: All stacks fit within bins.\")\n",
    "\n",
    "    # 6. Bin Overlaps\n",
    "    log(\"--- Check: Bin Overlaps ---\")\n",
    "    res = estimate_and_sample(datasets['LOCATIONS'], \"Bin Overlap\", check_bin_overlaps)\n",
    "    if not res.empty:\n",
    "        log(f\"FAIL: {len(res)} locations overlap physically.\")\n",
    "        res.to_csv(f\"{OUTPUT_DIR}/fail_bin_overlap.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: No bin overlaps.\")\n",
    "\n",
    "    # 7. Visualization\n",
    "    visualize_utilization_extremes(datasets)\n",
    "\n",
    "    with open(f\"{OUTPUT_DIR}/validation_report.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(report_buffer))\n",
    "    log(\"Validation Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_diagnostic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae130f-32a9-4d2c-a4db-fd37b1abbb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93c9b6-2c69-41bf-afc9-ceb6c26841b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

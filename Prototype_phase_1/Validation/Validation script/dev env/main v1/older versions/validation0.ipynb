{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a84296f-df2c-4d62-8268-29959ddd9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:06:51] Output folder 'validation_results' ready.\n",
      "[2025-12-28 14:06:51] --- Loading LOCATIONS (locations_dummy.csv) ---\n",
      "[2025-12-28 14:06:51] SUCCESS: LOCATIONS loaded and matches schema (357 rows).\n",
      "[2025-12-28 14:06:51] --- Loading ALLOCATIONS (allocations.csv) ---\n",
      "[2025-12-28 14:06:51] SUCCESS: ALLOCATIONS loaded and matches schema (10 rows).\n",
      "[2025-12-28 14:06:51] --- Loading PARTS (synthetic_parts_generated.csv) ---\n",
      "[2025-12-28 14:06:51] SUCCESS: PARTS loaded and matches schema (75 rows).\n",
      "[2025-12-28 14:06:51] \n",
      "All Schemas Valid. Proceeding to Logic Checks...\n",
      "\n",
      "[2025-12-28 14:06:51] --- Duplicate Checks ---\n",
      "[2025-12-28 14:06:51] PASS: No duplicates in LOCATIONS.\n",
      "[2025-12-28 14:06:51] PASS: No duplicates in ALLOCATIONS.\n",
      "[2025-12-28 14:06:51] PASS: No duplicates in PARTS.\n",
      "[2025-12-28 14:06:51] --- Checking Referential Integrity ---\n",
      "[2025-12-28 14:06:51] PASS: All Allocations match valid Locations.\n",
      "[2025-12-28 14:06:51] FAIL: 10 Allocations point to unknown SKUs.\n",
      "[2025-12-28 14:06:51] --- Geometric Fit Analysis ---\n",
      "[2025-12-28 14:06:51] Starting Check: Geometric Fit...\n",
      "[2025-12-28 14:06:51]    Estimated time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 6707.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:06:51] PASS: All checked allocations fit geometrically.\n",
      "[2025-12-28 14:06:51] --- Checking Unallocated Items ---\n",
      "[2025-12-28 14:06:51] INFO: 75 items are NOT allocated. Checking if they physically fit in empty bins...\n",
      "[2025-12-28 14:06:51] WARN: Unallocated items exist, and 20 of sampled items (20) physically fit in available empty bins.\n",
      "[2025-12-28 14:06:51]       -> This suggests logic errors in the allocation algorithm (why weren't these placed?).\n",
      "[2025-12-28 14:06:51] \n",
      "Validation finished. Report saved to validation_results/validation_report.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & SCHEMA DEFINITION\n",
    "# ==========================================\n",
    "\n",
    "# File Names\n",
    "LOCATIONS_FILE = 'locations_dummy.csv'\n",
    "ALLOCATIONS_FILE = 'allocations.csv'\n",
    "PARTS_FILE = 'synthetic_parts_generated.csv'\n",
    "\n",
    "# STRICT Schema Definition\n",
    "# These are the columns strictly required to run the Full Validation as requested.\n",
    "# If these are missing, we will flag it as a Critical Data Error.\n",
    "REQUIRED_SCHEMA = {\n",
    "    'LOCATIONS': {\n",
    "        'file': LOCATIONS_FILE,\n",
    "        'columns': [\n",
    "            'loc_inst_code', # Key\n",
    "            'width', 'depth', 'height', # Dims\n",
    "            'x', 'y', 'z' # Coordinates\n",
    "        ]\n",
    "    },\n",
    "    'ALLOCATIONS': {\n",
    "        'file': ALLOCATIONS_FILE,\n",
    "        'columns': [\n",
    "            'LOCATION_ID', 'SKU', # Keys\n",
    "            'GRID_X', 'GRID_Y', 'GRID_Z', # Stack Counts\n",
    "            'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', # Orientation Dims\n",
    "            'INIT_UNITS' # Quantity\n",
    "        ]\n",
    "    },\n",
    "    'PARTS': {\n",
    "        'file': PARTS_FILE,\n",
    "        'columns': [\n",
    "            'ITEM_ID', # Key\n",
    "            'LEN_MM', 'WID_MM', 'DEP_MM', # Dims\n",
    "            'WT_KG' # Weight\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = 'validation_results'\n",
    "MAX_EXECUTION_TIME_SEC = 300  # 5 minutes\n",
    "\n",
    "# Report Buffer\n",
    "report_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Prints to console and appends to report buffer.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    msg = f\"[{timestamp}] {message}\"\n",
    "    print(msg)\n",
    "    report_buffer.append(msg)\n",
    "\n",
    "def setup_environment():\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    log(f\"Output folder '{OUTPUT_DIR}' ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA LOADING & STRICT SCHEMA CHECK\n",
    "# ==========================================\n",
    "\n",
    "def load_and_validate_dataset(key, config):\n",
    "    filepath = config['file']\n",
    "    required_cols = config['columns']\n",
    "    \n",
    "    log(f\"--- Loading {key} ({filepath}) ---\")\n",
    "    \n",
    "    # 1. Check File Existence\n",
    "    if not os.path.exists(filepath):\n",
    "        log(f\"CRITICAL FAIL: File {filepath} not found.\")\n",
    "        # Debug helper: print what IS there\n",
    "        log(f\"       Info: Files in directory: {glob.glob('*.*')}\")\n",
    "        return None, False\n",
    "\n",
    "    try:\n",
    "        # 2. Read File (Auto-detect separator, but don't rename columns automatically)\n",
    "        df = pd.read_csv(filepath, sep=None, engine='python', dtype=str)\n",
    "        \n",
    "        # 3. Clean Headers (Remove hidden BOM characters or whitespace)\n",
    "        df.columns = df.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "        \n",
    "        # 4. Strict Schema Validation\n",
    "        missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            log(f\"CRITICAL SCHEMA ERROR in {key}:\")\n",
    "            log(f\"   Missing Required Columns: {missing_cols}\")\n",
    "            log(f\"   Columns Found in file:    {list(df.columns)}\")\n",
    "            log(f\"   -> Action: Rename columns in CSV to match expected schema.\")\n",
    "            return df, False\n",
    "        else:\n",
    "            log(f\"SUCCESS: {key} loaded and matches schema ({len(df)} rows).\")\n",
    "            return df, True\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"CRITICAL ERROR reading {filepath}: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def convert_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    setup_environment()\n",
    "    \n",
    "    datasets = {}\n",
    "    valid_flags = {}\n",
    "    \n",
    "    # Load Locations\n",
    "    df, valid = load_and_validate_dataset('LOCATIONS', REQUIRED_SCHEMA['LOCATIONS'])\n",
    "    if valid:\n",
    "        df = convert_numeric(df, ['width', 'depth', 'height', 'x', 'y', 'z'])\n",
    "    datasets['LOCATIONS'] = df\n",
    "    valid_flags['LOCATIONS'] = valid\n",
    "\n",
    "    # Load Allocations\n",
    "    df, valid = load_and_validate_dataset('ALLOCATIONS', REQUIRED_SCHEMA['ALLOCATIONS'])\n",
    "    if valid:\n",
    "        num_cols = ['GRID_X', 'GRID_Y', 'GRID_Z', 'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', 'INIT_UNITS']\n",
    "        df = convert_numeric(df, num_cols)\n",
    "    datasets['ALLOCATIONS'] = df\n",
    "    valid_flags['ALLOCATIONS'] = valid\n",
    "\n",
    "    # Load Parts\n",
    "    df, valid = load_and_validate_dataset('PARTS', REQUIRED_SCHEMA['PARTS'])\n",
    "    if valid:\n",
    "        df = convert_numeric(df, ['LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG'])\n",
    "    datasets['PARTS'] = df\n",
    "    valid_flags['PARTS'] = valid\n",
    "    \n",
    "    return datasets, valid_flags\n",
    "\n",
    "# ==========================================\n",
    "# 3. VALIDATION LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def estimate_and_sample(df, check_name, validation_func, *args):\n",
    "    \"\"\"\n",
    "    Time estimation wrapper. Random samples if process > 5 mins.\n",
    "    \"\"\"\n",
    "    log(f\"Starting Check: {check_name}...\")\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    test_size = min(1000, total_rows)\n",
    "    \n",
    "    if test_size == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Test Run\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        _ = validation_func(df.head(test_size), *args, quiet=True)\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR executing logic for {check_name}: {e}\")\n",
    "        return pd.DataFrame() # Stop this specific check\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    if duration == 0: duration = 0.001\n",
    "    \n",
    "    estimated_total_time = (duration / test_size) * total_rows\n",
    "    log(f\"   Estimated time: {estimated_total_time:.2f}s\")\n",
    "\n",
    "    if estimated_total_time > MAX_EXECUTION_TIME_SEC:\n",
    "        safe_rows = int((MAX_EXECUTION_TIME_SEC / duration) * test_size)\n",
    "        log(f\"   WARNING: Time limit exceeded. Sampling {safe_rows} rows.\")\n",
    "        df_to_process = df.sample(n=safe_rows, random_state=42)\n",
    "    else:\n",
    "        df_to_process = df\n",
    "\n",
    "    return validation_func(df_to_process, *args, quiet=False)\n",
    "\n",
    "def check_referential_integrity(datasets):\n",
    "    log(\"--- Checking Referential Integrity ---\")\n",
    "    \n",
    "    # Allocations -> Locations\n",
    "    df_alloc = datasets['ALLOCATIONS']\n",
    "    df_loc = datasets['LOCATIONS']\n",
    "    \n",
    "    if df_alloc is not None and df_loc is not None:\n",
    "        orphans = df_alloc[~df_alloc['LOCATION_ID'].isin(df_loc['loc_inst_code'])]\n",
    "        if len(orphans) > 0:\n",
    "            log(f\"FAIL: {len(orphans)} Allocations point to unknown Locations.\")\n",
    "            orphans.to_csv(f\"{OUTPUT_DIR}/integrity_fail_alloc_loc.csv\", index=False)\n",
    "        else:\n",
    "            log(\"PASS: All Allocations match valid Locations.\")\n",
    "            \n",
    "    # Allocations -> Parts\n",
    "    df_parts = datasets['PARTS']\n",
    "    if df_alloc is not None and df_parts is not None:\n",
    "        orphans = df_alloc[~df_alloc['SKU'].isin(df_parts['ITEM_ID'])]\n",
    "        if len(orphans) > 0:\n",
    "            log(f\"FAIL: {len(orphans)} Allocations point to unknown SKUs.\")\n",
    "            orphans.to_csv(f\"{OUTPUT_DIR}/integrity_fail_alloc_sku.csv\", index=False)\n",
    "        else:\n",
    "            log(\"PASS: All Allocations match valid Parts.\")\n",
    "\n",
    "def func_geometric_fit(df_alloc, df_loc, quiet=False):\n",
    "    \"\"\"\n",
    "    STRICT CHECK: Does Grid * Orientation fit in Location?\n",
    "    \"\"\"\n",
    "    merged = df_alloc.merge(df_loc, left_on='LOCATION_ID', right_on='loc_inst_code', how='left')\n",
    "    issues = []\n",
    "    \n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue \n",
    "\n",
    "        # Strict Logic: We rely on the GRID and ORIENT columns.\n",
    "        used_x = row['GRID_X'] * row['ORIENT_X_MM']\n",
    "        used_y = row['GRID_Y'] * row['ORIENT_Y_MM']\n",
    "        used_z = row['GRID_Z'] * row['ORIENT_Z_MM']\n",
    "        \n",
    "        tolerance = 1.0 # 1mm tolerance\n",
    "        \n",
    "        fail_x = used_x > (row['width'] + tolerance)\n",
    "        fail_y = used_y > (row['depth'] + tolerance)\n",
    "        fail_z = used_z > (row['height'] + tolerance)\n",
    "        \n",
    "        if fail_x or fail_y or fail_z:\n",
    "            issues.append({\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'SKU': row['SKU'],\n",
    "                'Issue': 'Does Not Fit',\n",
    "                'Loc_Dims': f\"{row['width']}x{row['depth']}x{row['height']}\",\n",
    "                'Stack_Dims': f\"{used_x:.1f}x{used_y:.1f}x{used_z:.1f}\"\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "def check_unallocated(datasets):\n",
    "    log(\"--- Checking Unallocated Items ---\")\n",
    "    df_parts = datasets['PARTS']\n",
    "    df_alloc = datasets['ALLOCATIONS']\n",
    "    df_loc = datasets['LOCATIONS']\n",
    "    \n",
    "    # Identify Unallocated\n",
    "    all_skus = set(df_parts['ITEM_ID'])\n",
    "    alloc_skus = set(df_alloc['SKU'])\n",
    "    unallocated = all_skus - alloc_skus\n",
    "    \n",
    "    if not unallocated:\n",
    "        log(\"PASS: All items are allocated.\")\n",
    "        return\n",
    "\n",
    "    log(f\"INFO: {len(unallocated)} items are NOT allocated. Checking if they physically fit in empty bins...\")\n",
    "    \n",
    "    # Identify Empty Bins\n",
    "    occupied_locs = set(df_alloc['LOCATION_ID'])\n",
    "    empty_locs = df_loc[~df_loc['loc_inst_code'].isin(occupied_locs)].copy()\n",
    "    \n",
    "    if len(empty_locs) == 0:\n",
    "        log(\"FAIL: Items unallocated, but no empty bins exist.\")\n",
    "        return\n",
    "\n",
    "    # Feasibility Check\n",
    "    # We check if Volume of Part < Volume of Bin AND Max Dim of Part < Max Dim of Bin\n",
    "    sample_unalloc = df_parts[df_parts['ITEM_ID'].isin(list(unallocated)[:20])] # Check first 20\n",
    "    \n",
    "    fits_found = 0\n",
    "    for _, part in sample_unalloc.iterrows():\n",
    "        p_vol = part['LEN_MM'] * part['WID_MM'] * part['DEP_MM']\n",
    "        p_max = max(part['LEN_MM'], part['WID_MM'], part['DEP_MM'])\n",
    "        \n",
    "        # Vectorized check against all empty bins\n",
    "        # 1. Volume fits?\n",
    "        # 2. Max dimension fits? (Rough heuristic for \"can it get inside\")\n",
    "        \n",
    "        # Calculate empty loc volumes\n",
    "        empty_locs['vol'] = empty_locs['width'] * empty_locs['depth'] * empty_locs['height']\n",
    "        empty_locs['max_dim'] = empty_locs[['width', 'depth', 'height']].max(axis=1)\n",
    "        \n",
    "        matches = empty_locs[\n",
    "            (empty_locs['vol'] >= p_vol) & \n",
    "            (empty_locs['max_dim'] >= p_max)\n",
    "        ]\n",
    "        \n",
    "        if len(matches) > 0:\n",
    "            fits_found += 1\n",
    "            \n",
    "    if fits_found > 0:\n",
    "        log(f\"WARN: Unallocated items exist, and {fits_found} of sampled items (20) physically fit in available empty bins.\")\n",
    "        log(\"      -> This suggests logic errors in the allocation algorithm (why weren't these placed?).\")\n",
    "    else:\n",
    "        log(\"INFO: Unallocated items exist, but they appear too large for the available empty bins.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def run_full_diagnostic():\n",
    "    datasets, valid_flags = load_all_data()\n",
    "    \n",
    "    # If any dataset failed SCHEMA validation, we pause and report.\n",
    "    # We do NOT proceed to logic that requires those columns.\n",
    "    \n",
    "    if not valid_flags['LOCATIONS']:\n",
    "        log(\"STOPPING: Locations dataset has schema errors (see above). Fix columns to proceed.\")\n",
    "        return\n",
    "    if not valid_flags['ALLOCATIONS']:\n",
    "        log(\"STOPPING: Allocations dataset has schema errors (see above). Fix columns to proceed.\")\n",
    "        return\n",
    "    if not valid_flags['PARTS']:\n",
    "        log(\"STOPPING: Parts dataset has schema errors (see above). Fix columns to proceed.\")\n",
    "        return\n",
    "\n",
    "    log(\"\\nAll Schemas Valid. Proceeding to Logic Checks...\\n\")\n",
    "\n",
    "    # 1. Duplicates\n",
    "    log(\"--- Duplicate Checks ---\")\n",
    "    for key, col in [('LOCATIONS', 'loc_inst_code'), ('ALLOCATIONS', 'LOCATION_ID'), ('PARTS', 'ITEM_ID')]:\n",
    "        dupes = datasets[key][datasets[key].duplicated(subset=col, keep=False)]\n",
    "        if len(dupes) > 0:\n",
    "            log(f\"FAIL: {len(dupes)} duplicates in {key} (Column: {col}).\")\n",
    "        else:\n",
    "            log(f\"PASS: No duplicates in {key}.\")\n",
    "\n",
    "    # 2. Integrity\n",
    "    check_referential_integrity(datasets)\n",
    "\n",
    "    # 3. Geometric Fit (The \"Real\" Check)\n",
    "    log(\"--- Geometric Fit Analysis ---\")\n",
    "    fit_issues = estimate_and_sample(\n",
    "        datasets['ALLOCATIONS'], \n",
    "        \"Geometric Fit\", \n",
    "        func_geometric_fit, \n",
    "        datasets['LOCATIONS']\n",
    "    )\n",
    "    \n",
    "    if not fit_issues.empty:\n",
    "        log(f\"FAIL: Found {len(fit_issues)} allocations that do not physically fit.\")\n",
    "        fit_issues.to_csv(f\"{OUTPUT_DIR}/fit_issues.csv\", index=False)\n",
    "        try:\n",
    "            plt.figure()\n",
    "            fit_issues['Issue'].value_counts().plot(kind='bar')\n",
    "            plt.title(\"Fit Issues\")\n",
    "            plt.savefig(f\"{OUTPUT_DIR}/fit_chart.png\")\n",
    "        except: pass\n",
    "    else:\n",
    "        log(\"PASS: All checked allocations fit geometrically.\")\n",
    "\n",
    "    # 4. Unallocated Logic\n",
    "    check_unallocated(datasets)\n",
    "\n",
    "    # Save Log\n",
    "    with open(f\"{OUTPUT_DIR}/validation_report.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(report_buffer))\n",
    "    log(f\"\\nValidation finished. Report saved to {OUTPUT_DIR}/validation_report.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_diagnostic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

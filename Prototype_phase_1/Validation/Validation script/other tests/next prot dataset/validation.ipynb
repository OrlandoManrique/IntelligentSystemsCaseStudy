{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a84296f-df2c-4d62-8268-29959ddd9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-06 16:17:20] Output folder 'validation_results' ready.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Loading LOCATIONS (locations_dummy.csv) ---\u001b[0m\n",
      "[2026-01-06 16:17:20] SUCCESS: LOCATIONS loaded (357 rows).\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Loading ALLOCATIONS (allocations.csv) ---\u001b[0m\n",
      "[2026-01-06 16:17:20] SUCCESS: ALLOCATIONS loaded (174 rows).\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Loading PARTS (synthetic_parts_generated.csv) ---\u001b[0m\n",
      "[2026-01-06 16:17:20] SUCCESS: PARTS loaded (67 rows).\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Volume Data Integrity ---\u001b[0m\n",
      "[2026-01-06 16:17:20] Starting Volume Data...\n",
      "[2026-01-06 16:17:20]    Est. time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 174/174 [00:00<00:00, 6815.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: Stated volumes match location dimensions.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Stated Utilization Accuracy ---\u001b[0m\n",
      "[2026-01-06 16:17:20] Starting Utilization Data...\n",
      "[2026-01-06 16:17:20]    Est. time: 0.03s\n",
      "[2026-01-06 16:17:20] INFO: Checking utilization accuracy using column: 'UTILIZATION_PCT'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 174/174 [00:00<00:00, 8083.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: Utilization data matches calculations.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Inventory Balance (Partial Unallocated) ---\u001b[0m\n",
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: Total allocated boxes match inventory on hand.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Single SKU per Bin ---\u001b[0m\n",
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: Single SKU constraint met.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Grid Math ---\u001b[0m\n",
      "[2026-01-06 16:17:20] Starting Grid Math...\n",
      "[2026-01-06 16:17:20]    Est. time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 174/174 [00:00<00:00, 7750.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: Grid Math consistent.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Rigid Body ---\u001b[0m\n",
      "[2026-01-06 16:17:20] Starting Rigid Body...\n",
      "[2026-01-06 16:17:20]    Est. time: 0.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 174/174 [00:00<00:00, 5414.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: Rigid Body dimensions valid.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Stack vs Bin Dimensions ---\u001b[0m\n",
      "[2026-01-06 16:17:20] Starting Stack Fit...\n",
      "[2026-01-06 16:17:20]    Est. time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 174/174 [00:00<00:00, 7555.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: All stacks fit within bins.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Bin Overlaps ---\u001b[0m\n",
      "[2026-01-06 16:17:20] Starting Bin Overlap...\n",
      "[2026-01-06 16:17:20]    Est. time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 357/357 [00:00<00:00, 53544.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: No bin overlaps.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Check: Unallocated Feasibility (6-Axis Geometric & Bulk) ---\u001b[0m\n",
      "[2026-01-06 16:17:20] \u001b[92m\u001b[1mPASS\u001b[0m: All items are allocated.\n",
      "[2026-01-06 16:17:20] \u001b[96m--- Generating Utilization Plots ---\u001b[0m\n",
      "[2026-01-06 16:17:21] Plot saved to validation_results/visual_utilization_max.png\n",
      "[2026-01-06 16:17:22] Plot saved to validation_results/visual_utilization_min.png\n",
      "[2026-01-06 16:17:22] Validation Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from itertools import permutations\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & SCHEMA\n",
    "# ==========================================\n",
    "\n",
    "LOCATIONS_FILE = 'locations_dummy.csv'\n",
    "ALLOCATIONS_FILE = 'allocations.csv'\n",
    "PARTS_FILE = 'synthetic_parts_generated.csv'\n",
    "OUTPUT_DIR = 'validation_results'\n",
    "MAX_EXECUTION_TIME_SEC = 300\n",
    "\n",
    "REQUIRED_SCHEMA = {\n",
    "    'LOCATIONS': {\n",
    "        'file': LOCATIONS_FILE,\n",
    "        # Ensure your locations file actually has these columns\n",
    "        'columns': ['loc_inst_code', 'width', 'depth', 'height', 'x', 'y', 'z']\n",
    "    },\n",
    "    'ALLOCATIONS': {\n",
    "        'file': ALLOCATIONS_FILE,\n",
    "        # UPDATED COLUMN LIST TO MATCH YOUR NEW FORMAT\n",
    "        'columns': ['loc_inst_code', 'LOCATION_TYPE', 'ITEM_ID', \n",
    "                    'QTY_ALLOCATED', 'MAX_UNITS', \n",
    "                    'GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                    'FULL_LAYERS', 'PARTIAL_UNITS',\n",
    "                    'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                    'LOCATION_VOL_MM3', 'LOCATION_VOL_M3', \n",
    "                    'STORED_VOL_M3', 'UTILIZATION_PCT'] \n",
    "    },\n",
    "    'PARTS': {\n",
    "        'file': PARTS_FILE,\n",
    "        'columns': ['ITEM_ID', 'LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG', \n",
    "                    'BOXES_ON_HAND', 'QTY_PER_BOX']\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOGGING & UTILS\n",
    "# ==========================================\n",
    "\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    CYAN = '\\033[96m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "report_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    report_buffer.append(f\"[{timestamp}] {message}\")\n",
    "    \n",
    "    c_msg = message\n",
    "    if \"PASS\" in message: c_msg = message.replace(\"PASS\", f\"{Colors.GREEN}{Colors.BOLD}PASS{Colors.RESET}\")\n",
    "    elif \"FAIL\" in message: c_msg = message.replace(\"FAIL\", f\"{Colors.RED}{Colors.BOLD}FAIL{Colors.RESET}\")\n",
    "    elif \"CRITICAL\" in message: c_msg = f\"{Colors.RED}{Colors.BOLD}{message}{Colors.RESET}\"\n",
    "    elif \"WARN\" in message: c_msg = message.replace(\"WARN\", f\"{Colors.YELLOW}{Colors.BOLD}WARN{Colors.RESET}\")\n",
    "    elif \"---\" in message: c_msg = f\"{Colors.CYAN}{message}{Colors.RESET}\"\n",
    "    \n",
    "    print(f\"[{timestamp}] {c_msg}\")\n",
    "\n",
    "def setup_environment():\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    log(f\"Output folder '{OUTPUT_DIR}' ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "\n",
    "def load_and_validate_dataset(key, config):\n",
    "    filepath = config['file']\n",
    "    log(f\"--- Loading {key} ({filepath}) ---\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        log(f\"CRITICAL: File {filepath} not found.\")\n",
    "        return None, False\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=None, engine='python', dtype=str)\n",
    "        df.columns = df.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "        \n",
    "        # Add Original Row ID\n",
    "        df['ROW_ID'] = df.index + 2 \n",
    "\n",
    "        missing = [c for c in config['columns'] if c not in df.columns]\n",
    "        if missing:\n",
    "            log(f\"CRITICAL SCHEMA ERROR in {key}. Missing: {missing}\")\n",
    "            return df, False\n",
    "        \n",
    "        log(f\"SUCCESS: {key} loaded ({len(df)} rows).\")\n",
    "        return df, True\n",
    "    except Exception as e:\n",
    "        log(f\"CRITICAL ERROR reading {filepath}: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def convert_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    setup_environment()\n",
    "    datasets = {}\n",
    "    valid_flags = {}\n",
    "    \n",
    "    # Locations\n",
    "    df, valid = load_and_validate_dataset('LOCATIONS', REQUIRED_SCHEMA['LOCATIONS'])\n",
    "    if valid: df = convert_numeric(df, ['width', 'depth', 'height', 'x', 'y', 'z'])\n",
    "    datasets['LOCATIONS'] = df\n",
    "    valid_flags['LOCATIONS'] = valid\n",
    "\n",
    "    # Allocations\n",
    "    df, valid = load_and_validate_dataset('ALLOCATIONS', REQUIRED_SCHEMA['ALLOCATIONS'])\n",
    "    if valid: \n",
    "        # UPDATED NUMERIC COLUMNS\n",
    "        df = convert_numeric(df, ['GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                                  'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                                  'MAX_UNITS', 'QTY_ALLOCATED', \n",
    "                                  'LOCATION_VOL_MM3', 'UTILIZATION_PCT'])\n",
    "    datasets['ALLOCATIONS'] = df\n",
    "    valid_flags['ALLOCATIONS'] = valid\n",
    "\n",
    "    # Parts\n",
    "    df, valid = load_and_validate_dataset('PARTS', REQUIRED_SCHEMA['PARTS'])\n",
    "    if valid: df = convert_numeric(df, ['LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG', \n",
    "                                        'BOXES_ON_HAND', 'QTY_PER_BOX'])\n",
    "    datasets['PARTS'] = df\n",
    "    valid_flags['PARTS'] = valid\n",
    "    \n",
    "    return datasets, valid_flags\n",
    "\n",
    "# ==========================================\n",
    "# 4. VALIDATION LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def estimate_and_sample(df, check_name, validation_func, *args):\n",
    "    log(f\"Starting {check_name}...\")\n",
    "    total = len(df)\n",
    "    test_size = min(1000, total)\n",
    "    if test_size == 0: return pd.DataFrame()\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        _ = validation_func(df.head(test_size), *args, quiet=True)\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in {check_name}: {e}\")\n",
    "        # raise e # Uncomment for debugging\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    dur = time.time() - t0\n",
    "    if dur == 0: dur = 0.001\n",
    "    est_time = (dur / test_size) * total\n",
    "    log(f\"   Est. time: {est_time:.2f}s\")\n",
    "\n",
    "    if est_time > MAX_EXECUTION_TIME_SEC:\n",
    "        safe_rows = int((MAX_EXECUTION_TIME_SEC / dur) * test_size)\n",
    "        log(f\"   WARN: Time limit exceeded. Sampling {safe_rows} rows.\")\n",
    "        df_to_process = df.sample(n=safe_rows, random_state=42)\n",
    "    else:\n",
    "        df_to_process = df\n",
    "\n",
    "    return validation_func(df_to_process, *args, quiet=False)\n",
    "\n",
    "# --- CHECK: Stated Utilization Accuracy ---\n",
    "# --- CHECK: Stated Utilization Accuracy ---\n",
    "def check_stated_utilization(df_alloc, df_loc, quiet=False):\n",
    "    col_name = 'UTILIZATION_PCT'\n",
    "    \n",
    "    if col_name not in df_alloc.columns:\n",
    "        if not quiet: log(f\"WARN: Column '{col_name}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not quiet: log(f\"INFO: Checking utilization accuracy using column: '{col_name}'\")\n",
    "\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    \n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue\n",
    "\n",
    "        # --- FIX STARTS HERE ---\n",
    "        \n",
    "        # 1. Calculate volume of ONE unit based on its orientation\n",
    "        one_unit_vol = row['ORIENT_X_MM'] * row['ORIENT_Y_MM'] * row['ORIENT_Z_MM']\n",
    "        \n",
    "        # 2. Multiply by the ACTUAL quantity in the bin (not the Grid capacity)\n",
    "        actual_occupied_vol = one_unit_vol * row['QTY_ALLOCATED']\n",
    "        \n",
    "        # --- FIX ENDS HERE ---\n",
    "        \n",
    "        loc_vol = row['width'] * row['depth'] * row['height']\n",
    "        \n",
    "        if loc_vol <= 0: continue\n",
    "\n",
    "        # Calculate percentage based on actual inventory\n",
    "        calc_pct = (actual_occupied_vol / loc_vol) * 100\n",
    "        stated_pct = row[col_name]\n",
    "\n",
    "        # Tolerance: 1% difference\n",
    "        if abs(calc_pct - stated_pct) > 1.0:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'loc_inst_code': row['loc_inst_code'],\n",
    "                'Issue': 'Utilization Data Mismatch',\n",
    "                # Updated error message to be clearer\n",
    "                'Details': f\"Stated: {stated_pct:.1f}% != Calc: {calc_pct:.1f}% (Qty {row['QTY_ALLOCATED']} * Vol / Bin Vol)\"\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Stack vs Bin Dimensions (Geometric Fit) ---\n",
    "def check_stack_fit(df_alloc, df_loc, quiet=False):\n",
    "    # UPDATED: Merge on 'loc_inst_code'\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    issues = []\n",
    "    \n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue \n",
    "\n",
    "        stack_x = row['GRID_X'] * row['ORIENT_X_MM']\n",
    "        stack_y = row['GRID_Y'] * row['ORIENT_Y_MM']\n",
    "        stack_z = row['GRID_Z'] * row['ORIENT_Z_MM']\n",
    "        \n",
    "        tol = 1.0\n",
    "        if (stack_x > row['width'] + tol) or (stack_y > row['depth'] + tol) or (stack_z > row['height'] + tol):\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'loc_inst_code': row['loc_inst_code'],\n",
    "                'ITEM_ID': row['ITEM_ID'], # Updated from SKU\n",
    "                'Issue': 'Stack Exceeds Bin',\n",
    "                'Details': f\"Bin: {row['width']}x{row['depth']}x{row['height']} | Stack: {stack_x:.1f}x{stack_y:.1f}x{stack_z:.1f}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Inventory Balance (Partial Unallocated) ---\n",
    "def check_inventory_balance(df_alloc, df_parts):\n",
    "    log(\"--- Check: Inventory Balance (Partial Unallocated) ---\")\n",
    "    \n",
    "    # FIX: Removed * df_parts['QTY_PER_BOX'] to ensure we count boxes, not contents\n",
    "    df_parts['TOTAL_DEMAND'] = df_parts['BOXES_ON_HAND'] \n",
    "    \n",
    "    demand_map = df_parts.set_index('ITEM_ID')['TOTAL_DEMAND'].to_dict()\n",
    "    \n",
    "    # Ensure numeric\n",
    "    df_alloc['QTY_ALLOCATED'] = pd.to_numeric(df_alloc['QTY_ALLOCATED'], errors='coerce').fillna(0)\n",
    "    alloc_sums = df_alloc.groupby('ITEM_ID')['QTY_ALLOCATED'].sum().to_dict()\n",
    "    \n",
    "    issues = []\n",
    "    for item_id, demand in demand_map.items():\n",
    "        allocated = alloc_sums.get(item_id, 0)\n",
    "        \n",
    "        # Use simple float tolerance just in case\n",
    "        if allocated < (demand - 0.01):\n",
    "            issues.append({'ITEM_ID': item_id, 'Issue': 'Partial/No Allocation', 'Demand': demand, 'Allocated': allocated, 'Missing': demand - allocated})\n",
    "        elif allocated > (demand + 0.01):\n",
    "            issues.append({'ITEM_ID': item_id, 'Issue': 'Over-Allocation', 'Demand': demand, 'Allocated': allocated, 'Excess': allocated - demand})\n",
    "            \n",
    "    df_issues = pd.DataFrame(issues)\n",
    "    if not df_issues.empty:\n",
    "        log(f\"FAIL: Found {len(df_issues)} Items with inventory mismatches.\")\n",
    "        df_issues.to_csv(f\"{OUTPUT_DIR}/fail_inventory_balance.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Total allocated boxes match inventory on hand.\")\n",
    "\n",
    "# --- CHECK: Volume Data Integrity ---\n",
    "def check_volume_data_integrity(df_alloc, df_loc, quiet=False):\n",
    "    # UPDATED: Merge on loc_inst_code\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue\n",
    "        real_vol = row['width'] * row['depth'] * row['height']\n",
    "        \n",
    "        # Using LOCATION_VOL_MM3 from new format\n",
    "        if abs(real_vol - row['LOCATION_VOL_MM3']) > 1.0:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'loc_inst_code': row['loc_inst_code'],\n",
    "                'Issue': 'Volume Data Error',\n",
    "                'Details': f\"Stated: {row['LOCATION_VOL_MM3']} != Real: {real_vol}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Grid Math ---\n",
    "def check_grid_consistency(df_alloc, quiet=False):\n",
    "    issues = []\n",
    "    iterator = tqdm(df_alloc.iterrows(), total=df_alloc.shape[0]) if not quiet else df_alloc.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        grid_cap = row['GRID_X'] * row['GRID_Y'] * row['GRID_Z']\n",
    "        if abs(grid_cap - row['MAX_UNITS']) > 0.1:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'], \n",
    "                'loc_inst_code': row['loc_inst_code'], \n",
    "                'Issue': 'Grid Math Mismatch', \n",
    "                'Details': f\"Grid ({grid_cap}) != MaxUnits ({row['MAX_UNITS']})\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Rigid Body ---\n",
    "def check_rigid_body(df_alloc, df_parts, quiet=False):\n",
    "    # UPDATED: Merge on ITEM_ID\n",
    "    merged = df_alloc.merge(df_parts, on='ITEM_ID', how='left', suffixes=('', '_PART'))\n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['LEN_MM']): continue\n",
    "        od = sorted([row['ORIENT_X_MM'], row['ORIENT_Y_MM'], row['ORIENT_Z_MM']])\n",
    "        pd_ = sorted([row['LEN_MM'], row['WID_MM'], row['DEP_MM']])\n",
    "        if any(abs(o - p) > 0.5 for o, p in zip(od, pd_)):\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'], \n",
    "                'loc_inst_code': row['loc_inst_code'], \n",
    "                'Issue': 'Dimensions Morphing', \n",
    "                'Details': f\"Alloc {od} vs Part {pd_}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Bin Overlap ---\n",
    "def check_bin_overlaps(df_loc, quiet=False):\n",
    "    df = df_loc.copy()\n",
    "    df['x2'] = df['x'] + df['width']\n",
    "    df['y2'] = df['y'] + df['depth']\n",
    "    df['z2'] = df['z'] + df['height']\n",
    "    df.sort_values('x', inplace=True)\n",
    "    issues = []\n",
    "    data = df.to_dict('records')\n",
    "    n = len(data)\n",
    "    if n == 0: return pd.DataFrame()\n",
    "    iter_range = tqdm(range(n), total=n) if not quiet else range(n)\n",
    "    for i in iter_range:\n",
    "        a = data[i]\n",
    "        for j in range(i + 1, n):\n",
    "            b = data[j]\n",
    "            if b['x'] >= a['x2'] - 0.1: break\n",
    "            if (a['y'] < b['y2']) and (a['y2'] > b['y']) and (a['z'] < b['z2']) and (a['z2'] > b['z']):\n",
    "                issues.append({'LOC_A': a['loc_inst_code'], 'LOC_B': b['loc_inst_code'], 'Issue': 'Physical Overlap', 'Details': f\"A({a['x']}) vs B({b['x']})\"})\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "\n",
    "def calculate_max_storage_capacity(item_dims, bin_dims):\n",
    "    \"\"\"\n",
    "    Returns max quantity of item_dims (L,W,D) that fit in bin_dims (W,D,H)\n",
    "    by testing all 6 rotation permutations.\n",
    "    \"\"\"\n",
    "    L, W, D = item_dims\n",
    "    bin_W, bin_D, bin_H = bin_dims\n",
    "    \n",
    "    # 1. Get all 6 unique rotations (e.g., LWD, LDW, WLD, etc.)\n",
    "    unique_orientations = list(set(permutations([L, W, D])))\n",
    "    \n",
    "    max_qty = 0\n",
    "    best_fail_reason = \"Dimensions too large\"\n",
    "\n",
    "    for orient in unique_orientations:\n",
    "        ox, oy, oz = orient\n",
    "        \n",
    "        # Check geometric fit (does 1 unit fit?)\n",
    "        if ox <= bin_W and oy <= bin_D and oz <= bin_H:\n",
    "            # Calculate bulk fit (how many units fit?)\n",
    "            nx = int(bin_W // ox)\n",
    "            ny = int(bin_D // oy)\n",
    "            nz = int(bin_H // oz)\n",
    "            total = nx * ny * nz\n",
    "            \n",
    "            if total > max_qty:\n",
    "                max_qty = total\n",
    "\n",
    "    # Logic to return a specific reason if it fits nowhere\n",
    "    if max_qty == 0:\n",
    "        # Compare sorted dimensions to give the most \"charitable\" fail reason\n",
    "        s_item = sorted([L, W, D])\n",
    "        s_bin = sorted([bin_W, bin_D, bin_H])\n",
    "        if s_item[0] > s_bin[0]: best_fail_reason = f\"Min Dim {s_item[0]} > Bin Min {s_bin[0]}\"\n",
    "        elif s_item[1] > s_bin[1]: best_fail_reason = f\"Mid Dim {s_item[1]} > Bin Mid {s_bin[1]}\"\n",
    "        elif s_item[2] > s_bin[2]: best_fail_reason = f\"Max Dim {s_item[2]} > Bin Max {s_bin[2]}\"\n",
    "\n",
    "    return max_qty, best_fail_reason\n",
    "\n",
    "# --- CHECK: Unallocated Items ---\n",
    "def check_unallocated_feasibility(df_parts, df_alloc, df_loc):\n",
    "    log(\"--- Check: Unallocated Feasibility (6-Axis Geometric & Bulk) ---\")\n",
    "    \n",
    "    # 1. Identify Unallocated Items\n",
    "    all_items = set(df_parts['ITEM_ID'])\n",
    "    alloc_items = set(df_alloc['ITEM_ID'])\n",
    "    unallocated_ids = list(all_items - alloc_items)\n",
    "    \n",
    "    if not unallocated_ids:\n",
    "        log(\"PASS: All items are allocated.\")\n",
    "        return\n",
    "\n",
    "    # 2. Identify EMPTY Bins\n",
    "    occupied_locs = set(df_alloc['loc_inst_code'])\n",
    "    empty_locs = df_loc[~df_loc['loc_inst_code'].isin(occupied_locs)].copy()\n",
    "    \n",
    "    if empty_locs.empty:\n",
    "        log(f\"WARN: {len(unallocated_ids)} unallocated items, but NO empty bins exist.\")\n",
    "        return\n",
    "\n",
    "    # 3. OPTIMIZATION: Group empty bins by unique dimensions\n",
    "    # We create a signature string \"WxDxH\" to find unique bin types\n",
    "    empty_locs['dim_sig'] = empty_locs.apply(lambda x: f\"{x['width']}_{x['depth']}_{x['height']}\", axis=1)\n",
    "    unique_bins = empty_locs.drop_duplicates(subset='dim_sig')\n",
    "    \n",
    "    log(f\"INFO: Checking {len(unallocated_ids)} items against {len(unique_bins)} unique empty bin types.\")\n",
    "    \n",
    "    feasible_unallocated = []\n",
    "    true_failures = []\n",
    "\n",
    "    for item_id in unallocated_ids:\n",
    "        # Get Item Dimensions\n",
    "        part = df_parts[df_parts['ITEM_ID'] == item_id].iloc[0]\n",
    "        item_dims = (part['LEN_MM'], part['WID_MM'], part['DEP_MM'])\n",
    "        \n",
    "        found_fit = False\n",
    "        last_reason = \"Unknown\"\n",
    "        \n",
    "        # Check against every UNIQUE empty bin type\n",
    "        for _, bin_row in unique_bins.iterrows():\n",
    "            bin_dims = (bin_row['width'], bin_row['depth'], bin_row['height'])\n",
    "            \n",
    "            # Run the helper function\n",
    "            max_qty, reason = calculate_max_storage_capacity(item_dims, bin_dims)\n",
    "            \n",
    "            if max_qty > 0:\n",
    "                # IT FITS!\n",
    "                feasible_unallocated.append({\n",
    "                    'ITEM_ID': item_id,\n",
    "                    'Item_Dims': item_dims,\n",
    "                    'Fits_In_Bin_Type': bin_row['dim_sig'],\n",
    "                    'Max_Possible_Qty': max_qty\n",
    "                })\n",
    "                found_fit = True\n",
    "                break # Stop checking other bins for this item, we proved it could fit.\n",
    "            else:\n",
    "                last_reason = reason\n",
    "\n",
    "        if not found_fit:\n",
    "            true_failures.append({\n",
    "                'ITEM_ID': item_id,\n",
    "                'Item_Dims': item_dims,\n",
    "                'Reason': last_reason\n",
    "            })\n",
    "\n",
    "    # 4. Reporting\n",
    "    if len(feasible_unallocated) > 0:\n",
    "        log(f\"FAIL: {len(feasible_unallocated)} Unallocated Items COULD fit in currently empty bins.\")\n",
    "        df_fail = pd.DataFrame(feasible_unallocated)\n",
    "        print(df_fail.head().to_string(index=False))\n",
    "        df_fail.to_csv(f\"{OUTPUT_DIR}/fail_unallocated_feasible.csv\", index=False)\n",
    "    else:\n",
    "        log(f\"PASS: Correct. All {len(true_failures)} unallocated items physically cannot fit in any available empty bin.\")\n",
    "        if len(true_failures) > 0:\n",
    "            df_pass = pd.DataFrame(true_failures)\n",
    "            df_pass.to_csv(f\"{OUTPUT_DIR}/pass_unallocated_impossible.csv\", index=False)\n",
    "\n",
    "# ==========================================\n",
    "# 5. VISUALIZATION\n",
    "# ==========================================\n",
    "\n",
    "def plot_single_bin(row, title_prefix, filename_tag):\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=[2, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, 0]); ax2 = fig.add_subplot(gs[1, 0]); ax_text = fig.add_subplot(gs[:, 1]); ax_text.axis('off')\n",
    "    \n",
    "    bin_w, bin_h, bin_d = row['width'], row['height'], row['depth']\n",
    "    item_w, item_h, item_d = row['ORIENT_X_MM'], row['ORIENT_Z_MM'], row['ORIENT_Y_MM']\n",
    "    gx, gy, gz = int(row['GRID_X']), int(row['GRID_Y']), int(row['GRID_Z'])\n",
    "    \n",
    "    # --- LOGIC FIX: Determine which slots are ACTUALLY occupied ---\n",
    "    qty_alloc = int(row['QTY_ALLOCATED'])\n",
    "    occupied_slots = []\n",
    "    count = 0\n",
    "    \n",
    "    # Simulate filling order: X (Width) -> Y (Depth) -> Z (Height)\n",
    "    for z in range(gz):\n",
    "        for y in range(gy):\n",
    "            for x in range(gx):\n",
    "                if count < qty_alloc:\n",
    "                    occupied_slots.append((x, y, z))\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    # Create sets of occupied 2D coordinates for the projections\n",
    "    # Front View (X, Z) - Draw if an item exists at this X,Z (at any depth)\n",
    "    valid_front_xz = set((x, z) for x, y, z in occupied_slots)\n",
    "    # Top View (X, Y) - Draw if an item exists at this X,Y (at any height)\n",
    "    valid_top_xy = set((x, y) for x, y, z in occupied_slots)\n",
    "\n",
    "    # --- DRAW FRONT VIEW (X-Z) ---\n",
    "    ax1.add_patch(patches.Rectangle((0, 0), bin_w, bin_h, fill=False, edgecolor='red', lw=3, label='Bin'))\n",
    "    for z in range(gz):\n",
    "        for x in range(gx):\n",
    "            # Only draw if this slot actually contains an item\n",
    "            if (x, z) in valid_front_xz:\n",
    "                ax1.add_patch(patches.Rectangle((x*item_w, z*item_h), item_w, item_h, lw=1, ec='black', fc='skyblue', alpha=0.6))\n",
    "    \n",
    "    ax1.set_title(\"FRONT VIEW (X-Z) - ACTUAL INVENTORY\", fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlim(-50, bin_w + 50); ax1.set_ylim(-50, bin_h + 50); ax1.set_aspect('equal'); ax1.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    # --- DRAW TOP VIEW (X-Y) ---\n",
    "    ax2.add_patch(patches.Rectangle((0, 0), bin_w, bin_d, fill=False, edgecolor='red', lw=3))\n",
    "    for y in range(gy):\n",
    "        for x in range(gx):\n",
    "            # Only draw if this slot actually contains an item\n",
    "            if (x, y) in valid_top_xy:\n",
    "                ax2.add_patch(patches.Rectangle((x*item_w, y*item_d), item_w, item_d, lw=1, ec='black', fc='orange', alpha=0.6))\n",
    "    \n",
    "    ax2.set_title(\"TOP VIEW (X-Y) - ACTUAL INVENTORY\", fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlim(-50, bin_w + 50); ax2.set_ylim(-50, bin_d + 50); ax2.set_aspect('equal'); ax2.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    info_text = (\n",
    "        f\"REPORT: {title_prefix}\\n----------------------------------------\\n\"\n",
    "        f\"ALLOCATION ROW:   {row['ROW_ID']}\\nLOCATION ID:      {row['loc_inst_code']}\\nITEM ID:          {row['ITEM_ID']}\\n\"\n",
    "        f\"----------------------------------------\\nMETRICS:\\n\"\n",
    "        f\"  Utilization:    {row['UTILIZATION_PCT']:.2f} % (Based on Qty {qty_alloc})\\n\"\n",
    "        f\"  Qty Allocated:  {qty_alloc}\\n\"\n",
    "        f\"  Max Capacity:   {row['MAX_UNITS']}\\n\\n\"\n",
    "        f\"DIMENSIONS (mm):\\n  Bin (WxDxH):    {bin_w} x {bin_d} x {bin_h}\\n\"\n",
    "        f\"  Item Orient:    {row['ORIENT_X_MM']} x {row['ORIENT_Y_MM']} x {row['ORIENT_Z_MM']}\\n\\n\"\n",
    "        f\"VOLUMES (mm3):\\n  Bin Volume:     {row['LOC_VOL']:,.0f}\\n  Allocated Vol:  {row['STACK_VOL']:,.0f}\\n\"\n",
    "    )\n",
    "    ax_text.text(0.05, 0.95, info_text, transform=ax_text.transAxes, fontsize=12, verticalalignment='top', family='monospace', bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "    plt.tight_layout(); save_path = f\"{OUTPUT_DIR}/{filename_tag}.png\"; plt.savefig(save_path); plt.close(); log(f\"Plot saved to {save_path}\")\n",
    "\n",
    "def visualize_utilization_extremes(datasets):\n",
    "    log(\"--- Generating Utilization Plots ---\")\n",
    "    df_alloc = datasets['ALLOCATIONS']; df_loc = datasets['LOCATIONS']\n",
    "    \n",
    "    # Merge on loc_inst_code\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='inner', suffixes=('', '_LOC'))\n",
    "    if merged.empty: return\n",
    "    \n",
    "    # RECALCULATE VOLUMES based on ACTUAL QTY (Not Grid Capacity)\n",
    "    merged['STACK_VOL'] = merged['QTY_ALLOCATED'] * (merged['ORIENT_X_MM'] * merged['ORIENT_Y_MM'] * merged['ORIENT_Z_MM'])\n",
    "    merged['LOC_VOL'] = merged['width'] * merged['depth'] * merged['height']\n",
    "    \n",
    "    # Filter valid bins\n",
    "    merged = merged[merged['LOC_VOL'] > 0]\n",
    "    \n",
    "    # FORCE CALCULATION of Real Utilization %\n",
    "    # This ensures we rank by what is actually in the box, matching the new plot logic\n",
    "    merged['UTILIZATION_PCT'] = (merged['STACK_VOL'] / merged['LOC_VOL']) * 100\n",
    "    \n",
    "    # Plot Max\n",
    "    top_row = merged.sort_values(by='UTILIZATION_PCT', ascending=False).iloc[0]\n",
    "    plot_single_bin(top_row, \"HIGHEST UTILIZATION\", \"visual_utilization_max\")\n",
    "    \n",
    "    # Plot Min (active only)\n",
    "    active = merged[merged['UTILIZATION_PCT'] > 0]\n",
    "    if not active.empty:\n",
    "        low_row = active.sort_values(by='UTILIZATION_PCT', ascending=True).iloc[0]\n",
    "        plot_single_bin(low_row, \"LOWEST UTILIZATION\", \"visual_utilization_min\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def run_full_diagnostic():\n",
    "    datasets, valid_flags = load_all_data()\n",
    "    if not all(valid_flags.values()): log(\"STOPPING: Schema Errors.\"); return\n",
    "\n",
    "    # 1. Volume Data Integrity\n",
    "    log(\"--- Check: Volume Data Integrity ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Volume Data\", check_volume_data_integrity, datasets['LOCATIONS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Volume Data mismatches.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_volume_data.csv\", index=False)\n",
    "    else: log(\"PASS: Stated volumes match location dimensions.\")\n",
    "    \n",
    "    # 2. Utilization Data Integrity\n",
    "    log(\"--- Check: Stated Utilization Accuracy ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Utilization Data\", check_stated_utilization, datasets['LOCATIONS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Utilization % mismatches.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_utilization_data.csv\", index=False)\n",
    "    else: log(\"PASS: Utilization data matches calculations.\")\n",
    "\n",
    "    # 3. Inventory Balance\n",
    "    check_inventory_balance(datasets['ALLOCATIONS'], datasets['PARTS'])\n",
    "\n",
    "    # 4. Standard Checks\n",
    "    log(\"--- Check: Single SKU per Bin ---\")\n",
    "    # UPDATED: using loc_inst_code\n",
    "    dupes = datasets['ALLOCATIONS'][datasets['ALLOCATIONS'].duplicated(subset=['loc_inst_code'], keep=False)]\n",
    "    if not dupes.empty: log(f\"FAIL: {len(dupes)} locations have multiple SKUs.\"); dupes.to_csv(f\"{OUTPUT_DIR}/fail_single_sku.csv\")\n",
    "    else: log(\"PASS: Single SKU constraint met.\")\n",
    "\n",
    "    log(\"--- Check: Grid Math ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Grid Math\", check_grid_consistency)\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Grid Math errors.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_grid_math.csv\", index=False)\n",
    "    else: log(\"PASS: Grid Math consistent.\")\n",
    "\n",
    "    log(\"--- Check: Rigid Body ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Rigid Body\", check_rigid_body, datasets['PARTS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Rigid Body errors.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_rigid_body.csv\", index=False)\n",
    "    else: log(\"PASS: Rigid Body dimensions valid.\")\n",
    "\n",
    "    log(\"--- Check: Stack vs Bin Dimensions ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Stack Fit\", check_stack_fit, datasets['LOCATIONS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} allocations exceed bin dimensions.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_stack_fit.csv\", index=False)\n",
    "    else: log(\"PASS: All stacks fit within bins.\")\n",
    "\n",
    "    log(\"--- Check: Bin Overlaps ---\")\n",
    "    res = estimate_and_sample(datasets['LOCATIONS'], \"Bin Overlap\", check_bin_overlaps)\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} locations overlap physically.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_bin_overlap.csv\", index=False)\n",
    "    else: log(\"PASS: No bin overlaps.\")\n",
    "\n",
    "    check_unallocated_feasibility(datasets['PARTS'], datasets['ALLOCATIONS'], datasets['LOCATIONS'])\n",
    "    visualize_utilization_extremes(datasets)\n",
    "\n",
    "    with open(f\"{OUTPUT_DIR}/validation_report.txt\", \"w\") as f: f.write(\"\\n\".join(report_buffer))\n",
    "    log(\"Validation Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_diagnostic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

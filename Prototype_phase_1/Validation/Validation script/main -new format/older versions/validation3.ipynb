{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a84296f-df2c-4d62-8268-29959ddd9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:59:40] Output folder 'validation_results' ready.\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Loading LOCATIONS ---\u001b[0m\n",
      "[2025-12-28 14:59:40] SUCCESS: LOCATIONS loaded (357 rows).\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Loading ALLOCATIONS ---\u001b[0m\n",
      "[2025-12-28 14:59:40] SUCCESS: ALLOCATIONS loaded (10 rows).\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Loading PARTS ---\u001b[0m\n",
      "[2025-12-28 14:59:40] SUCCESS: PARTS loaded (75 rows).\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Check: Single SKU per Bin ---\u001b[0m\n",
      "[2025-12-28 14:59:40] \u001b[92m\u001b[1mPASS\u001b[0m: All bins contain single SKU.\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Check: Grid Math Consistency ---\u001b[0m\n",
      "[2025-12-28 14:59:40] Starting Grid Math...\n",
      "[2025-12-28 14:59:40]    Est. time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 6951.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:59:40] \u001b[92m\u001b[1mPASS\u001b[0m: Grid math consistent.\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Check: Rigid Body Physics ---\u001b[0m\n",
      "[2025-12-28 14:59:40] Starting Rigid Body...\n",
      "[2025-12-28 14:59:40]    Est. time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2680.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:59:40] \u001b[92m\u001b[1mPASS\u001b[0m: Rigid body dimensions valid.\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Check: Global Bin Overlaps ---\u001b[0m\n",
      "[2025-12-28 14:59:40] Starting Bin Overlap...\n",
      "[2025-12-28 14:59:40]    Est. time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 357/357 [00:00<00:00, 53211.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:59:40] \u001b[92m\u001b[1mPASS\u001b[0m: No bin overlaps detected.\n",
      "[2025-12-28 14:59:40] \u001b[96m--- Generating Utilization Plots ---\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:59:40] Plot saved to validation_results/visual_utilization_max.png\n",
      "[2025-12-28 14:59:41] Plot saved to validation_results/visual_utilization_min.png\n",
      "[2025-12-28 14:59:41] Validation Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & SCHEMA\n",
    "# ==========================================\n",
    "\n",
    "LOCATIONS_FILE = 'locations_dummy.csv'\n",
    "ALLOCATIONS_FILE = 'allocations.csv'\n",
    "PARTS_FILE = 'synthetic_parts_generated.csv'\n",
    "OUTPUT_DIR = 'validation_results'\n",
    "MAX_EXECUTION_TIME_SEC = 300\n",
    "\n",
    "# Strict Schema\n",
    "REQUIRED_SCHEMA = {\n",
    "    'LOCATIONS': {\n",
    "        'file': LOCATIONS_FILE,\n",
    "        'columns': ['loc_inst_code', 'width', 'depth', 'height', 'x', 'y', 'z']\n",
    "    },\n",
    "    'ALLOCATIONS': {\n",
    "        'file': ALLOCATIONS_FILE,\n",
    "        'columns': ['LOCATION_ID', 'SKU', \n",
    "                    'GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                    'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                    'MAX_UNITS', 'CURRENT_STOCK'] \n",
    "    },\n",
    "    'PARTS': {\n",
    "        'file': PARTS_FILE,\n",
    "        'columns': ['ITEM_ID', 'LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG']\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOGGING & UTILS\n",
    "# ==========================================\n",
    "\n",
    "class Colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "report_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    plain_msg = f\"[{timestamp}] {message}\"\n",
    "    report_buffer.append(plain_msg)\n",
    "\n",
    "    colored_msg = message\n",
    "    if \"PASS\" in message:\n",
    "        colored_msg = message.replace(\"PASS\", f\"{Colors.GREEN}{Colors.BOLD}PASS{Colors.RESET}\")\n",
    "    elif \"FAIL\" in message:\n",
    "        colored_msg = message.replace(\"FAIL\", f\"{Colors.RED}{Colors.BOLD}FAIL{Colors.RESET}\")\n",
    "    elif \"CRITICAL\" in message:\n",
    "        colored_msg = f\"{Colors.RED}{Colors.BOLD}{message}{Colors.RESET}\"\n",
    "    elif \"WARN\" in message:\n",
    "        colored_msg = message.replace(\"WARN\", f\"{Colors.YELLOW}{Colors.BOLD}WARN{Colors.RESET}\")\n",
    "    elif \"---\" in message:\n",
    "        colored_msg = f\"{Colors.CYAN}{message}{Colors.RESET}\"\n",
    "    \n",
    "    print(f\"[{timestamp}] {colored_msg}\")\n",
    "\n",
    "def setup_environment():\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    log(f\"Output folder '{OUTPUT_DIR}' ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "\n",
    "def load_and_validate_dataset(key, config):\n",
    "    filepath = config['file']\n",
    "    required_cols = config['columns']\n",
    "    \n",
    "    log(f\"--- Loading {key} ---\")\n",
    "    if not os.path.exists(filepath):\n",
    "        log(f\"CRITICAL: File {filepath} not found.\")\n",
    "        return None, False\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=None, engine='python', dtype=str)\n",
    "        df.columns = df.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "        \n",
    "        missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            log(f\"CRITICAL SCHEMA ERROR in {key}. Missing: {missing_cols}\")\n",
    "            return df, False\n",
    "        \n",
    "        log(f\"SUCCESS: {key} loaded ({len(df)} rows).\")\n",
    "        return df, True\n",
    "    except Exception as e:\n",
    "        log(f\"CRITICAL ERROR reading {filepath}: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def convert_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    setup_environment()\n",
    "    datasets = {}\n",
    "    valid_flags = {}\n",
    "    \n",
    "    # Locations\n",
    "    df, valid = load_and_validate_dataset('LOCATIONS', REQUIRED_SCHEMA['LOCATIONS'])\n",
    "    if valid: df = convert_numeric(df, ['width', 'depth', 'height', 'x', 'y', 'z'])\n",
    "    datasets['LOCATIONS'] = df\n",
    "    valid_flags['LOCATIONS'] = valid\n",
    "\n",
    "    # Allocations\n",
    "    df, valid = load_and_validate_dataset('ALLOCATIONS', REQUIRED_SCHEMA['ALLOCATIONS'])\n",
    "    if valid: \n",
    "        df = convert_numeric(df, ['GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                                  'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                                  'MAX_UNITS', 'CURRENT_STOCK'])\n",
    "    datasets['ALLOCATIONS'] = df\n",
    "    valid_flags['ALLOCATIONS'] = valid\n",
    "\n",
    "    # Parts\n",
    "    df, valid = load_and_validate_dataset('PARTS', REQUIRED_SCHEMA['PARTS'])\n",
    "    if valid: df = convert_numeric(df, ['LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG'])\n",
    "    datasets['PARTS'] = df\n",
    "    valid_flags['PARTS'] = valid\n",
    "    \n",
    "    return datasets, valid_flags\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRUST NOTHING CHECKS\n",
    "# ==========================================\n",
    "\n",
    "def estimate_and_sample(df, check_name, validation_func, *args):\n",
    "    log(f\"Starting {check_name}...\")\n",
    "    total_rows = len(df)\n",
    "    test_size = min(1000, total_rows)\n",
    "    if test_size == 0: return pd.DataFrame()\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        _ = validation_func(df.head(test_size), *args, quiet=True)\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in {check_name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    if duration == 0: duration = 0.001\n",
    "    est_time = (duration / test_size) * total_rows\n",
    "    log(f\"   Est. time: {est_time:.2f}s\")\n",
    "\n",
    "    if est_time > MAX_EXECUTION_TIME_SEC:\n",
    "        safe_rows = int((MAX_EXECUTION_TIME_SEC / duration) * test_size)\n",
    "        log(f\"   WARN: Time limit exceeded. Sampling {safe_rows} rows.\")\n",
    "        df_to_process = df.sample(n=safe_rows, random_state=42)\n",
    "    else:\n",
    "        df_to_process = df\n",
    "\n",
    "    return validation_func(df_to_process, *args, quiet=False)\n",
    "\n",
    "def check_grid_consistency(df_alloc, quiet=False):\n",
    "    issues = []\n",
    "    iterator = tqdm(df_alloc.iterrows(), total=df_alloc.shape[0]) if not quiet else df_alloc.iterrows()\n",
    "\n",
    "    for idx, row in iterator:\n",
    "        grid_cap = row['GRID_X'] * row['GRID_Y'] * row['GRID_Z']\n",
    "        max_units = row['MAX_UNITS']\n",
    "        if abs(grid_cap - max_units) > 0.1:\n",
    "            issues.append({\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'SKU': row['SKU'],\n",
    "                'Issue': 'Grid Math Mismatch',\n",
    "                'Details': f\"Grid ({grid_cap}) != Max ({max_units})\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "def check_rigid_body(df_alloc, df_parts, quiet=False):\n",
    "    merged = df_alloc.merge(df_parts, left_on='SKU', right_on='ITEM_ID', how='left')\n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "\n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['LEN_MM']): continue\n",
    "        orient_dims = sorted([row['ORIENT_X_MM'], row['ORIENT_Y_MM'], row['ORIENT_Z_MM']])\n",
    "        part_dims = sorted([row['LEN_MM'], row['WID_MM'], row['DEP_MM']])\n",
    "        \n",
    "        match = True\n",
    "        for o, p in zip(orient_dims, part_dims):\n",
    "            if abs(o - p) > 0.5: match = False\n",
    "        \n",
    "        if not match:\n",
    "            issues.append({\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'SKU': row['SKU'],\n",
    "                'Issue': 'Dimensions Morphing',\n",
    "                'Details': f\"Alloc {orient_dims} vs Part {part_dims}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "def check_bin_overlaps(df_loc, quiet=False):\n",
    "    df = df_loc.copy()\n",
    "    df['x2'] = df['x'] + df['width']\n",
    "    df['y2'] = df['y'] + df['depth']\n",
    "    df['z2'] = df['z'] + df['height']\n",
    "    df.sort_values('x', inplace=True)\n",
    "    \n",
    "    issues = []\n",
    "    data = df.to_dict('records')\n",
    "    n = len(data)\n",
    "    if n == 0: return pd.DataFrame()\n",
    "\n",
    "    iter_range = tqdm(range(n), total=n) if not quiet else range(n)\n",
    "\n",
    "    for i in iter_range:\n",
    "        a = data[i]\n",
    "        for j in range(i + 1, n):\n",
    "            b = data[j]\n",
    "            if b['x'] >= a['x2'] - 0.1: break\n",
    "            if (a['y'] < b['y2']) and (a['y2'] > b['y']) and (a['z'] < b['z2']) and (a['z2'] > b['z']):\n",
    "                issues.append({\n",
    "                    'LOC_A': a['loc_inst_code'], 'LOC_B': b['loc_inst_code'],\n",
    "                    'Issue': 'Physical Overlap', 'Details': f\"A({a['x']}) overlaps B({b['x']})\"\n",
    "                })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "def check_single_sku(df_alloc):\n",
    "    dupes = df_alloc[df_alloc.duplicated(subset=['LOCATION_ID'], keep=False)]\n",
    "    if len(dupes) > 0: return dupes\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# 5. VISUALIZATION\n",
    "# ==========================================\n",
    "\n",
    "def plot_single_bin(row, title_prefix, filename_tag):\n",
    "    \"\"\"Generic plotter for a specific allocation row.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle(f\"{title_prefix}: {row['LOCATION_ID']} ({row['UTILIZATION_PCT']:.1f}%)\", fontsize=16)\n",
    "\n",
    "    bin_w, bin_h, bin_d = row['width'], row['height'], row['depth']\n",
    "    item_w, item_h, item_d = row['ORIENT_X_MM'], row['ORIENT_Z_MM'], row['ORIENT_Y_MM']\n",
    "    gx, gy, gz = int(row['GRID_X']), int(row['GRID_Y']), int(row['GRID_Z'])\n",
    "\n",
    "    # FRONT VIEW\n",
    "    ax1.add_patch(patches.Rectangle((0, 0), bin_w, bin_h, fill=False, edgecolor='red', lw=3))\n",
    "    for z in range(gz):\n",
    "        for x in range(gx):\n",
    "            ax1.add_patch(patches.Rectangle((x*item_w, z*item_h), item_w, item_h, \n",
    "                                            lw=1, ec='black', fc='skyblue', alpha=0.6))\n",
    "    ax1.set_title(\"Front View (X-Z)\")\n",
    "    ax1.set_xlim(-100, bin_w+100); ax1.set_ylim(-100, bin_h+100)\n",
    "    ax1.set_aspect('equal')\n",
    "\n",
    "    # TOP VIEW\n",
    "    ax2.add_patch(patches.Rectangle((0, 0), bin_w, bin_d, fill=False, edgecolor='red', lw=3))\n",
    "    for y in range(gy):\n",
    "        for x in range(gx):\n",
    "            ax2.add_patch(patches.Rectangle((x*item_w, y*item_d), item_w, item_d, \n",
    "                                            lw=1, ec='black', fc='orange', alpha=0.6))\n",
    "    ax2.set_title(\"Top View (X-Y)\")\n",
    "    ax2.set_xlim(-100, bin_w+100); ax2.set_ylim(-100, bin_d+100)\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    save_path = f\"{OUTPUT_DIR}/{filename_tag}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    log(f\"Plot saved to {save_path}\")\n",
    "\n",
    "def visualize_utilization_extremes(datasets):\n",
    "    log(\"--- Generating Utilization Plots ---\")\n",
    "    df_alloc = datasets['ALLOCATIONS']\n",
    "    df_loc = datasets['LOCATIONS']\n",
    "    merged = df_alloc.merge(df_loc, left_on='LOCATION_ID', right_on='loc_inst_code', how='inner')\n",
    "    \n",
    "    if merged.empty: return\n",
    "\n",
    "    # Calc Stats\n",
    "    merged['STACK_VOL'] = (merged['GRID_X'] * merged['ORIENT_X_MM']) * \\\n",
    "                          (merged['GRID_Y'] * merged['ORIENT_Y_MM']) * \\\n",
    "                          (merged['GRID_Z'] * merged['ORIENT_Z_MM'])\n",
    "    merged['LOC_VOL'] = merged['width'] * merged['depth'] * merged['height']\n",
    "    merged = merged[merged['LOC_VOL'] > 0]\n",
    "    merged['UTILIZATION_PCT'] = (merged['STACK_VOL'] / merged['LOC_VOL']) * 100\n",
    "    \n",
    "    # 1. Top Utilization\n",
    "    top_row = merged.sort_values(by='UTILIZATION_PCT', ascending=False).iloc[0]\n",
    "    plot_single_bin(top_row, \"Top Utilization\", \"visual_utilization_max\")\n",
    "\n",
    "    # 2. Lowest Utilization (Non-Zero)\n",
    "    # We want active allocations, so filter > 0\n",
    "    active_allocs = merged[merged['UTILIZATION_PCT'] > 0]\n",
    "    if not active_allocs.empty:\n",
    "        low_row = active_allocs.sort_values(by='UTILIZATION_PCT', ascending=True).iloc[0]\n",
    "        plot_single_bin(low_row, \"Lowest Utilization\", \"visual_utilization_min\")\n",
    "    else:\n",
    "        log(\"WARN: No active allocations found to plot lowest utilization.\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def run_full_diagnostic():\n",
    "    datasets, valid_flags = load_all_data()\n",
    "    \n",
    "    if not all(valid_flags.values()):\n",
    "        log(\"STOPPING: Schema Errors. Check log above.\")\n",
    "        return\n",
    "\n",
    "    # 1. Single SKU\n",
    "    log(\"--- Check: Single SKU per Bin ---\")\n",
    "    dupes = check_single_sku(datasets['ALLOCATIONS'])\n",
    "    if not dupes.empty:\n",
    "        log(f\"FAIL: {len(dupes)} locations with multiple SKUs/duplicates.\")\n",
    "        dupes.to_csv(f\"{OUTPUT_DIR}/fail_single_sku.csv\")\n",
    "    else:\n",
    "        log(\"PASS: All bins contain single SKU.\")\n",
    "\n",
    "    # 2. Grid Math\n",
    "    log(\"--- Check: Grid Math Consistency ---\")\n",
    "    grid_issues = estimate_and_sample(datasets['ALLOCATIONS'], \"Grid Math\", check_grid_consistency)\n",
    "    if not grid_issues.empty:\n",
    "        log(f\"FAIL: {len(grid_issues)} allocations have grid mismatch.\")\n",
    "        grid_issues.to_csv(f\"{OUTPUT_DIR}/fail_grid_math.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Grid math consistent.\")\n",
    "\n",
    "    # 3. Rigid Body\n",
    "    log(\"--- Check: Rigid Body Physics ---\")\n",
    "    rigid_issues = estimate_and_sample(datasets['ALLOCATIONS'], \"Rigid Body\", check_rigid_body, datasets['PARTS'])\n",
    "    if not rigid_issues.empty:\n",
    "        log(f\"FAIL: {len(rigid_issues)} allocations morph dimensions.\")\n",
    "        rigid_issues.to_csv(f\"{OUTPUT_DIR}/fail_rigid_body.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Rigid body dimensions valid.\")\n",
    "\n",
    "    # 4. Bin Overlap\n",
    "    log(\"--- Check: Global Bin Overlaps ---\")\n",
    "    overlap_issues = estimate_and_sample(datasets['LOCATIONS'], \"Bin Overlap\", check_bin_overlaps)\n",
    "    if not overlap_issues.empty:\n",
    "        log(f\"FAIL: {len(overlap_issues)} pairs of locations overlap.\")\n",
    "        overlap_issues.to_csv(f\"{OUTPUT_DIR}/fail_bin_overlap.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: No bin overlaps detected.\")\n",
    "\n",
    "    # 5. Visualization\n",
    "    visualize_utilization_extremes(datasets)\n",
    "\n",
    "    with open(f\"{OUTPUT_DIR}/validation_report.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(report_buffer))\n",
    "    log(\"Validation Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_diagnostic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

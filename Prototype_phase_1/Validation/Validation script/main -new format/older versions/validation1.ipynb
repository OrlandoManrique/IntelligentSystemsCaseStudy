{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a84296f-df2c-4d62-8268-29959ddd9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:14:10] Output folder 'validation_results' ready.\n",
      "[2025-12-28 14:14:10] \u001b[96m--- Loading LOCATIONS ---\u001b[0m\n",
      "[2025-12-28 14:14:10] SUCCESS: LOCATIONS loaded (357 rows).\n",
      "[2025-12-28 14:14:10] \u001b[96m--- Loading ALLOCATIONS ---\u001b[0m\n",
      "[2025-12-28 14:14:10] SUCCESS: ALLOCATIONS loaded (10 rows).\n",
      "[2025-12-28 14:14:10] \u001b[96m--- Loading PARTS ---\u001b[0m\n",
      "[2025-12-28 14:14:10] SUCCESS: PARTS loaded (75 rows).\n",
      "[2025-12-28 14:14:10] \u001b[96m--- Checking Referential Integrity ---\u001b[0m\n",
      "[2025-12-28 14:14:10] \u001b[92m\u001b[1mPASS\u001b[0m: All Allocations match valid Locations.\n",
      "[2025-12-28 14:14:10] \u001b[91m\u001b[1mFAIL\u001b[0m: 10 Allocations point to unknown SKUs.\n",
      "[2025-12-28 14:14:10] \u001b[96m--- Geometric Fit Analysis ---\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3687.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:14:10] \u001b[92m\u001b[1mPASS\u001b[0m: All allocations fit geometrically.\n",
      "[2025-12-28 14:14:10] \u001b[96m--- Generating Top Utilization Plot ---\u001b[0m\n",
      "[2025-12-28 14:14:10] Top Utilization Found: 87.36% at A1-00004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-28 14:14:11] Visualization saved to validation_results/top_utilization_visual.png\n",
      "[2025-12-28 14:14:11] Report saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & SCHEMA DEFINITION\n",
    "# ==========================================\n",
    "\n",
    "LOCATIONS_FILE = 'locations_dummy.csv'\n",
    "ALLOCATIONS_FILE = 'allocations.csv'\n",
    "PARTS_FILE = 'synthetic_parts_generated.csv'\n",
    "OUTPUT_DIR = 'validation_results'\n",
    "MAX_EXECUTION_TIME_SEC = 300\n",
    "\n",
    "# Strict Schema required for Geometric Logic\n",
    "REQUIRED_SCHEMA = {\n",
    "    'LOCATIONS': {\n",
    "        'file': LOCATIONS_FILE,\n",
    "        'columns': ['loc_inst_code', 'width', 'depth', 'height', 'x', 'y', 'z']\n",
    "    },\n",
    "    'ALLOCATIONS': {\n",
    "        'file': ALLOCATIONS_FILE,\n",
    "        'columns': ['LOCATION_ID', 'SKU', 'GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                    'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', 'INIT_UNITS']\n",
    "    },\n",
    "    'PARTS': {\n",
    "        'file': PARTS_FILE,\n",
    "        'columns': ['ITEM_ID', 'LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG']\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. COLOR LOGGING UTILS\n",
    "# ==========================================\n",
    "\n",
    "class Colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "report_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"\n",
    "    Prints to console with color coding based on keywords, \n",
    "    but saves plain text to the report buffer.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    plain_msg = f\"[{timestamp}] {message}\"\n",
    "    report_buffer.append(plain_msg)\n",
    "\n",
    "    # Color Logic for Terminal\n",
    "    colored_msg = message\n",
    "    if \"PASS\" in message:\n",
    "        colored_msg = message.replace(\"PASS\", f\"{Colors.GREEN}{Colors.BOLD}PASS{Colors.RESET}\")\n",
    "    elif \"FAIL\" in message:\n",
    "        colored_msg = message.replace(\"FAIL\", f\"{Colors.RED}{Colors.BOLD}FAIL{Colors.RESET}\")\n",
    "    elif \"CRITICAL\" in message:\n",
    "        colored_msg = f\"{Colors.RED}{Colors.BOLD}{message}{Colors.RESET}\"\n",
    "    elif \"WARN\" in message:\n",
    "        colored_msg = message.replace(\"WARN\", f\"{Colors.YELLOW}{Colors.BOLD}WARN{Colors.RESET}\")\n",
    "    elif \"---\" in message:\n",
    "        colored_msg = f\"{Colors.CYAN}{message}{Colors.RESET}\"\n",
    "    \n",
    "    print(f\"[{timestamp}] {colored_msg}\")\n",
    "\n",
    "def setup_environment():\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    log(f\"Output folder '{OUTPUT_DIR}' ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "\n",
    "def load_and_validate_dataset(key, config):\n",
    "    filepath = config['file']\n",
    "    required_cols = config['columns']\n",
    "    \n",
    "    log(f\"--- Loading {key} ---\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        log(f\"CRITICAL: File {filepath} not found.\")\n",
    "        return None, False\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=None, engine='python', dtype=str)\n",
    "        df.columns = df.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "        \n",
    "        missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            log(f\"CRITICAL SCHEMA ERROR in {key}.\")\n",
    "            log(f\"   Missing: {missing_cols}\")\n",
    "            return df, False\n",
    "        else:\n",
    "            log(f\"SUCCESS: {key} loaded ({len(df)} rows).\")\n",
    "            return df, True\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"CRITICAL ERROR reading {filepath}: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def convert_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    setup_environment()\n",
    "    datasets = {}\n",
    "    valid_flags = {}\n",
    "    \n",
    "    # Load and convert based on config\n",
    "    # Locations\n",
    "    df, valid = load_and_validate_dataset('LOCATIONS', REQUIRED_SCHEMA['LOCATIONS'])\n",
    "    if valid: df = convert_numeric(df, ['width', 'depth', 'height', 'x', 'y', 'z'])\n",
    "    datasets['LOCATIONS'] = df\n",
    "    valid_flags['LOCATIONS'] = valid\n",
    "\n",
    "    # Allocations\n",
    "    df, valid = load_and_validate_dataset('ALLOCATIONS', REQUIRED_SCHEMA['ALLOCATIONS'])\n",
    "    if valid: \n",
    "        df = convert_numeric(df, ['GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                                  'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', 'INIT_UNITS'])\n",
    "    datasets['ALLOCATIONS'] = df\n",
    "    valid_flags['ALLOCATIONS'] = valid\n",
    "\n",
    "    # Parts\n",
    "    df, valid = load_and_validate_dataset('PARTS', REQUIRED_SCHEMA['PARTS'])\n",
    "    if valid: df = convert_numeric(df, ['LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG'])\n",
    "    datasets['PARTS'] = df\n",
    "    valid_flags['PARTS'] = valid\n",
    "    \n",
    "    return datasets, valid_flags\n",
    "\n",
    "# ==========================================\n",
    "# 4. VISUALIZATION LOGIC (NEW)\n",
    "# ==========================================\n",
    "\n",
    "def visualize_top_utilization(datasets):\n",
    "    log(\"--- Generating Top Utilization Plot ---\")\n",
    "    \n",
    "    df_alloc = datasets['ALLOCATIONS']\n",
    "    df_loc = datasets['LOCATIONS']\n",
    "    \n",
    "    # Merge to get dimensions\n",
    "    merged = df_alloc.merge(df_loc, left_on='LOCATION_ID', right_on='loc_inst_code', how='inner')\n",
    "    \n",
    "    if merged.empty:\n",
    "        log(\"WARN: No merged data available for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Calculate Utilization\n",
    "    # Stack Vol = Grid Count * Orient Dim\n",
    "    # Bin Vol = W * D * H\n",
    "    merged['STACK_VOL'] = (merged['GRID_X'] * merged['ORIENT_X_MM']) * \\\n",
    "                          (merged['GRID_Y'] * merged['ORIENT_Y_MM']) * \\\n",
    "                          (merged['GRID_Z'] * merged['ORIENT_Z_MM'])\n",
    "    \n",
    "    merged['LOC_VOL'] = merged['width'] * merged['depth'] * merged['height']\n",
    "    \n",
    "    # Avoid div by zero\n",
    "    merged = merged[merged['LOC_VOL'] > 0]\n",
    "    merged['UTILIZATION_PCT'] = (merged['STACK_VOL'] / merged['LOC_VOL']) * 100\n",
    "    \n",
    "    # Get Top Item\n",
    "    top_row = merged.sort_values(by='UTILIZATION_PCT', ascending=False).iloc[0]\n",
    "    \n",
    "    log(f\"Top Utilization Found: {top_row['UTILIZATION_PCT']:.2f}% at {top_row['LOCATION_ID']}\")\n",
    "    \n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    fig.suptitle(f\"Top Utilization: {top_row['LOCATION_ID']} (SKU: {top_row['SKU']}) - {top_row['UTILIZATION_PCT']:.1f}% Full\", fontsize=16)\n",
    "\n",
    "    # --- Data Prep ---\n",
    "    bin_w = top_row['width']\n",
    "    bin_d = top_row['depth']\n",
    "    bin_h = top_row['height']\n",
    "    \n",
    "    item_w = top_row['ORIENT_X_MM']\n",
    "    item_d = top_row['ORIENT_Y_MM']\n",
    "    item_h = top_row['ORIENT_Z_MM']\n",
    "    \n",
    "    grid_x = int(top_row['GRID_X'])\n",
    "    grid_y = int(top_row['GRID_Y'])\n",
    "    grid_z = int(top_row['GRID_Z'])\n",
    "\n",
    "    # --- Front View (Width vs Height) ---\n",
    "    # Draw Bin\n",
    "    ax1.add_patch(patches.Rectangle((0, 0), bin_w, bin_h, fill=False, edgecolor='red', linewidth=3, label='Bin Boundary'))\n",
    "    \n",
    "    # Draw Items (Loop Z then X)\n",
    "    for z in range(grid_z):\n",
    "        for x in range(grid_x):\n",
    "            # Bottom-left corner of item\n",
    "            pos_x = x * item_w\n",
    "            pos_z = z * item_h\n",
    "            \n",
    "            rect = patches.Rectangle((pos_x, pos_z), item_w, item_h, \n",
    "                                     linewidth=1, edgecolor='black', facecolor='skyblue', alpha=0.6)\n",
    "            ax1.add_patch(rect)\n",
    "            \n",
    "    ax1.set_xlim(-100, bin_w + 100)\n",
    "    ax1.set_ylim(-100, bin_h + 100)\n",
    "    ax1.set_title(f\"FRONT VIEW (X-Z)\\nGrid: {grid_x} Wide x {grid_z} High\")\n",
    "    ax1.set_xlabel(\"Width (mm)\")\n",
    "    ax1.set_ylabel(\"Height (mm)\")\n",
    "    ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "\n",
    "    # --- Top View (Width vs Depth) ---\n",
    "    # Draw Bin\n",
    "    ax2.add_patch(patches.Rectangle((0, 0), bin_w, bin_d, fill=False, edgecolor='red', linewidth=3, label='Bin Boundary'))\n",
    "    \n",
    "    # Draw Items (Loop Y then X) - Note: In top view, we see the 'footprint'\n",
    "    for y in range(grid_y):\n",
    "        for x in range(grid_x):\n",
    "            pos_x = x * item_w\n",
    "            pos_y = y * item_d\n",
    "            \n",
    "            rect = patches.Rectangle((pos_x, pos_y), item_w, item_d, \n",
    "                                     linewidth=1, edgecolor='black', facecolor='orange', alpha=0.6)\n",
    "            ax2.add_patch(rect)\n",
    "\n",
    "    ax2.set_xlim(-100, bin_w + 100)\n",
    "    ax2.set_ylim(-100, bin_d + 100)\n",
    "    ax2.set_title(f\"TOP VIEW (X-Y)\\nGrid: {grid_x} Wide x {grid_y} Deep\")\n",
    "    ax2.set_xlabel(\"Width (mm)\")\n",
    "    ax2.set_ylabel(\"Depth (mm)\")\n",
    "    ax2.grid(True, linestyle='--', alpha=0.3)\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # Save\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"{OUTPUT_DIR}/top_utilization_visual.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    log(f\"Visualization saved to {save_path}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. CORE VALIDATION LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def func_geometric_fit(df_alloc, df_loc, quiet=False):\n",
    "    merged = df_alloc.merge(df_loc, left_on='LOCATION_ID', right_on='loc_inst_code', how='left')\n",
    "    issues = []\n",
    "    \n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue \n",
    "\n",
    "        used_x = row['GRID_X'] * row['ORIENT_X_MM']\n",
    "        used_y = row['GRID_Y'] * row['ORIENT_Y_MM']\n",
    "        used_z = row['GRID_Z'] * row['ORIENT_Z_MM']\n",
    "        \n",
    "        tolerance = 1.0 \n",
    "        \n",
    "        fail_x = used_x > (row['width'] + tolerance)\n",
    "        fail_y = used_y > (row['depth'] + tolerance)\n",
    "        fail_z = used_z > (row['height'] + tolerance)\n",
    "        \n",
    "        if fail_x or fail_y or fail_z:\n",
    "            issues.append({\n",
    "                'LOCATION_ID': row['LOCATION_ID'],\n",
    "                'Issue': 'Geometric Fail',\n",
    "                'Details': f\"Loc: {row['width']}x{row['depth']}x{row['height']} | Stack: {used_x}x{used_y}x{used_z}\"\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "def check_referential_integrity(datasets):\n",
    "    log(\"--- Checking Referential Integrity ---\")\n",
    "    \n",
    "    orphans = datasets['ALLOCATIONS'][~datasets['ALLOCATIONS']['LOCATION_ID'].isin(datasets['LOCATIONS']['loc_inst_code'])]\n",
    "    if len(orphans) > 0:\n",
    "        log(f\"FAIL: {len(orphans)} Allocations point to unknown Locations.\")\n",
    "    else:\n",
    "        log(\"PASS: All Allocations match valid Locations.\")\n",
    "            \n",
    "    orphans = datasets['ALLOCATIONS'][~datasets['ALLOCATIONS']['SKU'].isin(datasets['PARTS']['ITEM_ID'])]\n",
    "    if len(orphans) > 0:\n",
    "        log(f\"FAIL: {len(orphans)} Allocations point to unknown SKUs.\")\n",
    "    else:\n",
    "        log(\"PASS: All Allocations match valid Parts.\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def run_full_diagnostic():\n",
    "    datasets, valid_flags = load_all_data()\n",
    "    \n",
    "    if not all(valid_flags.values()):\n",
    "        log(\"STOPPING: Fix Schema Errors defined above to proceed.\")\n",
    "        return\n",
    "\n",
    "    # 1. Integrity\n",
    "    check_referential_integrity(datasets)\n",
    "\n",
    "    # 2. Geometric Fit\n",
    "    log(\"--- Geometric Fit Analysis ---\")\n",
    "    # Simple sampling for estimation, keeping logic clean\n",
    "    fit_issues = func_geometric_fit(datasets['ALLOCATIONS'], datasets['LOCATIONS'], quiet=False)\n",
    "    \n",
    "    if not fit_issues.empty:\n",
    "        log(f\"FAIL: Found {len(fit_issues)} allocations that do not fit.\")\n",
    "        fit_issues.to_csv(f\"{OUTPUT_DIR}/fit_issues.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: All allocations fit geometrically.\")\n",
    "\n",
    "    # 3. Visualization (The new feature)\n",
    "    visualize_top_utilization(datasets)\n",
    "\n",
    "    # Save Log\n",
    "    with open(f\"{OUTPUT_DIR}/validation_report.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(report_buffer))\n",
    "    log(f\"Report saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_diagnostic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

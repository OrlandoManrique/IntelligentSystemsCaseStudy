{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a84296f-df2c-4d62-8268-29959ddd9fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] Output folder 'validation_results' ready.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Loading LOCATIONS (locations_dummy.csv) ---\u001b[0m\n",
      "[2026-01-05 18:41:37] SUCCESS: LOCATIONS loaded (357 rows).\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Loading ALLOCATIONS (allocations.csv) ---\u001b[0m\n",
      "[2026-01-05 18:41:37] SUCCESS: ALLOCATIONS loaded (10 rows).\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Loading PARTS (synthetic_parts_generated.csv) ---\u001b[0m\n",
      "[2026-01-05 18:41:37] SUCCESS: PARTS loaded (75 rows).\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Volume Data Integrity ---\u001b[0m\n",
      "[2026-01-05 18:41:37] Starting Volume Data...\n",
      "[2026-01-05 18:41:37]    Est. time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 4843.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] \u001b[92m\u001b[1mPASS\u001b[0m: Stated volumes match location dimensions.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Stated Utilization Accuracy ---\u001b[0m\n",
      "[2026-01-05 18:41:37] Starting Utilization Data...\n",
      "[2026-01-05 18:41:37]    Est. time: 0.01s\n",
      "[2026-01-05 18:41:37] INFO: Checking utilization accuracy using column: 'UTILIZATION_PCT'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3740.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] \u001b[92m\u001b[1mPASS\u001b[0m: Utilization data matches calculations.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Inventory Balance (Partial Unallocated) ---\u001b[0m\n",
      "[2026-01-05 18:41:37] \u001b[91m\u001b[1mFAIL\u001b[0m: Found 66 Items with inventory mismatches.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Single SKU per Bin ---\u001b[0m\n",
      "[2026-01-05 18:41:37] \u001b[92m\u001b[1mPASS\u001b[0m: Single SKU constraint met.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Grid Math ---\u001b[0m\n",
      "[2026-01-05 18:41:37] Starting Grid Math...\n",
      "[2026-01-05 18:41:37]    Est. time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3665.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] \u001b[92m\u001b[1mPASS\u001b[0m: Grid Math consistent.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Rigid Body ---\u001b[0m\n",
      "[2026-01-05 18:41:37] Starting Rigid Body...\n",
      "[2026-01-05 18:41:37]    Est. time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3460.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] \u001b[92m\u001b[1mPASS\u001b[0m: Rigid Body dimensions valid.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Stack vs Bin Dimensions ---\u001b[0m\n",
      "[2026-01-05 18:41:37] Starting Stack Fit...\n",
      "[2026-01-05 18:41:37]    Est. time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3738.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] \u001b[92m\u001b[1mPASS\u001b[0m: All stacks fit within bins.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Bin Overlaps ---\u001b[0m\n",
      "[2026-01-05 18:41:37] Starting Bin Overlap...\n",
      "[2026-01-05 18:41:37]    Est. time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 357/357 [00:00<00:00, 70444.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] \u001b[92m\u001b[1mPASS\u001b[0m: No bin overlaps.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Check: Unallocated Feasibility ---\u001b[0m\n",
      "[2026-01-05 18:41:37] INFO: 66 items are Unallocated. Checking if they fit in empty bins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-05 18:41:37] \u001b[91m\u001b[1mFAIL\u001b[0m: 66 Items Unallocated. Sample check (20 items) shows 19 COULD fit in currently empty bins.\n",
      "[2026-01-05 18:41:37] \u001b[96m--- Generating Utilization Plots ---\u001b[0m\n",
      "[2026-01-05 18:41:38] Plot saved to validation_results/visual_utilization_max.png\n",
      "[2026-01-05 18:41:39] Plot saved to validation_results/visual_utilization_min.png\n",
      "[2026-01-05 18:41:39] Validation Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & SCHEMA\n",
    "# ==========================================\n",
    "\n",
    "LOCATIONS_FILE = 'locations_dummy.csv'\n",
    "ALLOCATIONS_FILE = 'allocations.csv'\n",
    "PARTS_FILE = 'synthetic_parts_generated.csv'\n",
    "OUTPUT_DIR = 'validation_results'\n",
    "MAX_EXECUTION_TIME_SEC = 300\n",
    "\n",
    "REQUIRED_SCHEMA = {\n",
    "    'LOCATIONS': {\n",
    "        'file': LOCATIONS_FILE,\n",
    "        # Ensure your locations file actually has these columns\n",
    "        'columns': ['loc_inst_code', 'width', 'depth', 'height', 'x', 'y', 'z']\n",
    "    },\n",
    "    'ALLOCATIONS': {\n",
    "        'file': ALLOCATIONS_FILE,\n",
    "        # UPDATED COLUMN LIST TO MATCH YOUR NEW FORMAT\n",
    "        'columns': ['loc_inst_code', 'LOCATION_TYPE', 'ITEM_ID', \n",
    "                    'QTY_ALLOCATED', 'MAX_UNITS', \n",
    "                    'GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                    'FULL_LAYERS', 'PARTIAL_UNITS',\n",
    "                    'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                    'LOCATION_VOL_MM3', 'LOCATION_VOL_M3', \n",
    "                    'STORED_VOL_M3', 'UTILIZATION_PCT'] \n",
    "    },\n",
    "    'PARTS': {\n",
    "        'file': PARTS_FILE,\n",
    "        'columns': ['ITEM_ID', 'LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG', \n",
    "                    'BOXES_ON_HAND', 'QTY_PER_BOX']\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. LOGGING & UTILS\n",
    "# ==========================================\n",
    "\n",
    "class Colors:\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    CYAN = '\\033[96m'\n",
    "    RESET = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "\n",
    "report_buffer = []\n",
    "\n",
    "def log(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    report_buffer.append(f\"[{timestamp}] {message}\")\n",
    "    \n",
    "    c_msg = message\n",
    "    if \"PASS\" in message: c_msg = message.replace(\"PASS\", f\"{Colors.GREEN}{Colors.BOLD}PASS{Colors.RESET}\")\n",
    "    elif \"FAIL\" in message: c_msg = message.replace(\"FAIL\", f\"{Colors.RED}{Colors.BOLD}FAIL{Colors.RESET}\")\n",
    "    elif \"CRITICAL\" in message: c_msg = f\"{Colors.RED}{Colors.BOLD}{message}{Colors.RESET}\"\n",
    "    elif \"WARN\" in message: c_msg = message.replace(\"WARN\", f\"{Colors.YELLOW}{Colors.BOLD}WARN{Colors.RESET}\")\n",
    "    elif \"---\" in message: c_msg = f\"{Colors.CYAN}{message}{Colors.RESET}\"\n",
    "    \n",
    "    print(f\"[{timestamp}] {c_msg}\")\n",
    "\n",
    "def setup_environment():\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    log(f\"Output folder '{OUTPUT_DIR}' ready.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA LOADING\n",
    "# ==========================================\n",
    "\n",
    "def load_and_validate_dataset(key, config):\n",
    "    filepath = config['file']\n",
    "    log(f\"--- Loading {key} ({filepath}) ---\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        log(f\"CRITICAL: File {filepath} not found.\")\n",
    "        return None, False\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=None, engine='python', dtype=str)\n",
    "        df.columns = df.columns.str.strip().str.replace('^ï»¿', '', regex=True)\n",
    "        \n",
    "        # Add Original Row ID\n",
    "        df['ROW_ID'] = df.index + 2 \n",
    "\n",
    "        missing = [c for c in config['columns'] if c not in df.columns]\n",
    "        if missing:\n",
    "            log(f\"CRITICAL SCHEMA ERROR in {key}. Missing: {missing}\")\n",
    "            return df, False\n",
    "        \n",
    "        log(f\"SUCCESS: {key} loaded ({len(df)} rows).\")\n",
    "        return df, True\n",
    "    except Exception as e:\n",
    "        log(f\"CRITICAL ERROR reading {filepath}: {e}\")\n",
    "        return None, False\n",
    "\n",
    "def convert_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    return df\n",
    "\n",
    "def load_all_data():\n",
    "    setup_environment()\n",
    "    datasets = {}\n",
    "    valid_flags = {}\n",
    "    \n",
    "    # Locations\n",
    "    df, valid = load_and_validate_dataset('LOCATIONS', REQUIRED_SCHEMA['LOCATIONS'])\n",
    "    if valid: df = convert_numeric(df, ['width', 'depth', 'height', 'x', 'y', 'z'])\n",
    "    datasets['LOCATIONS'] = df\n",
    "    valid_flags['LOCATIONS'] = valid\n",
    "\n",
    "    # Allocations\n",
    "    df, valid = load_and_validate_dataset('ALLOCATIONS', REQUIRED_SCHEMA['ALLOCATIONS'])\n",
    "    if valid: \n",
    "        # UPDATED NUMERIC COLUMNS\n",
    "        df = convert_numeric(df, ['GRID_X', 'GRID_Y', 'GRID_Z', \n",
    "                                  'ORIENT_X_MM', 'ORIENT_Y_MM', 'ORIENT_Z_MM', \n",
    "                                  'MAX_UNITS', 'QTY_ALLOCATED', \n",
    "                                  'LOCATION_VOL_MM3', 'UTILIZATION_PCT'])\n",
    "    datasets['ALLOCATIONS'] = df\n",
    "    valid_flags['ALLOCATIONS'] = valid\n",
    "\n",
    "    # Parts\n",
    "    df, valid = load_and_validate_dataset('PARTS', REQUIRED_SCHEMA['PARTS'])\n",
    "    if valid: df = convert_numeric(df, ['LEN_MM', 'WID_MM', 'DEP_MM', 'WT_KG', \n",
    "                                        'BOXES_ON_HAND', 'QTY_PER_BOX'])\n",
    "    datasets['PARTS'] = df\n",
    "    valid_flags['PARTS'] = valid\n",
    "    \n",
    "    return datasets, valid_flags\n",
    "\n",
    "# ==========================================\n",
    "# 4. VALIDATION LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def estimate_and_sample(df, check_name, validation_func, *args):\n",
    "    log(f\"Starting {check_name}...\")\n",
    "    total = len(df)\n",
    "    test_size = min(1000, total)\n",
    "    if test_size == 0: return pd.DataFrame()\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        _ = validation_func(df.head(test_size), *args, quiet=True)\n",
    "    except Exception as e:\n",
    "        log(f\"ERROR in {check_name}: {e}\")\n",
    "        # raise e # Uncomment for debugging\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    dur = time.time() - t0\n",
    "    if dur == 0: dur = 0.001\n",
    "    est_time = (dur / test_size) * total\n",
    "    log(f\"   Est. time: {est_time:.2f}s\")\n",
    "\n",
    "    if est_time > MAX_EXECUTION_TIME_SEC:\n",
    "        safe_rows = int((MAX_EXECUTION_TIME_SEC / dur) * test_size)\n",
    "        log(f\"   WARN: Time limit exceeded. Sampling {safe_rows} rows.\")\n",
    "        df_to_process = df.sample(n=safe_rows, random_state=42)\n",
    "    else:\n",
    "        df_to_process = df\n",
    "\n",
    "    return validation_func(df_to_process, *args, quiet=False)\n",
    "\n",
    "# --- CHECK: Stated Utilization Accuracy ---\n",
    "# --- CHECK: Stated Utilization Accuracy ---\n",
    "def check_stated_utilization(df_alloc, df_loc, quiet=False):\n",
    "    col_name = 'UTILIZATION_PCT'\n",
    "    \n",
    "    if col_name not in df_alloc.columns:\n",
    "        if not quiet: log(f\"WARN: Column '{col_name}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if not quiet: log(f\"INFO: Checking utilization accuracy using column: '{col_name}'\")\n",
    "\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    \n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue\n",
    "\n",
    "        # --- FIX STARTS HERE ---\n",
    "        \n",
    "        # 1. Calculate volume of ONE unit based on its orientation\n",
    "        one_unit_vol = row['ORIENT_X_MM'] * row['ORIENT_Y_MM'] * row['ORIENT_Z_MM']\n",
    "        \n",
    "        # 2. Multiply by the ACTUAL quantity in the bin (not the Grid capacity)\n",
    "        actual_occupied_vol = one_unit_vol * row['QTY_ALLOCATED']\n",
    "        \n",
    "        # --- FIX ENDS HERE ---\n",
    "        \n",
    "        loc_vol = row['width'] * row['depth'] * row['height']\n",
    "        \n",
    "        if loc_vol <= 0: continue\n",
    "\n",
    "        # Calculate percentage based on actual inventory\n",
    "        calc_pct = (actual_occupied_vol / loc_vol) * 100\n",
    "        stated_pct = row[col_name]\n",
    "\n",
    "        # Tolerance: 1% difference\n",
    "        if abs(calc_pct - stated_pct) > 1.0:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'loc_inst_code': row['loc_inst_code'],\n",
    "                'Issue': 'Utilization Data Mismatch',\n",
    "                # Updated error message to be clearer\n",
    "                'Details': f\"Stated: {stated_pct:.1f}% != Calc: {calc_pct:.1f}% (Qty {row['QTY_ALLOCATED']} * Vol / Bin Vol)\"\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Stack vs Bin Dimensions (Geometric Fit) ---\n",
    "def check_stack_fit(df_alloc, df_loc, quiet=False):\n",
    "    # UPDATED: Merge on 'loc_inst_code'\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    issues = []\n",
    "    \n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue \n",
    "\n",
    "        stack_x = row['GRID_X'] * row['ORIENT_X_MM']\n",
    "        stack_y = row['GRID_Y'] * row['ORIENT_Y_MM']\n",
    "        stack_z = row['GRID_Z'] * row['ORIENT_Z_MM']\n",
    "        \n",
    "        tol = 1.0\n",
    "        if (stack_x > row['width'] + tol) or (stack_y > row['depth'] + tol) or (stack_z > row['height'] + tol):\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'loc_inst_code': row['loc_inst_code'],\n",
    "                'ITEM_ID': row['ITEM_ID'], # Updated from SKU\n",
    "                'Issue': 'Stack Exceeds Bin',\n",
    "                'Details': f\"Bin: {row['width']}x{row['depth']}x{row['height']} | Stack: {stack_x:.1f}x{stack_y:.1f}x{stack_z:.1f}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Inventory Balance (Partial Unallocated) ---\n",
    "def check_inventory_balance(df_alloc, df_parts):\n",
    "    log(\"--- Check: Inventory Balance (Partial Unallocated) ---\")\n",
    "    \n",
    "    # FIX: Removed * df_parts['QTY_PER_BOX'] to ensure we count boxes, not contents\n",
    "    df_parts['TOTAL_DEMAND'] = df_parts['BOXES_ON_HAND'] \n",
    "    \n",
    "    demand_map = df_parts.set_index('ITEM_ID')['TOTAL_DEMAND'].to_dict()\n",
    "    \n",
    "    # Ensure numeric\n",
    "    df_alloc['QTY_ALLOCATED'] = pd.to_numeric(df_alloc['QTY_ALLOCATED'], errors='coerce').fillna(0)\n",
    "    alloc_sums = df_alloc.groupby('ITEM_ID')['QTY_ALLOCATED'].sum().to_dict()\n",
    "    \n",
    "    issues = []\n",
    "    for item_id, demand in demand_map.items():\n",
    "        allocated = alloc_sums.get(item_id, 0)\n",
    "        \n",
    "        # Use simple float tolerance just in case\n",
    "        if allocated < (demand - 0.01):\n",
    "            issues.append({'ITEM_ID': item_id, 'Issue': 'Partial/No Allocation', 'Demand': demand, 'Allocated': allocated, 'Missing': demand - allocated})\n",
    "        elif allocated > (demand + 0.01):\n",
    "            issues.append({'ITEM_ID': item_id, 'Issue': 'Over-Allocation', 'Demand': demand, 'Allocated': allocated, 'Excess': allocated - demand})\n",
    "            \n",
    "    df_issues = pd.DataFrame(issues)\n",
    "    if not df_issues.empty:\n",
    "        log(f\"FAIL: Found {len(df_issues)} Items with inventory mismatches.\")\n",
    "        df_issues.to_csv(f\"{OUTPUT_DIR}/fail_inventory_balance.csv\", index=False)\n",
    "    else:\n",
    "        log(\"PASS: Total allocated boxes match inventory on hand.\")\n",
    "\n",
    "# --- CHECK: Volume Data Integrity ---\n",
    "def check_volume_data_integrity(df_alloc, df_loc, quiet=False):\n",
    "    # UPDATED: Merge on loc_inst_code\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='left', suffixes=('', '_LOC'))\n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['width']): continue\n",
    "        real_vol = row['width'] * row['depth'] * row['height']\n",
    "        \n",
    "        # Using LOCATION_VOL_MM3 from new format\n",
    "        if abs(real_vol - row['LOCATION_VOL_MM3']) > 1.0:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'],\n",
    "                'loc_inst_code': row['loc_inst_code'],\n",
    "                'Issue': 'Volume Data Error',\n",
    "                'Details': f\"Stated: {row['LOCATION_VOL_MM3']} != Real: {real_vol}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Grid Math ---\n",
    "def check_grid_consistency(df_alloc, quiet=False):\n",
    "    issues = []\n",
    "    iterator = tqdm(df_alloc.iterrows(), total=df_alloc.shape[0]) if not quiet else df_alloc.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        grid_cap = row['GRID_X'] * row['GRID_Y'] * row['GRID_Z']\n",
    "        if abs(grid_cap - row['MAX_UNITS']) > 0.1:\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'], \n",
    "                'loc_inst_code': row['loc_inst_code'], \n",
    "                'Issue': 'Grid Math Mismatch', \n",
    "                'Details': f\"Grid ({grid_cap}) != MaxUnits ({row['MAX_UNITS']})\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Rigid Body ---\n",
    "def check_rigid_body(df_alloc, df_parts, quiet=False):\n",
    "    # UPDATED: Merge on ITEM_ID\n",
    "    merged = df_alloc.merge(df_parts, on='ITEM_ID', how='left', suffixes=('', '_PART'))\n",
    "    issues = []\n",
    "    iterator = tqdm(merged.iterrows(), total=merged.shape[0]) if not quiet else merged.iterrows()\n",
    "    for idx, row in iterator:\n",
    "        if pd.isna(row['LEN_MM']): continue\n",
    "        od = sorted([row['ORIENT_X_MM'], row['ORIENT_Y_MM'], row['ORIENT_Z_MM']])\n",
    "        pd_ = sorted([row['LEN_MM'], row['WID_MM'], row['DEP_MM']])\n",
    "        if any(abs(o - p) > 0.5 for o, p in zip(od, pd_)):\n",
    "            issues.append({\n",
    "                'ROW_ID': row['ROW_ID'], \n",
    "                'loc_inst_code': row['loc_inst_code'], \n",
    "                'Issue': 'Dimensions Morphing', \n",
    "                'Details': f\"Alloc {od} vs Part {pd_}\"\n",
    "            })\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Bin Overlap ---\n",
    "def check_bin_overlaps(df_loc, quiet=False):\n",
    "    df = df_loc.copy()\n",
    "    df['x2'] = df['x'] + df['width']\n",
    "    df['y2'] = df['y'] + df['depth']\n",
    "    df['z2'] = df['z'] + df['height']\n",
    "    df.sort_values('x', inplace=True)\n",
    "    issues = []\n",
    "    data = df.to_dict('records')\n",
    "    n = len(data)\n",
    "    if n == 0: return pd.DataFrame()\n",
    "    iter_range = tqdm(range(n), total=n) if not quiet else range(n)\n",
    "    for i in iter_range:\n",
    "        a = data[i]\n",
    "        for j in range(i + 1, n):\n",
    "            b = data[j]\n",
    "            if b['x'] >= a['x2'] - 0.1: break\n",
    "            if (a['y'] < b['y2']) and (a['y2'] > b['y']) and (a['z'] < b['z2']) and (a['z2'] > b['z']):\n",
    "                issues.append({'LOC_A': a['loc_inst_code'], 'LOC_B': b['loc_inst_code'], 'Issue': 'Physical Overlap', 'Details': f\"A({a['x']}) vs B({b['x']})\"})\n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "# --- CHECK: Unallocated Items ---\n",
    "def check_unallocated_feasibility(df_parts, df_alloc, df_loc):\n",
    "    log(\"--- Check: Unallocated Feasibility ---\")\n",
    "    all_items = set(df_parts['ITEM_ID'])\n",
    "    alloc_items = set(df_alloc['ITEM_ID'])\n",
    "    unallocated = list(all_items - alloc_items)\n",
    "    \n",
    "    if not unallocated:\n",
    "        log(\"PASS: All items are allocated.\")\n",
    "        return\n",
    "    log(f\"INFO: {len(unallocated)} items are Unallocated. Checking if they fit in empty bins...\")\n",
    "    \n",
    "    occupied_locs = set(df_alloc['loc_inst_code'])\n",
    "    empty_locs = df_loc[~df_loc['loc_inst_code'].isin(occupied_locs)].copy()\n",
    "    \n",
    "    if empty_locs.empty:\n",
    "        log(\"WARN: Unallocated items exist, but NO empty bins are available.\")\n",
    "        return\n",
    "    empty_locs['vol'] = empty_locs['width'] * empty_locs['depth'] * empty_locs['height']\n",
    "    empty_locs['max_dim'] = empty_locs[['width', 'depth', 'height']].max(axis=1)\n",
    "    \n",
    "    sample_size = min(20, len(unallocated))\n",
    "    sample_items = df_parts[df_parts['ITEM_ID'].isin(unallocated[:sample_size])]\n",
    "    fits_found = 0\n",
    "    for _, part in sample_items.iterrows():\n",
    "        p_vol = part['LEN_MM'] * part['WID_MM'] * part['DEP_MM']\n",
    "        p_max = max(part['LEN_MM'], part['WID_MM'], part['DEP_MM'])\n",
    "        matches = empty_locs[(empty_locs['vol'] >= p_vol) & (empty_locs['max_dim'] >= p_max)]\n",
    "        if not matches.empty: fits_found += 1\n",
    "    if fits_found > 0: log(f\"FAIL: {len(unallocated)} Items Unallocated. Sample check ({sample_size} items) shows {fits_found} COULD fit in currently empty bins.\")\n",
    "    else: log(\"WARN: Unallocated items exist, but appear too large for available empty bins.\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. VISUALIZATION\n",
    "# ==========================================\n",
    "\n",
    "def plot_single_bin(row, title_prefix, filename_tag):\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=[2, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, 0]); ax2 = fig.add_subplot(gs[1, 0]); ax_text = fig.add_subplot(gs[:, 1]); ax_text.axis('off')\n",
    "    bin_w, bin_h, bin_d = row['width'], row['height'], row['depth']\n",
    "    item_w, item_h, item_d = row['ORIENT_X_MM'], row['ORIENT_Z_MM'], row['ORIENT_Y_MM']\n",
    "    gx, gy, gz = int(row['GRID_X']), int(row['GRID_Y']), int(row['GRID_Z'])\n",
    "    \n",
    "    ax1.add_patch(patches.Rectangle((0, 0), bin_w, bin_h, fill=False, edgecolor='red', lw=3, label='Bin'))\n",
    "    for z in range(gz):\n",
    "        for x in range(gx):\n",
    "            ax1.add_patch(patches.Rectangle((x*item_w, z*item_h), item_w, item_h, lw=1, ec='black', fc='skyblue', alpha=0.6))\n",
    "    ax1.set_title(\"FRONT VIEW (X-Z)\", fontsize=12, fontweight='bold'); ax1.set_xlim(-50, bin_w + 50); ax1.set_ylim(-50, bin_h + 50); ax1.set_aspect('equal'); ax1.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    ax2.add_patch(patches.Rectangle((0, 0), bin_w, bin_d, fill=False, edgecolor='red', lw=3))\n",
    "    for y in range(gy):\n",
    "        for x in range(gx):\n",
    "            ax2.add_patch(patches.Rectangle((x*item_w, y*item_d), item_w, item_d, lw=1, ec='black', fc='orange', alpha=0.6))\n",
    "    ax2.set_title(\"TOP VIEW (X-Y)\", fontsize=12, fontweight='bold'); ax2.set_xlim(-50, bin_w + 50); ax2.set_ylim(-50, bin_d + 50); ax2.set_aspect('equal'); ax2.grid(True, linestyle=':', alpha=0.5)\n",
    "\n",
    "    info_text = (\n",
    "        f\"REPORT: {title_prefix}\\n----------------------------------------\\n\"\n",
    "        f\"ALLOCATION ROW:   {row['ROW_ID']}\\nLOCATION ID:      {row['loc_inst_code']}\\nITEM ID:          {row['ITEM_ID']}\\n\"\n",
    "        f\"----------------------------------------\\nMETRICS:\\n\"\n",
    "        f\"  Utilization:    {row['UTILIZATION_PCT']:.2f} %\\n  Total Units:    {row['MAX_UNITS']}\\n\\n\"\n",
    "        f\"DIMENSIONS (mm):\\n  Bin (WxDxH):    {bin_w} x {bin_d} x {bin_h}\\n\"\n",
    "        f\"  Item Orient:    {row['ORIENT_X_MM']} x {row['ORIENT_Y_MM']} x {row['ORIENT_Z_MM']}\\n\"\n",
    "        f\"  Stack Total:    {gx*item_w:.1f} x {gy*item_d:.1f} x {gz*item_h:.1f}\\n\\n\"\n",
    "        f\"GRID CONFIG:\\n  Cols (X):       {gx}\\n  Rows (Y):       {gy}\\n  Layers (Z):     {gz}\\n\\n\"\n",
    "        f\"VOLUMES (mm3):\\n  Bin Volume:     {row['LOC_VOL']:,.0f}\\n  Bulk Item Vol:  {row['STACK_VOL']:,.0f}\\n\"\n",
    "    )\n",
    "    ax_text.text(0.05, 0.95, info_text, transform=ax_text.transAxes, fontsize=12, verticalalignment='top', family='monospace', bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "    plt.tight_layout(); save_path = f\"{OUTPUT_DIR}/{filename_tag}.png\"; plt.savefig(save_path); plt.close(); log(f\"Plot saved to {save_path}\")\n",
    "\n",
    "def visualize_utilization_extremes(datasets):\n",
    "    log(\"--- Generating Utilization Plots ---\")\n",
    "    df_alloc = datasets['ALLOCATIONS']; df_loc = datasets['LOCATIONS']\n",
    "    # UPDATED: Merge on loc_inst_code\n",
    "    merged = df_alloc.merge(df_loc, on='loc_inst_code', how='inner', suffixes=('', '_LOC'))\n",
    "    if merged.empty: return\n",
    "    merged['STACK_VOL'] = merged['QTY_ALLOCATED'] * (merged['ORIENT_X_MM'] * merged['ORIENT_Y_MM'] * merged['ORIENT_Z_MM'])\n",
    "    merged['LOC_VOL'] = merged['width'] * merged['depth'] * merged['height']\n",
    "    merged = merged[merged['LOC_VOL'] > 0]\n",
    "    \n",
    "    # We can rely on the existing UTILIZATION_PCT column now, but calculating it ensures the plot math matches geometry\n",
    "    # merged['UTILIZATION_PCT'] = (merged['STACK_VOL'] / merged['LOC_VOL']) * 100\n",
    "    \n",
    "    top_row = merged.sort_values(by='UTILIZATION_PCT', ascending=False).iloc[0]\n",
    "    plot_single_bin(top_row, \"HIGHEST UTILIZATION\", \"visual_utilization_max\")\n",
    "    active = merged[merged['UTILIZATION_PCT'] > 0]\n",
    "    if not active.empty:\n",
    "        low_row = active.sort_values(by='UTILIZATION_PCT', ascending=True).iloc[0]\n",
    "        plot_single_bin(low_row, \"LOWEST UTILIZATION\", \"visual_utilization_min\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. RUNNER\n",
    "# ==========================================\n",
    "\n",
    "def run_full_diagnostic():\n",
    "    datasets, valid_flags = load_all_data()\n",
    "    if not all(valid_flags.values()): log(\"STOPPING: Schema Errors.\"); return\n",
    "\n",
    "    # 1. Volume Data Integrity\n",
    "    log(\"--- Check: Volume Data Integrity ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Volume Data\", check_volume_data_integrity, datasets['LOCATIONS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Volume Data mismatches.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_volume_data.csv\", index=False)\n",
    "    else: log(\"PASS: Stated volumes match location dimensions.\")\n",
    "    \n",
    "    # 2. Utilization Data Integrity\n",
    "    log(\"--- Check: Stated Utilization Accuracy ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Utilization Data\", check_stated_utilization, datasets['LOCATIONS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Utilization % mismatches.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_utilization_data.csv\", index=False)\n",
    "    else: log(\"PASS: Utilization data matches calculations.\")\n",
    "\n",
    "    # 3. Inventory Balance\n",
    "    check_inventory_balance(datasets['ALLOCATIONS'], datasets['PARTS'])\n",
    "\n",
    "    # 4. Standard Checks\n",
    "    log(\"--- Check: Single SKU per Bin ---\")\n",
    "    # UPDATED: using loc_inst_code\n",
    "    dupes = datasets['ALLOCATIONS'][datasets['ALLOCATIONS'].duplicated(subset=['loc_inst_code'], keep=False)]\n",
    "    if not dupes.empty: log(f\"FAIL: {len(dupes)} locations have multiple SKUs.\"); dupes.to_csv(f\"{OUTPUT_DIR}/fail_single_sku.csv\")\n",
    "    else: log(\"PASS: Single SKU constraint met.\")\n",
    "\n",
    "    log(\"--- Check: Grid Math ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Grid Math\", check_grid_consistency)\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Grid Math errors.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_grid_math.csv\", index=False)\n",
    "    else: log(\"PASS: Grid Math consistent.\")\n",
    "\n",
    "    log(\"--- Check: Rigid Body ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Rigid Body\", check_rigid_body, datasets['PARTS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} Rigid Body errors.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_rigid_body.csv\", index=False)\n",
    "    else: log(\"PASS: Rigid Body dimensions valid.\")\n",
    "\n",
    "    log(\"--- Check: Stack vs Bin Dimensions ---\")\n",
    "    res = estimate_and_sample(datasets['ALLOCATIONS'], \"Stack Fit\", check_stack_fit, datasets['LOCATIONS'])\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} allocations exceed bin dimensions.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_stack_fit.csv\", index=False)\n",
    "    else: log(\"PASS: All stacks fit within bins.\")\n",
    "\n",
    "    log(\"--- Check: Bin Overlaps ---\")\n",
    "    res = estimate_and_sample(datasets['LOCATIONS'], \"Bin Overlap\", check_bin_overlaps)\n",
    "    if not res.empty: log(f\"FAIL: {len(res)} locations overlap physically.\"); res.to_csv(f\"{OUTPUT_DIR}/fail_bin_overlap.csv\", index=False)\n",
    "    else: log(\"PASS: No bin overlaps.\")\n",
    "\n",
    "    check_unallocated_feasibility(datasets['PARTS'], datasets['ALLOCATIONS'], datasets['LOCATIONS'])\n",
    "    visualize_utilization_extremes(datasets)\n",
    "\n",
    "    with open(f\"{OUTPUT_DIR}/validation_report.txt\", \"w\") as f: f.write(\"\\n\".join(report_buffer))\n",
    "    log(\"Validation Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_diagnostic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
